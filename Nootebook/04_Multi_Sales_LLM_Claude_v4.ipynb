{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b539bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Auto-ARIMA no disponible, se omitir√°\n",
      "üì¶ Librer√≠as cargadas exitosamente!\n",
      "üñ•Ô∏è Configuraci√≥n del sistema:\n",
      "   CPUs disponibles: 8\n",
      "   Directorio de salida: kaggle_predictions_advanced\n",
      "   Optimizaci√≥n de hiperpar√°metros: True\n",
      "   Ensemble stacking: True\n",
      "   Modelos de series de tiempo: True\n",
      "üåô INICIANDO PIPELINE NOCTURNO AVANZADO\n",
      "‚è∞ Hora de inicio: 2025-06-04 08:33:34\n",
      "üöÄ INICIANDO PIPELINE AVANZADO NOCTURNO\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ CARGA DE DATOS\n",
      "üîÑ Cargando datasets...\n",
      "‚úÖ Sales: 2,945,818 filas, 7 columnas\n",
      "‚úÖ Stocks: 13,691 filas, 3 columnas\n",
      "‚úÖ Products: 1,251 productos\n",
      "‚úÖ Products to predict: 780 productos\n",
      "üìÖ Rango de fechas: 2017-01-01 00:00:00 a 2019-12-01 00:00:00\n",
      "üá¶üá∑ Cargando datos del IPC INDEC...\n",
      "‚úÖ IPC INDEC procesado: 48 per√≠odos\n",
      "üìä Rango IPC: 1.13 a 5.68\n",
      "\n",
      "2Ô∏è‚É£ FEATURE ENGINEERING AVANZADO\n",
      "üîß Creando features avanzadas v2...\n",
      "   ‚Üí Despu√©s de merge productos: 2945818 filas\n",
      "   ‚Üí Despu√©s de merge stocks: 2945818 filas\n",
      "   ‚Üí Despu√©s de merge IPC: 2945818 filas\n",
      "üìä Aplicando feature engineering avanzado...\n",
      "üîÑ Creando lags extendidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando lags: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:06<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creando rolling features extendidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling windows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [1:44:50<00:00, 786.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Creando features de tendencia...\n",
      "üìä Creando features estad√≠sticas...\n",
      "üè∑Ô∏è Creando agregaciones categ√≥ricas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agregaciones categ√≥ricas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Creando features de interacci√≥n...\n",
      "üì¶ Creando features de stock avanzadas...\n",
      "üí∞ Creando features de IPC avanzadas...\n",
      "üèÜ Creando features de ranking...\n",
      "üîÑ Creando features de frecuencia...\n",
      "\n",
      "üí• ERROR CR√çTICO: '>' not supported between instances of 'Rolling' and 'int'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_1640\\3219245548.py\", line 1436, in <module>\n",
      "    results = run_advanced_nocturnal_pipeline()\n",
      "  File \"C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_1640\\3219245548.py\", line 939, in run_advanced_nocturnal_pipeline\n",
      "    data = create_advanced_features_v2(sales, stocks, product_info, indec_data)\n",
      "  File \"C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_1640\\3219245548.py\", line 404, in create_advanced_features_v2\n",
      "    data[f'consistency_{window}'] = rolling_group.transform(\n",
      "                                    ~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        lambda x: (x.rolling(window, min_periods=1) > 0).mean()\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\m\\Documents\\Labo3\\labo3-2025r\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 517, in transform\n",
      "    return self._transform(\n",
      "           ~~~~~~~~~~~~~~~^\n",
      "        func, *args, engine=engine, engine_kwargs=engine_kwargs, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\m\\Documents\\Labo3\\labo3-2025r\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py\", line 2021, in _transform\n",
      "    return self._transform_general(func, engine, engine_kwargs, *args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\m\\Documents\\Labo3\\labo3-2025r\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py\", line 557, in _transform_general\n",
      "    res = func(group, *args, **kwargs)\n",
      "  File \"C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_1640\\3219245548.py\", line 405, in <lambda>\n",
      "    lambda x: (x.rolling(window, min_periods=1) > 0).mean()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: '>' not supported between instances of 'Rolling' and 'int'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è∞ Tiempo total: 6434.7 segundos (107.2 minutos)\n",
      "üåô Fin del pipeline nocturno\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREDICCI√ìN AVANZADA NOCTURNA - M√öLTIPLES MODELOS Y SERIES DE TIEMPO\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Proyecto: Predicci√≥n de Ventas - Competencia Kaggle (Versi√≥n Nocturna Avanzada)\n",
    "Objetivo: Predecir ventas para febrero 2020 (mes +2)\n",
    "Modelos: RF, XGB, LGB, CatBoost, GradientBoosting, ExtraTrees, SVR,\n",
    "         ARIMA, SARIMA, Prophet, AutoArima, Ensemble Avanzado\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORTS Y CONFIGURACI√ìN EXTENDIDA\n",
    "# ============================================================================\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning B√°sico\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                             ExtraTreesRegressor, VotingRegressor)\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import (train_test_split, TimeSeriesSplit, \n",
    "                                   cross_val_score, GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Modelos avanzados\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Series de tiempo\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Prophet para series de tiempo\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Prophet no disponible, se omitir√°\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# AutoML para series de tiempo\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    AUTO_ARIMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Auto-ARIMA no disponible, se omitir√°\")\n",
    "    AUTO_ARIMA_AVAILABLE = False\n",
    "\n",
    "# Utilidades\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "print(\"üì¶ Librer√≠as cargadas exitosamente!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CONFIGURACI√ìN GLOBAL EXTENDIDA\n",
    "# ============================================================================\n",
    "\n",
    "# Configuraci√≥n\n",
    "RANDOM_STATE = 42\n",
    "TARGET_DATE = '2020-02-01'\n",
    "VALIDATION_MONTHS = 2\n",
    "OUTPUT_DIR = 'kaggle_predictions_advanced'\n",
    "N_JOBS = -1  # Usar todos los cores disponibles\n",
    "\n",
    "# Crear directorio de salida\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configuraci√≥n de modelos\n",
    "MODEL_CONFIG = {\n",
    "    'use_hyperparameter_tuning': True,\n",
    "    'use_ensemble_stacking': True,\n",
    "    'use_time_series_models': True,\n",
    "    'use_prophet': PROPHET_AVAILABLE,\n",
    "    'use_auto_arima': AUTO_ARIMA_AVAILABLE,\n",
    "    'cross_validation_folds': 3,\n",
    "    'optuna_trials': 100\n",
    "}\n",
    "\n",
    "# Configuraci√≥n visual\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"üñ•Ô∏è Configuraci√≥n del sistema:\")\n",
    "print(f\"   CPUs disponibles: {cpu_count()}\")\n",
    "print(f\"   Directorio de salida: {OUTPUT_DIR}\")\n",
    "print(f\"   Optimizaci√≥n de hiperpar√°metros: {MODEL_CONFIG['use_hyperparameter_tuning']}\")\n",
    "print(f\"   Ensemble stacking: {MODEL_CONFIG['use_ensemble_stacking']}\")\n",
    "print(f\"   Modelos de series de tiempo: {MODEL_CONFIG['use_time_series_models']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FUNCIONES DE CARGA Y PREPARACI√ìN (MEJORADAS)\n",
    "# ============================================================================\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Carga y prepara todos los datasets con validaciones mejoradas\"\"\"\n",
    "    print(\"üîÑ Cargando datasets...\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar datasets con validaciones\n",
    "        sales = pd.read_csv(\"../datasets/sell-in.txt\", sep=\"\\t\", dtype={\"periodo\": str})\n",
    "        stocks = pd.read_csv(\"../datasets/tb_stocks.txt\", sep=\"\\t\", dtype={\"periodo\": str}) \n",
    "        product_info = pd.read_csv(\"../datasets/tb_productos.txt\", sep=\"\\t\")\n",
    "        products_to_predict = pd.read_csv('../datasets/product_id_apredecir201912.txt')\n",
    "        \n",
    "        # Validaciones b√°sicas\n",
    "        assert not sales.empty, \"Sales dataset est√° vac√≠o\"\n",
    "        assert not products_to_predict.empty, \"Products to predict est√° vac√≠o\"\n",
    "        \n",
    "        # Convertir periodos con validaci√≥n\n",
    "        sales['periodo'] = pd.to_datetime(sales['periodo'], format='%Y%m', errors='coerce')\n",
    "        stocks['periodo'] = pd.to_datetime(stocks['periodo'], format='%Y%m', errors='coerce')\n",
    "        \n",
    "        # Eliminar fechas inv√°lidas\n",
    "        sales = sales.dropna(subset=['periodo'])\n",
    "        stocks = stocks.dropna(subset=['periodo'])\n",
    "        \n",
    "        print(f\"‚úÖ Sales: {sales.shape[0]:,} filas, {sales.shape[1]} columnas\")\n",
    "        print(f\"‚úÖ Stocks: {stocks.shape[0]:,} filas, {stocks.shape[1]} columnas\") \n",
    "        print(f\"‚úÖ Products: {product_info.shape[0]:,} productos\")\n",
    "        print(f\"‚úÖ Products to predict: {len(products_to_predict):,} productos\")\n",
    "        print(f\"üìÖ Rango de fechas: {sales['periodo'].min()} a {sales['periodo'].max()}\")\n",
    "        \n",
    "        return sales, stocks, product_info, products_to_predict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando datos: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_indec_data():\n",
    "    \"\"\"Carga y procesa datos del INDEC con limpieza mejorada\"\"\"\n",
    "    print(\"üá¶üá∑ Cargando datos del IPC INDEC...\")\n",
    "    \n",
    "    try:\n",
    "        INDEC = pd.read_csv('../datasets/serie_ipc_aperturas.csv', sep=';', encoding='latin-1')\n",
    "        INDEC['periodo'] = INDEC['periodo'].astype(str)\n",
    "        \n",
    "        INDEC_filtered = INDEC[\n",
    "            (INDEC['periodo'] >= '201701') & \n",
    "            (INDEC['periodo'] <= '202012') & \n",
    "            (INDEC['Descripcion_aperturas'] == 'Nivel general')\n",
    "        ].copy()\n",
    "        \n",
    "        def clean_and_convert(value):\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Limpiar m√∫ltiples formatos posibles\n",
    "                    cleaned = value.replace(',', '.').replace(' ', '')\n",
    "                    # Extraer n√∫meros\n",
    "                    import re\n",
    "                    numbers = re.findall(r'-?\\d+\\.?\\d*', cleaned)\n",
    "                    if numbers:\n",
    "                        return float(numbers[0])\n",
    "                    return np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "            return float(value) if pd.notna(value) else np.nan\n",
    "        \n",
    "        INDEC_filtered['v_m_IPC'] = INDEC_filtered['v_m_IPC'].apply(clean_and_convert)\n",
    "        INDEC_filtered = INDEC_filtered.dropna(subset=['v_m_IPC'])\n",
    "        INDEC_processed = INDEC_filtered.groupby('periodo')['v_m_IPC'].mean().reset_index()\n",
    "        INDEC_processed['periodo'] = pd.to_datetime(INDEC_processed['periodo'], format='%Y%m')\n",
    "        \n",
    "        # Suavizado de valores extremos\n",
    "        q1, q3 = INDEC_processed['v_m_IPC'].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Cap outliers\n",
    "        INDEC_processed['v_m_IPC'] = np.clip(INDEC_processed['v_m_IPC'], lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"‚úÖ IPC INDEC procesado: {len(INDEC_processed)} per√≠odos\")\n",
    "        print(f\"üìä Rango IPC: {INDEC_processed['v_m_IPC'].min():.2f} a {INDEC_processed['v_m_IPC'].max():.2f}\")\n",
    "        \n",
    "        return INDEC_processed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error procesando INDEC: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_advanced_features_v2(sales, stocks, product_info, indec_data):\n",
    "    \"\"\"Versi√≥n mejorada de feature engineering con m√°s features\"\"\"\n",
    "    print(\"üîß Creando features avanzadas v2...\")\n",
    "    \n",
    "    # Merge inicial con validaciones\n",
    "    data = sales.copy()\n",
    "    initial_shape = data.shape[0]\n",
    "    \n",
    "    # Agregar informaci√≥n de productos\n",
    "    if product_info is not None:\n",
    "        data = data.merge(product_info, on='product_id', how='left')\n",
    "        print(f\"   ‚Üí Despu√©s de merge productos: {data.shape[0]} filas\")\n",
    "    \n",
    "    # Agregar stocks\n",
    "    if stocks is not None:\n",
    "        data = data.merge(stocks, on=['periodo', 'product_id'], how='left')\n",
    "        print(f\"   ‚Üí Despu√©s de merge stocks: {data.shape[0]} filas\")\n",
    "    \n",
    "    # Agregar IPC\n",
    "    if indec_data is not None:\n",
    "        data = data.merge(indec_data, on='periodo', how='left')\n",
    "        print(f\"   ‚Üí Despu√©s de merge IPC: {data.shape[0]} filas\")\n",
    "    \n",
    "    # FEATURE ENGINEERING COMPLETO Y MEJORADO\n",
    "    print(\"üìä Aplicando feature engineering avanzado...\")\n",
    "    \n",
    "    # 1. Features temporales extendidas\n",
    "    data['year'] = data['periodo'].dt.year\n",
    "    data['month'] = data['periodo'].dt.month\n",
    "    data['quarter'] = data['periodo'].dt.quarter\n",
    "    data['day_of_year'] = data['periodo'].dt.dayofyear\n",
    "    data['week_of_year'] = data['periodo'].dt.isocalendar().week\n",
    "    data['is_weekend'] = data['periodo'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    data['is_month_start'] = data['periodo'].dt.is_month_start.astype(int)\n",
    "    data['is_month_end'] = data['periodo'].dt.is_month_end.astype(int)\n",
    "    data['is_quarter_start'] = data['periodo'].dt.is_quarter_start.astype(int)\n",
    "    data['is_quarter_end'] = data['periodo'].dt.is_quarter_end.astype(int)\n",
    "    data['days_in_month'] = data['periodo'].dt.days_in_month\n",
    "    \n",
    "    # 2. Features de estacionalidad m√∫ltiples\n",
    "    for period in [3, 4, 6, 12]:\n",
    "        data[f'sin_month_{period}'] = np.sin(2 * np.pi * data['month'] / period)\n",
    "        data[f'cos_month_{period}'] = np.cos(2 * np.pi * data['month'] / period)\n",
    "    \n",
    "    # Features c√≠clicas para quarter\n",
    "    data['sin_quarter'] = np.sin(2 * np.pi * data['quarter'] / 4)\n",
    "    data['cos_quarter'] = np.cos(2 * np.pi * data['quarter'] / 4)\n",
    "    \n",
    "    # 3. Lags extendidos con m√∫ltiples targets\n",
    "    print(\"üîÑ Creando lags extendidos...\")\n",
    "    lag_periods = [1, 2, 3, 4, 5, 6, 9, 12, 15, 18, 24, 36]\n",
    "    \n",
    "    for lag in tqdm(lag_periods, desc=\"Creando lags\"):\n",
    "        data[f'sales_lag_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].shift(lag)\n",
    "        \n",
    "    # Lags por producto solamente (agregados)\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'product_sales_lag_{lag}'] = data.groupby('product_id')['tn'].shift(lag)\n",
    "\n",
    "    # 4. Rolling windows extendidos\n",
    "    print(\"üîÑ Creando rolling features extendidos...\")\n",
    "    windows = [2, 3, 4, 6, 9, 12, 18, 24]\n",
    "    operations = ['mean', 'std', 'min', 'max', 'median', 'skew']\n",
    "    \n",
    "    for window in tqdm(windows, desc=\"Rolling windows\"):\n",
    "        # Por producto-cliente\n",
    "        rolling_group = data.groupby(['product_id', 'customer_id'])['tn']\n",
    "        data[f'sales_rolling_mean_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        data[f'sales_rolling_std_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "        data[f'sales_rolling_min_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "        data[f'sales_rolling_max_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        data[f'sales_rolling_median_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "        \n",
    "        # EWMA con diferentes alphas\n",
    "        for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "            data[f'sales_ewma_{window}_alpha_{str(alpha).replace(\".\", \"\")}'] = rolling_group.transform(\n",
    "                lambda x: x.ewm(alpha=alpha, min_periods=1).mean()\n",
    "            )\n",
    "\n",
    "    # 5. Features de tendencia y momentum\n",
    "    print(\"üìà Creando features de tendencia...\")\n",
    "    \n",
    "    # Diferencias y cambios porcentuales\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'sales_diff_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].diff(periods=lag)\n",
    "        data[f'sales_pct_change_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].pct_change(periods=lag)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    for short, long in [(3, 6), (6, 12), (12, 24)]:\n",
    "        data[f'momentum_{short}_{long}'] = (\n",
    "            data[f'sales_rolling_mean_{short}'] - data[f'sales_rolling_mean_{long}']\n",
    "        )\n",
    "        \n",
    "    # Acceleration (second derivative)\n",
    "    data['sales_acceleration'] = data.groupby(['product_id', 'customer_id'])['tn'].diff().diff()\n",
    "    \n",
    "    # 6. Features estad√≠sticas avanzadas\n",
    "    print(\"üìä Creando features estad√≠sticas...\")\n",
    "    \n",
    "    # Ratios y volatilidad\n",
    "    for window in [3, 6, 12]:\n",
    "        data[f'cv_{window}'] = data[f'sales_rolling_std_{window}'] / (data[f'sales_rolling_mean_{window}'] + 1e-8)\n",
    "        data[f'zscore_{window}'] = (data['tn'] - data[f'sales_rolling_mean_{window}']) / (data[f'sales_rolling_std_{window}'] + 1e-8)\n",
    "        data[f'range_ratio_{window}'] = (data[f'sales_rolling_max_{window}'] - data[f'sales_rolling_min_{window}']) / (data[f'sales_rolling_mean_{window}'] + 1e-8)\n",
    "    \n",
    "    # 7. Agregaciones categ√≥ricas extendidas\n",
    "    print(\"üè∑Ô∏è Creando agregaciones categ√≥ricas...\")\n",
    "    \n",
    "    categorical_cols = ['product_id', 'customer_id']\n",
    "    if 'brand' in data.columns:\n",
    "        categorical_cols.extend(['brand', 'cat1', 'cat2', 'cat3'])\n",
    "    \n",
    "    aggregations = ['mean', 'std', 'median', 'min', 'max', 'count', 'sum']\n",
    "    \n",
    "    for cat in tqdm(categorical_cols, desc=\"Agregaciones categ√≥ricas\"):\n",
    "        if cat in data.columns:\n",
    "            grouped = data.groupby(cat)['tn']\n",
    "            for agg in aggregations:\n",
    "                try:\n",
    "                    data[f'{cat}_{agg}'] = grouped.transform(agg)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # 8. Features de interacci√≥n avanzadas\n",
    "    print(\"üîó Creando features de interacci√≥n...\")\n",
    "    \n",
    "    # Interacciones temporales\n",
    "    data['sales_month_interaction'] = data['tn'] * data['month']\n",
    "    data['sales_quarter_interaction'] = data['tn'] * data['quarter']\n",
    "    data['sales_year_interaction'] = data['tn'] * (data['year'] - data['year'].min())\n",
    "    \n",
    "    # Interacciones con lags\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'sales_lag_ratio_{lag}'] = data['tn'] / (data[f'sales_lag_{lag}'] + 1e-8)\n",
    "        data[f'sales_lag_diff_{lag}'] = data['tn'] - data[f'sales_lag_{lag}']\n",
    "    \n",
    "    # 9. Features de stock mejoradas\n",
    "    if 'stock_final' in data.columns:\n",
    "        print(\"üì¶ Creando features de stock avanzadas...\")\n",
    "        \n",
    "        data['stock_turnover'] = data['tn'] / (data['stock_final'] + 1e-8)\n",
    "        data['days_of_stock'] = data['stock_final'] / (data['tn'] + 1e-8) * 30\n",
    "        data['stock_ratio'] = data['stock_final'] / (data['stock_final'].mean() + 1e-8)\n",
    "        \n",
    "        # Lags de stock\n",
    "        for lag in [1, 3, 6]:\n",
    "            data[f'stock_lag_{lag}'] = data.groupby(['product_id', 'customer_id'])['stock_final'].shift(lag)\n",
    "            data[f'stock_change_{lag}'] = data['stock_final'] - data[f'stock_lag_{lag}']\n",
    "    \n",
    "    # 10. Features de IPC avanzadas\n",
    "    if 'v_m_IPC' in data.columns:\n",
    "        print(\"üí∞ Creando features de IPC avanzadas...\")\n",
    "        \n",
    "        # Lags de IPC\n",
    "        for lag in [1, 2, 3, 6, 12]:\n",
    "            data[f'ipc_lag_{lag}'] = data['v_m_IPC'].shift(lag)\n",
    "        \n",
    "        # Rolling IPC\n",
    "        for window in [3, 6, 12]:\n",
    "            data[f'ipc_rolling_{window}'] = data['v_m_IPC'].rolling(window, min_periods=1).mean()\n",
    "            data[f'ipc_std_{window}'] = data['v_m_IPC'].rolling(window, min_periods=1).std()\n",
    "        \n",
    "        # Interacciones IPC-ventas\n",
    "        data['sales_ipc_interaction'] = data['tn'] * data['v_m_IPC']\n",
    "        data['sales_ipc_ratio'] = data['tn'] / (data['v_m_IPC'] + 1e-8)\n",
    "        \n",
    "        # Cambios en IPC\n",
    "        data['ipc_change'] = data['v_m_IPC'].diff()\n",
    "        data['ipc_acceleration'] = data['ipc_change'].diff()\n",
    "    \n",
    "    # 11. Features de ranking y percentiles\n",
    "    print(\"üèÜ Creando features de ranking...\")\n",
    "    \n",
    "    # Rankings por per√≠odo\n",
    "    data['product_rank_period'] = data.groupby('periodo')['tn'].rank(pct=True)\n",
    "    data['customer_rank_period'] = data.groupby(['periodo', 'customer_id'])['tn'].rank(pct=True)\n",
    "    \n",
    "    # Rankings hist√≥ricos\n",
    "    data['product_rank_historical'] = data.groupby('product_id')['tn'].rank(pct=True)\n",
    "    data['customer_rank_historical'] = data.groupby('customer_id')['tn'].rank(pct=True)\n",
    "    \n",
    "    # 12. Features de frecuencia y consistencia\n",
    "    print(\"üîÑ Creando features de frecuencia...\")\n",
    "    \n",
    "    # Conteo de per√≠odos activos\n",
    "    data['periods_active'] = data.groupby(['product_id', 'customer_id']).cumcount() + 1\n",
    "    data['total_periods'] = data.groupby(['product_id', 'customer_id'])['periodo'].transform('count')\n",
    "    data['activity_ratio'] = data['periods_active'] / data['total_periods']\n",
    "    \n",
    "    # Consistencia de ventas\n",
    "    for window in [6, 12]:\n",
    "        # Porcentaje de per√≠odos con ventas > 0\n",
    "        rolling_group = data.groupby(['product_id', 'customer_id'])['tn']\n",
    "        data[f'consistency_{window}'] = rolling_group.transform(\n",
    "            lambda x: (x.rolling(window, min_periods=1) > 0).mean()\n",
    "        )\n",
    "    \n",
    "    # 13. Features avanzadas de detecci√≥n de patrones\n",
    "    print(\"üîç Creando features de patrones...\")\n",
    "    \n",
    "    # Detectar picos y valles\n",
    "    for window in [3, 6]:\n",
    "        rolling_mean = data[f'sales_rolling_mean_{window}']\n",
    "        rolling_std = data[f'sales_rolling_std_{window}']\n",
    "        data[f'is_peak_{window}'] = (data['tn'] > rolling_mean + 2 * rolling_std).astype(int)\n",
    "        data[f'is_valley_{window}'] = (data['tn'] < rolling_mean - rolling_std).astype(int)\n",
    "    \n",
    "    # Detectar estacionalidad\n",
    "    data['seasonal_strength'] = np.abs(data['sin_month_12'])\n",
    "    \n",
    "    # 14. Features de target encoding\n",
    "    print(\"üéØ Creando target encoding...\")\n",
    "    \n",
    "    # Target encoding con validaci√≥n cruzada para evitar overfitting\n",
    "    for cat_col in ['cat1', 'cat2', 'cat3', 'brand']:\n",
    "        if cat_col in data.columns:\n",
    "            # Media global como fallback\n",
    "            global_mean = data['tn'].mean()\n",
    "            \n",
    "            # Target encoding suavizado\n",
    "            cat_stats = data.groupby(cat_col)['tn'].agg(['mean', 'count']).reset_index()\n",
    "            cat_stats['smoothed_mean'] = (cat_stats['mean'] * cat_stats['count'] + global_mean * 10) / (cat_stats['count'] + 10)\n",
    "            \n",
    "            # Merge back\n",
    "            data = data.merge(cat_stats[['cat1' if cat_col == 'cat1' else cat_col, 'smoothed_mean']], \n",
    "                            on=cat_col, how='left', suffixes=('', f'_{cat_col}_target'))\n",
    "            data = data.rename(columns={'smoothed_mean': f'{cat_col}_target_encoded'})\n",
    "    \n",
    "    # 15. Limpieza final y validaciones\n",
    "    print(\"üßπ Limpieza final...\")\n",
    "    \n",
    "    # Reemplazar infinitos y valores extremos\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Imputaci√≥n inteligente\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'tn':  # No imputar el target\n",
    "            if data[col].isna().sum() > 0:\n",
    "                # Usar mediana para imputaci√≥n robusta\n",
    "                data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    # Llenar categ√≥ricas\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].fillna('unknown')\n",
    "    \n",
    "    print(f\"‚úÖ Features v2 creadas. Shape final: {data.shape}\")\n",
    "    print(f\"üìä Features num√©ricas: {len(data.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"üìä Features categ√≥ricas: {len(data.select_dtypes(include=['object']).columns)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MODELOS DE SERIES DE TIEMPO\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesModels:\n",
    "    \"\"\"Clase para manejar modelos espec√≠ficos de series de tiempo\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.fitted_models = {}\n",
    "    \n",
    "    def prepare_time_series_data(self, data, product_id):\n",
    "        \"\"\"Prepara datos para series de tiempo de un producto espec√≠fico\"\"\"\n",
    "        product_data = data[data['product_id'] == product_id].copy()\n",
    "        product_data = product_data.sort_values('periodo')\n",
    "        \n",
    "        # Crear series temporal completa (rellenar per√≠odos faltantes)\n",
    "        date_range = pd.date_range(\n",
    "            start=product_data['periodo'].min(),\n",
    "            end=product_data['periodo'].max(),\n",
    "            freq='M'\n",
    "        )\n",
    "        \n",
    "        # Reindexar y rellenar\n",
    "        product_data = product_data.set_index('periodo')\n",
    "        product_data = product_data.reindex(date_range, fill_value=0)\n",
    "        product_data.index.name = 'periodo'\n",
    "        product_data = product_data.reset_index()\n",
    "        \n",
    "        return product_data\n",
    "    \n",
    "    def fit_arima(self, ts_data, order=(1,1,1)):\n",
    "        \"\"\"Ajusta modelo ARIMA\"\"\"\n",
    "        try:\n",
    "            if len(ts_data) < 10:\n",
    "                return None, None\n",
    "                \n",
    "            # Eliminar ceros iniciales si existen\n",
    "            first_nonzero = np.argmax(ts_data > 0)\n",
    "            if first_nonzero > 0:\n",
    "                ts_data = ts_data[first_nonzero:]\n",
    "            \n",
    "            if len(ts_data) < 5:\n",
    "                return None, None\n",
    "            \n",
    "            model = ARIMA(ts_data, order=order)\n",
    "            fitted_model = model.fit()\n",
    "            return fitted_model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_sarima(self, ts_data, order=(1,1,1), seasonal_order=(0,1,1,12)):\n",
    "        \"\"\"Ajusta modelo SARIMA\"\"\"\n",
    "        try:\n",
    "            if len(ts_data) < 24:  # Necesita al menos 2 a√±os para estacionalidad\n",
    "                return None, \"Insufficient data for SARIMA\"\n",
    "                \n",
    "            model = SARIMAX(ts_data, order=order, seasonal_order=seasonal_order)\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            return fitted_model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_prophet(self, data, product_id):\n",
    "        \"\"\"Ajusta modelo Prophet\"\"\"\n",
    "        if not PROPHET_AVAILABLE:\n",
    "            return None, \"Prophet not available\"\n",
    "            \n",
    "        try:\n",
    "            product_data = self.prepare_time_series_data(data, product_id)\n",
    "            \n",
    "            if len(product_data) < 10:\n",
    "                return None, \"Insufficient data\"\n",
    "            \n",
    "            # Preparar datos para Prophet\n",
    "            prophet_data = pd.DataFrame({\n",
    "                'ds': product_data['periodo'],\n",
    "                'y': product_data['tn']\n",
    "            })\n",
    "            \n",
    "            # Configurar Prophet\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            \n",
    "            # Agregar regresores externos si est√°n disponibles\n",
    "            if 'v_m_IPC' in product_data.columns:\n",
    "                model.add_regressor('ipc')\n",
    "                prophet_data['ipc'] = product_data['v_m_IPC'].fillna(method='ffill')\n",
    "            \n",
    "            model.fit(prophet_data)\n",
    "            return model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_auto_arima(self, ts_data):\n",
    "        \"\"\"Ajusta Auto-ARIMA\"\"\"\n",
    "        if not AUTO_ARIMA_AVAILABLE:\n",
    "            return None, \"Auto-ARIMA not available\"\n",
    "            \n",
    "        try:\n",
    "            if len(ts_data) < 10:\n",
    "                return None, \"Insufficient data\"\n",
    "            \n",
    "            model = auto_arima(\n",
    "                ts_data,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=3, max_q=3,\n",
    "                seasonal=True,\n",
    "                start_P=0, start_Q=0,\n",
    "                max_P=2, max_Q=2,\n",
    "                m=12,\n",
    "                stepwise=True,\n",
    "                suppress_warnings=True,\n",
    "                error_action='ignore'\n",
    "            )\n",
    "            \n",
    "            return model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def predict_time_series(self, data, products_list, target_date='2020-02-01'):\n",
    "        \"\"\"Genera predicciones usando modelos de series de tiempo\"\"\"\n",
    "        print(\"‚è∞ Generando predicciones con modelos de series de tiempo...\")\n",
    "        \n",
    "        predictions = {}\n",
    "        errors = {}\n",
    "        \n",
    "        for product_id in tqdm(products_list, desc=\"Time Series Predictions\"):\n",
    "            try:\n",
    "                product_data = self.prepare_time_series_data(data, product_id)\n",
    "                product_sales = data[data['product_id'] == product_id]['tn'].values\n",
    "                \n",
    "                if len(product_sales) == 0:\n",
    "                    predictions[product_id] = {'arima': 0, 'sarima': 0, 'prophet': 0, 'auto_arima': 0}\n",
    "                    continue\n",
    "                \n",
    "                pred_dict = {}\n",
    "                \n",
    "                # ARIMA\n",
    "                arima_model, arima_error = self.fit_arima(product_sales)\n",
    "                if arima_model is not None:\n",
    "                    try:\n",
    "                        forecast = arima_model.forecast(steps=1)\n",
    "                        pred_dict['arima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # SARIMA\n",
    "                sarima_model, sarima_error = self.fit_sarima(product_sales)\n",
    "                if sarima_model is not None:\n",
    "                    try:\n",
    "                        forecast = sarima_model.forecast(steps=1)\n",
    "                        pred_dict['sarima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['sarima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['sarima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # Prophet\n",
    "                prophet_model, prophet_error = self.fit_prophet(data, product_id)\n",
    "                if prophet_model is not None:\n",
    "                    try:\n",
    "                        future = prophet_model.make_future_dataframe(periods=1, freq='M')\n",
    "                        if 'ipc' in future.columns:\n",
    "                            future['ipc'] = future['ipc'].fillna(method='ffill')\n",
    "                        forecast = prophet_model.predict(future)\n",
    "                        pred_dict['prophet'] = max(0, forecast['yhat'].iloc[-1])\n",
    "                    except:\n",
    "                        pred_dict['prophet'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['prophet'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # Auto-ARIMA\n",
    "                auto_arima_model, auto_arima_error = self.fit_auto_arima(product_sales)\n",
    "                if auto_arima_model is not None:\n",
    "                    try:\n",
    "                        forecast = auto_arima_model.predict(n_periods=1)\n",
    "                        pred_dict['auto_arima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['auto_arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['auto_arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                predictions[product_id] = pred_dict\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors[product_id] = str(e)\n",
    "                predictions[product_id] = {'arima': 0, 'sarima': 0, 'prophet': 0, 'auto_arima': 0}\n",
    "        \n",
    "        return predictions, errors\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENSEMBLE AVANZADO Y STACKING\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedEnsemble:\n",
    "    \"\"\"Ensemble avanzado con stacking multinivel\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.level1_models = {}\n",
    "        self.level2_models = {}\n",
    "        self.stacking_model = None\n",
    "        self.model_weights = {}\n",
    "        \n",
    "    def create_level1_models(self):\n",
    "        \"\"\"Crea modelos de primer nivel\"\"\"\n",
    "        models = {\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'catboost': CatBoostRegressor(\n",
    "                iterations=300,\n",
    "                depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                random_state=self.random_state,\n",
    "                verbose=False\n",
    "            ),\n",
    "            'gb': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'extra_trees': ExtraTreesRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'ridge': Ridge(alpha=1.0),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        self.level1_models = models\n",
    "        return models\n",
    "    \n",
    "    def create_stacking_model(self):\n",
    "        \"\"\"Crea modelo de stacking de segundo nivel\"\"\"\n",
    "        stacking_models = [\n",
    "            ('ridge', Ridge(alpha=0.1)),\n",
    "            ('lasso', Lasso(alpha=0.01, random_state=self.random_state)),\n",
    "            ('xgb_stack', xgb.XGBRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        self.stacking_model = VotingRegressor(\n",
    "            estimators=stacking_models,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return self.stacking_model\n",
    "    \n",
    "    def fit_ensemble(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrena ensemble con validaci√≥n cruzada\"\"\"\n",
    "        print(\"üéØ Entrenando ensemble avanzado...\")\n",
    "        \n",
    "        # Crear modelos\n",
    "        self.create_level1_models()\n",
    "        \n",
    "        # Entrenar modelos de nivel 1\n",
    "        level1_predictions_train = np.zeros((len(X_train), len(self.level1_models)))\n",
    "        level1_predictions_val = np.zeros((len(X_val), len(self.level1_models)))\n",
    "        \n",
    "        model_scores = {}\n",
    "        \n",
    "        for i, (name, model) in enumerate(tqdm(self.level1_models.items(), desc=\"Training Level 1\")):\n",
    "            try:\n",
    "                # Entrenar modelo\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predicciones\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_val = model.predict(X_val)\n",
    "                \n",
    "                level1_predictions_train[:, i] = pred_train\n",
    "                level1_predictions_val[:, i] = pred_val\n",
    "                \n",
    "                # Evaluar\n",
    "                val_mae = mean_absolute_error(y_val, pred_val)\n",
    "                val_rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "                val_r2 = r2_score(y_val, pred_val)\n",
    "                \n",
    "                model_scores[name] = {\n",
    "                    'mae': val_mae,\n",
    "                    'rmse': val_rmse,\n",
    "                    'r2': val_r2\n",
    "                }\n",
    "                \n",
    "                print(f\"   ‚úÖ {name}: MAE={val_mae:.3f}, RMSE={val_rmse:.3f}, R2={val_r2:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error en {name}: {e}\")\n",
    "                level1_predictions_train[:, i] = y_train.mean()\n",
    "                level1_predictions_val[:, i] = y_train.mean()\n",
    "        \n",
    "        # Entrenar modelo de stacking\n",
    "        self.create_stacking_model()\n",
    "        self.stacking_model.fit(level1_predictions_train, y_train)\n",
    "        \n",
    "        # Predicci√≥n final del ensemble\n",
    "        final_pred = self.stacking_model.predict(level1_predictions_val)\n",
    "        \n",
    "        # Evaluar ensemble\n",
    "        ensemble_mae = mean_absolute_error(y_val, final_pred)\n",
    "        ensemble_rmse = np.sqrt(mean_squared_error(y_val, final_pred))\n",
    "        ensemble_r2 = r2_score(y_val, final_pred)\n",
    "        \n",
    "        print(f\"üèÜ ENSEMBLE FINAL: MAE={ensemble_mae:.3f}, RMSE={ensemble_rmse:.3f}, R2={ensemble_r2:.3f}\")\n",
    "        \n",
    "        return model_scores, {\n",
    "            'mae': ensemble_mae,\n",
    "            'rmse': ensemble_rmse,\n",
    "            'r2': ensemble_r2\n",
    "        }\n",
    "    \n",
    "    def predict_ensemble(self, X_test):\n",
    "        \"\"\"Genera predicciones del ensemble\"\"\"\n",
    "        level1_predictions = np.zeros((len(X_test), len(self.level1_models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(self.level1_models.items()):\n",
    "            try:\n",
    "                level1_predictions[:, i] = model.predict(X_test)\n",
    "            except:\n",
    "                level1_predictions[:, i] = 0\n",
    "        \n",
    "        return self.stacking_model.predict(level1_predictions)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS CON OPTUNA\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train, X_val, y_val, model_type='xgb', n_trials=100):\n",
    "    \"\"\"Optimiza hiperpar√°metros usando Optuna\"\"\"\n",
    "    print(f\"üîß Optimizando {model_type} con Optuna...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_type == 'xgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            \n",
    "        elif model_type == 'lgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "                'depth': trial.suggest_int('depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'verbose': False\n",
    "            }\n",
    "            model = CatBoostRegressor(**params)\n",
    "        \n",
    "        elif model_type == 'rf':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Modelo {model_type} no soportado\")\n",
    "        \n",
    "        # Entrenar y evaluar\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        \n",
    "        return rmse\n",
    "    \n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   üèÜ Mejor RMSE: {study.best_value:.4f}\")\n",
    "    print(f\"   üìä Mejores par√°metros: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# ============================================================================\n",
    "# 7. PIPELINE PRINCIPAL EXTENDIDO\n",
    "# ============================================================================\n",
    "\n",
    "def run_advanced_nocturnal_pipeline():\n",
    "    \"\"\"Pipeline principal con todos los modelos y optimizaciones\"\"\"\n",
    "    print(\"üöÄ INICIANDO PIPELINE AVANZADO NOCTURNO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Carga de datos\n",
    "    print(\"\\n1Ô∏è‚É£ CARGA DE DATOS\")\n",
    "    sales, stocks, product_info, products_to_predict = load_and_prepare_data()\n",
    "    if sales is None:\n",
    "        print(\"‚ùå Error cargando datos principales\")\n",
    "        return\n",
    "    \n",
    "    indec_data = load_indec_data()\n",
    "    \n",
    "    # 2. Feature Engineering Avanzado\n",
    "    print(\"\\n2Ô∏è‚É£ FEATURE ENGINEERING AVANZADO\")\n",
    "    data = create_advanced_features_v2(sales, stocks, product_info, indec_data)\n",
    "    \n",
    "    # 3. Agregaci√≥n y preparaci√≥n\n",
    "    print(\"\\n3Ô∏è‚É£ AGREGACI√ìN DE DATOS\")\n",
    "    print(\"üìä Agregando datos por producto-cliente...\")\n",
    "    \n",
    "    # Filtrar datos hasta diciembre 2019\n",
    "    data_filtered = data[data['periodo'] <= '2019-12-01'].copy()\n",
    "    \n",
    "    # Agregaci√≥n mensual\n",
    "    agg_data = data_filtered.groupby(['product_id', 'customer_id', 'periodo']).agg({\n",
    "        'tn': 'sum',\n",
    "        **{col: 'first' for col in data_filtered.columns if col not in ['tn', 'product_id', 'customer_id', 'periodo']}\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"‚úÖ Datos agregados: {agg_data.shape}\")\n",
    "    \n",
    "    # 4. Splits temporales\n",
    "    print(\"\\n4Ô∏è‚É£ CREACI√ìN DE SPLITS TEMPORALES\")\n",
    "    cutoff_date = pd.to_datetime('2019-10-01')\n",
    "    \n",
    "    train_data = agg_data[agg_data['periodo'] <= cutoff_date]\n",
    "    val_data = agg_data[agg_data['periodo'] > cutoff_date]\n",
    "    \n",
    "    print(f\"üìä Training: {len(train_data):,} registros (hasta {cutoff_date.strftime('%Y-%m')})\")\n",
    "    print(f\"üìä Validation: {len(val_data):,} registros (desde {cutoff_date.strftime('%Y-%m')})\")\n",
    "    \n",
    "    # Preparar features\n",
    "    feature_cols = [col for col in agg_data.columns if col not in ['tn', 'periodo', 'product_id', 'customer_id']]\n",
    "    \n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['tn']\n",
    "    X_val = val_data[feature_cols]\n",
    "    y_val = val_data['tn']\n",
    "    \n",
    "    # Handling missing values y encoding\n",
    "    print(\"üîß Procesando features...\")\n",
    "    \n",
    "    # Separar num√©ricas y categ√≥ricas\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', RobustScaler())\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Ajustar preprocessor\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    print(f\"‚úÖ Features procesadas: {X_train_processed.shape[1]} dimensiones\")\n",
    "    \n",
    "    # 5. Modelos de Machine Learning\n",
    "    print(\"\\n5Ô∏è‚É£ EVALUACI√ìN DE MODELOS DE ML\")\n",
    "    \n",
    "    models_to_test = {\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, \n",
    "            n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostRegressor(\n",
    "            iterations=300, depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=RANDOM_STATE, verbose=False\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingRegressor(\n",
    "            n_estimators=200, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        'ExtraTrees': ExtraTreesRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=0.1, random_state=RANDOM_STATE),\n",
    "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    ml_results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    for name, model in tqdm(models_to_test.items(), desc=\"Evaluando modelos ML\"):\n",
    "        try:\n",
    "            # Entrenar\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Predecir\n",
    "            y_pred = model.predict(X_val_processed)\n",
    "            \n",
    "            # M√©tricas\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred) * 100\n",
    "            \n",
    "            ml_results[name] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2,\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            \n",
    "            best_models[name] = model\n",
    "            \n",
    "            print(f\"‚úÖ {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}, MAPE: {mape:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error en {name}: {e}\")\n",
    "    \n",
    "    # 6. Optimizaci√≥n de hiperpar√°metros (para los mejores modelos)\n",
    "    if MODEL_CONFIG['use_hyperparameter_tuning']:\n",
    "        print(\"\\n6Ô∏è‚É£ OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "        \n",
    "        # Seleccionar top 3 modelos\n",
    "        top_models = sorted(ml_results.items(), key=lambda x: x[1]['RMSE'])[:3]\n",
    "        optimized_models = {}\n",
    "        \n",
    "        for model_name, _ in top_models:\n",
    "            if model_name.lower() in ['xgboost', 'lightgbm', 'catboost']:\n",
    "                print(f\"üîß Optimizando {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    best_params = optimize_hyperparameters(\n",
    "                        X_train_processed, y_train, X_val_processed, y_val,\n",
    "                        model_type=model_name.lower().replace('boost', 'b').replace('gradient', 'gb'),\n",
    "                        n_trials=MODEL_CONFIG['optuna_trials']\n",
    "                    )\n",
    "                    \n",
    "                    # Crear modelo optimizado\n",
    "                    if 'xg' in model_name.lower():\n",
    "                        optimized_model = xgb.XGBRegressor(**best_params)\n",
    "                    elif 'light' in model_name.lower():\n",
    "                        optimized_model = lgb.LGBMRegressor(**best_params)\n",
    "                    elif 'cat' in model_name.lower():\n",
    "                        optimized_model = CatBoostRegressor(**best_params)\n",
    "                    \n",
    "                    # Evaluar modelo optimizado\n",
    "                    optimized_model.fit(X_train_processed, y_train)\n",
    "                    y_pred_opt = optimized_model.predict(X_val_processed)\n",
    "                    \n",
    "                    mae_opt = mean_absolute_error(y_val, y_pred_opt)\n",
    "                    rmse_opt = np.sqrt(mean_squared_error(y_val, y_pred_opt))\n",
    "                    r2_opt = r2_score(y_val, y_pred_opt)\n",
    "                    mape_opt = mean_absolute_percentage_error(y_val, y_pred_opt) * 100\n",
    "                    \n",
    "                    optimized_models[f\"{model_name}_Optimized\"] = {\n",
    "                        'model': optimized_model,\n",
    "                        'MAE': mae_opt,\n",
    "                        'RMSE': rmse_opt,\n",
    "                        'R2': r2_opt,\n",
    "                        'MAPE': mape_opt\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ {model_name} Optimizado - MAE: {mae_opt:.2f}, RMSE: {rmse_opt:.2f}, R2: {r2_opt:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error optimizando {model_name}: {e}\")\n",
    "    \n",
    "    # 7. Modelos de Series de Tiempo\n",
    "    if MODEL_CONFIG['use_time_series_models']:\n",
    "        print(\"\\n7Ô∏è‚É£ MODELOS DE SERIES DE TIEMPO\")\n",
    "        \n",
    "        ts_models = TimeSeriesModels(random_state=RANDOM_STATE)\n",
    "        products_list = products_to_predict.iloc[:, 0].tolist()[:50]  # Limitamos a 50 para prueba nocturna\n",
    "        \n",
    "        ts_predictions, ts_errors = ts_models.predict_time_series(\n",
    "            data_filtered, products_list, TARGET_DATE\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Predicciones de series de tiempo completadas para {len(ts_predictions)} productos\")\n",
    "        if ts_errors:\n",
    "            print(f\"‚ö†Ô∏è Errores en {len(ts_errors)} productos\")\n",
    "    \n",
    "    # 8. Ensemble Avanzado\n",
    "    if MODEL_CONFIG['use_ensemble_stacking']:\n",
    "        print(\"\\n8Ô∏è‚É£ ENSEMBLE AVANZADO\")\n",
    "        \n",
    "        ensemble = AdvancedEnsemble(random_state=RANDOM_STATE)\n",
    "        model_scores, ensemble_score = ensemble.fit_ensemble(\n",
    "            X_train_processed, y_train, X_val_processed, y_val\n",
    "        )\n",
    "        \n",
    "        # Agregar ensemble a resultados\n",
    "        ml_results['AdvancedEnsemble'] = ensemble_score\n",
    "        best_models['AdvancedEnsemble'] = ensemble\n",
    "    \n",
    "    # 9. Selecci√≥n del mejor modelo\n",
    "    print(\"\\n9Ô∏è‚É£ SELECCI√ìN DEL MEJOR MODELO\")\n",
    "    \n",
    "    # Combinar resultados normales y optimizados\n",
    "    all_results = ml_results.copy()\n",
    "    for name, result in optimized_models.items():\n",
    "        all_results[name] = {\n",
    "            'MAE': result['MAE'],\n",
    "            'RMSE': result['RMSE'],\n",
    "            'R2': result['R2'],\n",
    "            'MAPE': result['MAPE']\n",
    "        }\n",
    "        best_models[name] = result['model']\n",
    "    \n",
    "    # Crear DataFrame de resultados\n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    results_df = results_df.sort_values('RMSE')\n",
    "    \n",
    "    print(\"\\nüìä RESUMEN DE RESULTADOS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results_df.round(3))\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_model_name = results_df.index[0]\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\nüèÜ MEJOR MODELO: {best_model_name}\")\n",
    "    print(f\"   RMSE: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "    print(f\"   MAE: {results_df.loc[best_model_name, 'MAE']:.3f}\")\n",
    "    print(f\"   R2: {results_df.loc[best_model_name, 'R2']:.3f}\")\n",
    "    \n",
    "    # 10. Reentrenamiento con datos completos\n",
    "    print(\"\\nüîü REENTRENAMIENTO CON DATOS COMPLETOS\")\n",
    "    print(\"üîÑ Reentrenando el mejor modelo con todos los datos disponibles...\")\n",
    "    \n",
    "    # Combinar train y validation\n",
    "    X_full = np.vstack([X_train_processed, X_val_processed])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Reentrenar\n",
    "    best_model.fit(X_full, y_full)\n",
    "    \n",
    "    # 11. Predicciones finales\n",
    "    print(\"\\n1Ô∏è‚É£1Ô∏è‚É£ PREDICCIONES FINALES\")\n",
    "    print(f\"üéØ Generando predicciones para {TARGET_DATE}...\")\n",
    "    \n",
    "    # Preparar datos para predicci√≥n\n",
    "    products_list = products_to_predict.iloc[:, 0].unique()\n",
    "    predictions_final = []\n",
    "    \n",
    "    for product_id in tqdm(products_list, desc=\"Generando predicciones\"):\n",
    "        try:\n",
    "            # Obtener √∫ltimo per√≠odo del producto\n",
    "            product_data = agg_data[agg_data['product_id'] == product_id]\n",
    "            \n",
    "            if len(product_data) == 0:\n",
    "                # Usar predicci√≥n por defecto\n",
    "                predictions_final.append({\n",
    "                    'product_id': product_id,\n",
    "                    'prediction': 0,\n",
    "                    'method': 'default'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Tomar el √∫ltimo registro disponible como base\n",
    "            last_record = product_data.iloc[-1:][feature_cols]\n",
    "            \n",
    "            # Procesar features\n",
    "            last_record_processed = preprocessor.transform(last_record)\n",
    "            \n",
    "            # Predecir\n",
    "            if best_model_name == 'AdvancedEnsemble':\n",
    "                pred = best_model.predict_ensemble(last_record_processed)[0]\n",
    "            else:\n",
    "                pred = best_model.predict(last_record_processed)[0]\n",
    "            \n",
    "            predictions_final.append({\n",
    "                'product_id': product_id,\n",
    "                'prediction': max(0, pred),  # No predicciones negativas\n",
    "                'method': best_model_name\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            predictions_final.append({\n",
    "                'product_id': product_id,\n",
    "                'prediction': 0,\n",
    "                'method': 'error'\n",
    "            })\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    predictions_df = pd.DataFrame(predictions_final)\n",
    "    \n",
    "    print(f\"‚úÖ Predicciones completadas:\")\n",
    "    print(f\"   Total: {len(predictions_df)}\")\n",
    "    print(f\"   Con modelo: {len(predictions_df[predictions_df['method'] != 'error'])}\")\n",
    "    print(f\"   Errores: {len(predictions_df[predictions_df['method'] == 'error'])}\")\n",
    "    \n",
    "    # 12. Guardar resultados\n",
    "    print(\"\\n1Ô∏è‚É£2Ô∏è‚É£ GUARDANDO RESULTADOS\")\n",
    "    \n",
    "    # Crear submission\n",
    "    # Crear submission\n",
    "    submission = pd.DataFrame({\n",
    "        'product_id': predictions_df['product_id'],\n",
    "        'target_202002': predictions_df['prediction']\n",
    "    })\n",
    "    \n",
    "    # Estad√≠sticas de predicciones\n",
    "    print(f\"\\nüìà ESTAD√çSTICAS DE PREDICCIONES:\")\n",
    "    print(f\"   Media: {submission['target_202002'].mean():.2f}\")\n",
    "    print(f\"   Mediana: {submission['target_202002'].median():.2f}\")\n",
    "    print(f\"   Std: {submission['target_202002'].std():.2f}\")\n",
    "    print(f\"   Min: {submission['target_202002'].min():.2f}\")\n",
    "    print(f\"   Max: {submission['target_202002'].max():.2f}\")\n",
    "    print(f\"   Predicciones = 0: {(submission['target_202002'] == 0).sum()}\")\n",
    "    \n",
    "    # Guardar submission\n",
    "    submission_filename = f'submission_nocturnal_{best_model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"üíæ Submission guardada: {submission_filename}\")\n",
    "    \n",
    "    # Guardar resultados detallados\n",
    "    results_filename = f'results_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "    results_df.to_csv(results_filename)\n",
    "    print(f\"üìä Resultados detallados guardados: {results_filename}\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    if MODEL_CONFIG['save_model']:\n",
    "        model_filename = f'best_model_nocturnal_{best_model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.pkl'\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        \n",
    "        # Guardar tambi√©n el preprocessor\n",
    "        preprocessor_filename = f'preprocessor_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.pkl'\n",
    "        joblib.dump(preprocessor, preprocessor_filename)\n",
    "        \n",
    "        print(f\"ü§ñ Modelo guardado: {model_filename}\")\n",
    "        print(f\"üîß Preprocessor guardado: {preprocessor_filename}\")\n",
    "    \n",
    "    # 13. An√°lisis de feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(\"\\n1Ô∏è‚É£3Ô∏è‚É£ AN√ÅLISIS DE FEATURE IMPORTANCE\")\n",
    "        \n",
    "        # Obtener nombres de features despu√©s del preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Features num√©ricas\n",
    "        feature_names.extend(numeric_features)\n",
    "        \n",
    "        # Features categ√≥ricas (despu√©s de one-hot encoding)\n",
    "        if categorical_features:\n",
    "            cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "            cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "            feature_names.extend(cat_feature_names)\n",
    "        \n",
    "        # Crear DataFrame de importancias\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüîç TOP 20 FEATURES M√ÅS IMPORTANTES:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f}\")\n",
    "        \n",
    "        # Guardar importancias\n",
    "        importance_filename = f'feature_importance_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "        importance_df.to_csv(importance_filename, index=False)\n",
    "        print(f\"\\nüíæ Feature importance guardada: {importance_filename}\")\n",
    "    \n",
    "    # 14. M√©tricas de calidad de predicci√≥n\n",
    "    print(\"\\n1Ô∏è‚É£4Ô∏è‚É£ M√âTRICAS DE CALIDAD\")\n",
    "    \n",
    "    # Distribuci√≥n de predicciones\n",
    "    pred_stats = {\n",
    "        'zero_predictions': (submission['target_202002'] == 0).sum(),\n",
    "        'low_predictions': (submission['target_202002'] < 1).sum(),\n",
    "        'medium_predictions': ((submission['target_202002'] >= 1) & (submission['target_202002'] < 10)).sum(),\n",
    "        'high_predictions': (submission['target_202002'] >= 10).sum(),\n",
    "        'extreme_predictions': (submission['target_202002'] >= 100).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"üìä Distribuci√≥n de predicciones:\")\n",
    "    for key, value in pred_stats.items():\n",
    "        percentage = (value / len(submission)) * 100\n",
    "        print(f\"   {key}: {value} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Comparaci√≥n con datos hist√≥ricos\n",
    "    historical_mean = data_filtered['tn'].mean()\n",
    "    historical_median = data_filtered['tn'].median()\n",
    "    \n",
    "    print(f\"\\nüìà Comparaci√≥n con hist√≥ricos:\")\n",
    "    print(f\"   Media hist√≥rica: {historical_mean:.2f}\")\n",
    "    print(f\"   Media predicha: {submission['target_202002'].mean():.2f}\")\n",
    "    print(f\"   Ratio: {submission['target_202002'].mean() / historical_mean:.2f}\")\n",
    "    print(f\"   Mediana hist√≥rica: {historical_median:.2f}\")\n",
    "    print(f\"   Mediana predicha: {submission['target_202002'].median():.2f}\")\n",
    "    \n",
    "    # 15. Validaci√≥n cruzada temporal (si hay tiempo)\n",
    "    if MODEL_CONFIG['use_cross_validation']:\n",
    "        print(\"\\n1Ô∏è‚É£5Ô∏è‚É£ VALIDACI√ìN CRUZADA TEMPORAL\")\n",
    "        \n",
    "        cv_scores = []\n",
    "        n_splits = 3\n",
    "        \n",
    "        # Crear splits temporales para CV\n",
    "        periods = sorted(agg_data['periodo'].unique())\n",
    "        split_size = len(periods) // (n_splits + 1)\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            # Definir fechas de corte\n",
    "            train_end_idx = (i + 1) * split_size\n",
    "            val_start_idx = train_end_idx\n",
    "            val_end_idx = train_end_idx + split_size\n",
    "            \n",
    "            if val_end_idx > len(periods):\n",
    "                break\n",
    "            \n",
    "            train_end_date = periods[train_end_idx - 1]\n",
    "            val_start_date = periods[val_start_idx]\n",
    "            val_end_date = periods[val_end_idx - 1]\n",
    "            \n",
    "            # Crear splits\n",
    "            cv_train = agg_data[agg_data['periodo'] <= train_end_date]\n",
    "            cv_val = agg_data[(agg_data['periodo'] >= val_start_date) & \n",
    "                             (agg_data['periodo'] <= val_end_date)]\n",
    "            \n",
    "            if len(cv_train) == 0 or len(cv_val) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Preparar datos\n",
    "                X_cv_train = cv_train[feature_cols]\n",
    "                y_cv_train = cv_train['tn']\n",
    "                X_cv_val = cv_val[feature_cols]\n",
    "                y_cv_val = cv_val['tn']\n",
    "                \n",
    "                # Procesar\n",
    "                X_cv_train_processed = preprocessor.fit_transform(X_cv_train)\n",
    "                X_cv_val_processed = preprocessor.transform(X_cv_val)\n",
    "                \n",
    "                # Entrenar y evaluar\n",
    "                cv_model = clone(best_model)\n",
    "                cv_model.fit(X_cv_train_processed, y_cv_train)\n",
    "                cv_pred = cv_model.predict(X_cv_val_processed)\n",
    "                \n",
    "                cv_rmse = np.sqrt(mean_squared_error(y_cv_val, cv_pred))\n",
    "                cv_scores.append(cv_rmse)\n",
    "                \n",
    "                print(f\"   Fold {i+1}: RMSE = {cv_rmse:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error en fold {i+1}: {e}\")\n",
    "        \n",
    "        if cv_scores:\n",
    "            print(f\"\\nüìä CV Results:\")\n",
    "            print(f\"   Mean RMSE: {np.mean(cv_scores):.3f} ¬± {np.std(cv_scores):.3f}\")\n",
    "            print(f\"   Scores: {[f'{score:.3f}' for score in cv_scores]}\")\n",
    "    \n",
    "    # 16. Resumen final\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ RESUMEN FINAL DEL PIPELINE NOCTURNO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚è∞ Tiempo total de ejecuci√≥n: {time.time() - start_time:.1f} segundos\")\n",
    "    print(f\"üèÜ Mejor modelo: {best_model_name}\")\n",
    "    print(f\"üìä RMSE de validaci√≥n: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "    print(f\"üìä MAE de validaci√≥n: {results_df.loc[best_model_name, 'MAE']:.3f}\")\n",
    "    print(f\"üìä R¬≤ de validaci√≥n: {results_df.loc[best_model_name, 'R2']:.3f}\")\n",
    "    print(f\"üéØ Predicciones generadas: {len(submission)}\")\n",
    "    print(f\"üíæ Archivo de submission: {submission_filename}\")\n",
    "    print(f\"‚úÖ Pipeline completado exitosamente!\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'preprocessor': preprocessor,\n",
    "        'results': results_df,\n",
    "        'submission': submission,\n",
    "        'predictions': predictions_df,\n",
    "        'model_name': best_model_name\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. EJECUCI√ìN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üåô INICIANDO PIPELINE NOCTURNO AVANZADO\")\n",
    "    print(\"‚è∞ Hora de inicio:\", pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Ejecutar pipeline principal\n",
    "        results = run_advanced_nocturnal_pipeline()\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\nüéâ PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
    "            print(f\"üèÜ Mejor modelo: {results['model_name']}\")\n",
    "            print(f\"üìä Submission generada con {len(results['submission'])} predicciones\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n‚ùå ERROR EN EL PIPELINE\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚èπÔ∏è Pipeline interrumpido por el usuario\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nüí• ERROR CR√çTICO: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n‚è∞ Tiempo total: {total_time:.1f} segundos ({total_time/60:.1f} minutos)\")\n",
    "        print(\"üåô Fin del pipeline nocturno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10829fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
