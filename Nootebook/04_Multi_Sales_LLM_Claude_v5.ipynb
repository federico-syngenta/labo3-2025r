{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Auto-ARIMA no disponible, se omitirá\n",
      "📦 Librerías cargadas exitosamente!\n",
      "🖥️ Configuración del sistema:\n",
      "   CPUs disponibles: 8\n",
      "   Directorio de salida: kaggle_predictions_advanced\n",
      "   Optimización de hiperparámetros: True\n",
      "   Ensemble stacking: True\n",
      "   Modelos de series de tiempo: True\n",
      "🌙 INICIANDO PIPELINE NOCTURNO AVANZADO\n",
      "⏰ Hora de inicio: 2025-06-09 17:24:03\n",
      "🚀 INICIANDO PIPELINE AVANZADO NOCTURNO\n",
      "============================================================\n",
      "\n",
      "1️⃣ CARGA DE DATOS\n",
      "🔄 Cargando datasets...\n",
      "✅ Sales: 2,945,818 filas, 7 columnas\n",
      "✅ Stocks: 13,691 filas, 3 columnas\n",
      "✅ Products: 1,251 productos\n",
      "✅ Products to predict: 780 productos\n",
      "📅 Rango de fechas: 2017-01-01 00:00:00 a 2019-12-01 00:00:00\n",
      "🇦🇷 Cargando datos del IPC INDEC...\n",
      "✅ IPC INDEC procesado: 48 períodos\n",
      "📊 Rango IPC: 1.13 a 5.68\n",
      "\n",
      "2️⃣ FEATURE ENGINEERING AVANZADO\n",
      "🔧 Creando features avanzadas v2...\n",
      "   → Después de merge productos: 2945818 filas\n",
      "   → Después de merge stocks: 2945818 filas\n",
      "   → Después de merge IPC: 2945818 filas\n",
      "📊 Aplicando feature engineering avanzado...\n",
      "🔄 Creando lags extendidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando lags: 100%|██████████| 12/12 [00:06<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Creando rolling features extendidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling windows: 100%|██████████| 8/8 [1:46:51<00:00, 801.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Creando features de tendencia...\n",
      "📊 Creando features estadísticas...\n",
      "🏷️ Creando agregaciones categóricas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agregaciones categóricas: 100%|██████████| 6/6 [00:03<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Creando features de interacción...\n",
      "📦 Creando features de stock avanzadas...\n",
      "💰 Creando features de IPC avanzadas...\n",
      "🏆 Creando features de ranking...\n",
      "🔄 Creando features de frecuencia...\n",
      "🔍 Creando features de patrones...\n",
      "🎯 Creando target encoding...\n",
      "   Procesando cat1...\n",
      "   ✅ cat1_target_encoded creada\n",
      "   Procesando cat2...\n",
      "   ✅ cat2_target_encoded creada\n",
      "   Procesando cat3...\n",
      "   ✅ cat3_target_encoded creada\n",
      "   Procesando brand...\n",
      "   ✅ brand_target_encoded creada\n",
      "🧹 Limpieza final...\n",
      "✅ Features v2 creadas. Shape final: (2945818, 248)\n",
      "📊 Features numéricas: 242\n",
      "📊 Features categóricas: 5\n",
      "\n",
      "3️⃣ AGREGACIÓN DE DATOS\n",
      "📊 Agregando datos por producto-cliente...\n",
      "🔧 Reduciendo features para optimizar memoria...\n",
      "🔍 Verificando columnas de agrupación...\n",
      "   product_id: int64, shape: (2945818,)\n",
      "   customer_id: int64, shape: (2945818,)\n",
      "   periodo: datetime64[ns], shape: (2945818,)\n",
      "🧹 Eliminando columnas duplicadas...\n",
      "📊 Features reducidas a 54\n",
      "📊 Columnas finales: ['product_id', 'customer_id', 'periodo', 'tn', 'plan_precios_cuidados', 'cust_request_qty', 'cust_request_tn', 'sku_size', 'stock_final', 'v_m_IPC', 'year', 'month', 'quarter', 'day_of_year', 'week_of_year', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'days_in_month', 'sin_month_3', 'cos_month_3', 'sin_month_4', 'cos_month_4', 'sin_month_6', 'cos_month_6', 'sin_month_12', 'cos_month_12', 'sin_quarter', 'cos_quarter', 'sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_4', 'sales_lag_5', 'sales_lag_6', 'sales_lag_9', 'sales_lag_12', 'sales_lag_15', 'sales_lag_18', 'sales_lag_24', 'sales_lag_36', 'product_sales_lag_1', 'product_sales_lag_3', 'product_sales_lag_6', 'product_sales_lag_12', 'sales_rolling_mean_2', 'sales_rolling_std_2', 'sales_rolling_min_2', 'sales_rolling_max_2', 'sales_rolling_median_2', 'sales_ewma_2_alpha_01', 'sales_ewma_2_alpha_03']\n",
      "🔍 Verificando tipos de datos finales...\n",
      "   product_id: int64\n",
      "   customer_id: int64\n",
      "   periodo: datetime64[ns]\n",
      "🔄 Agregando por chunks...\n",
      "   Chunk 1: 100000 registros\n",
      "   Chunk 2: 100000 registros\n",
      "   Chunk 3: 100000 registros\n",
      "   Chunk 4: 100000 registros\n",
      "   Chunk 5: 100000 registros\n",
      "   Chunk 6: 100000 registros\n",
      "   Chunk 7: 100000 registros\n",
      "   Chunk 8: 100000 registros\n",
      "   Chunk 9: 100000 registros\n",
      "   Chunk 10: 100000 registros\n",
      "   Chunk 11: 100000 registros\n",
      "   Chunk 12: 100000 registros\n",
      "   Chunk 13: 100000 registros\n",
      "   Chunk 14: 100000 registros\n",
      "   Chunk 15: 100000 registros\n",
      "   Chunk 16: 100000 registros\n",
      "   Chunk 17: 100000 registros\n",
      "   Chunk 18: 100000 registros\n",
      "   Chunk 19: 100000 registros\n",
      "   Chunk 20: 100000 registros\n",
      "   Chunk 21: 100000 registros\n",
      "   Chunk 22: 100000 registros\n",
      "   Chunk 23: 100000 registros\n",
      "   Chunk 24: 100000 registros\n",
      "   Chunk 25: 100000 registros\n",
      "   Chunk 26: 100000 registros\n",
      "   Chunk 27: 100000 registros\n",
      "   Chunk 28: 100000 registros\n",
      "   Chunk 29: 100000 registros\n",
      "   Chunk 30: 45818 registros\n",
      "🔗 Combinando chunks...\n",
      "🔄 Consolidando datos finales...\n",
      "✅ Datos agregados: (2945818, 54)\n",
      "\n",
      "4️⃣ CREACIÓN DE SPLITS TEMPORALES\n",
      "📊 Training: 2,814,453 registros (hasta 2019-10)\n",
      "📊 Validation: 131,365 registros (desde 2019-10)\n",
      "🔧 Procesando features...\n",
      "✅ Features procesadas: 49 dimensiones\n",
      "\n",
      "5️⃣ EVALUACIÓN DE MODELOS DE ML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  11%|█         | 1/9 [1:18:08<10:25:11, 4688.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RandomForest - MAE: 0.01, RMSE: 0.48, R2: 0.980, MAPE: 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  22%|██▏       | 2/9 [1:19:17<3:49:57, 1971.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost - MAE: 0.06, RMSE: 1.44, R2: 0.818, MAPE: 13.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  33%|███▎      | 3/9 [1:19:46<1:48:24, 1084.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM - MAE: 0.07, RMSE: 1.51, R2: 0.800, MAPE: 64.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  44%|████▍     | 4/9 [1:22:09<59:22, 712.55s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CatBoost - MAE: 0.08, RMSE: 1.58, R2: 0.781, MAPE: 74.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  56%|█████▌    | 5/9 [4:35:13<5:08:52, 4633.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GradientBoosting - MAE: 0.01, RMSE: 0.36, R2: 0.988, MAPE: 2.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  67%|██████▋   | 6/9 [5:26:43<3:25:25, 4108.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ExtraTrees - MAE: 0.01, RMSE: 0.48, R2: 0.980, MAPE: 6.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  78%|███████▊  | 7/9 [5:26:46<1:32:12, 2766.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ridge - MAE: 0.03, RMSE: 0.28, R2: 0.993, MAPE: 160.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  89%|████████▉ | 8/9 [5:30:32<32:37, 1957.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lasso - MAE: 0.02, RMSE: 0.34, R2: 0.990, MAPE: 28.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML: 100%|██████████| 9/9 [5:34:31<00:00, 2230.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ElasticNet - MAE: 0.03, RMSE: 0.31, R2: 0.991, MAPE: 43.0%\n",
      "\n",
      "6️⃣ OPTIMIZACIÓN DE HIPERPARÁMETROS\n",
      "\n",
      "7️⃣ MODELOS DE SERIES DE TIEMPO\n",
      "⏰ Generando predicciones con modelos de series de tiempo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time Series Predictions: 100%|██████████| 50/50 [00:01<00:00, 36.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicciones de series de tiempo completadas para 50 productos\n",
      "⚠️ Errores en 50 productos\n",
      "\n",
      "8️⃣ ENSEMBLE AVANZADO\n",
      "🎯 Entrenando ensemble avanzado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  11%|█         | 1/9 [1:17:59<10:23:59, 4679.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ rf: MAE=0.008, RMSE=0.478, R2=0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  22%|██▏       | 2/9 [1:19:18<3:50:14, 1973.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ xgb: MAE=0.061, RMSE=1.439, R2=0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  33%|███▎      | 3/9 [1:19:57<1:48:59, 1089.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ lgb: MAE=0.067, RMSE=1.507, R2=0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  44%|████▍     | 4/9 [1:22:20<59:40, 716.15s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ catboost: MAE=0.082, RMSE=1.579, R2=0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  56%|█████▌    | 5/9 [4:35:07<5:08:36, 4629.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ gb: MAE=0.006, RMSE=0.365, R2=0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  67%|██████▋   | 6/9 [5:26:40<3:25:20, 4106.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ extra_trees: MAE=0.009, RMSE=0.480, R2=0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  78%|███████▊  | 7/9 [5:26:43<1:32:10, 2765.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ ridge: MAE=0.031, RMSE=0.281, R2=0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  89%|████████▉ | 8/9 [5:30:30<32:37, 1957.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ lasso: MAE=0.025, RMSE=0.336, R2=0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1: 100%|██████████| 9/9 [5:34:29<00:00, 2229.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ elastic: MAE=0.025, RMSE=0.312, R2=0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 ENSEMBLE FINAL: MAE=0.029, RMSE=0.695, R2=0.958\n",
      "\n",
      "9️⃣ SELECCIÓN DEL MEJOR MODELO\n",
      "\n",
      "📊 RESUMEN DE RESULTADOS\n",
      "============================================================\n",
      "                    MAE   RMSE     R2     MAPE    mae   rmse     r2\n",
      "Ridge             0.031  0.281  0.993  160.513    NaN    NaN    NaN\n",
      "ElasticNet        0.025  0.312  0.991   43.004    NaN    NaN    NaN\n",
      "Lasso             0.025  0.336  0.990   28.868    NaN    NaN    NaN\n",
      "GradientBoosting  0.006  0.365  0.988    2.861    NaN    NaN    NaN\n",
      "RandomForest      0.008  0.478  0.980    0.726    NaN    NaN    NaN\n",
      "ExtraTrees        0.009  0.480  0.980    6.852    NaN    NaN    NaN\n",
      "XGBoost           0.061  1.439  0.818   13.188    NaN    NaN    NaN\n",
      "LightGBM          0.067  1.507  0.800   64.729    NaN    NaN    NaN\n",
      "CatBoost          0.082  1.579  0.781   74.818    NaN    NaN    NaN\n",
      "AdvancedEnsemble    NaN    NaN    NaN      NaN  0.029  0.695  0.958\n",
      "\n",
      "🏆 MEJOR MODELO: Ridge\n",
      "   RMSE: 0.281\n",
      "   MAE: 0.031\n",
      "   R2: 0.993\n",
      "\n",
      "🔟 REENTRENAMIENTO CON DATOS COMPLETOS\n",
      "🔄 Reentrenando el mejor modelo con todos los datos disponibles...\n",
      "\n",
      "1️⃣1️⃣ PREDICCIONES FINALES\n",
      "🎯 Generando predicciones para 2020-02-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:   6%|▋         | 50/780 [11:05<2:41:03, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20054:\n",
      "Clientes procesados: 489\n",
      "Predicción total: 251.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  13%|█▎        | 100/780 [21:20<2:18:17, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20114:\n",
      "Clientes procesados: 413\n",
      "Predicción total: 93.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  19%|█▉        | 150/780 [31:05<2:06:06, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20180:\n",
      "Clientes procesados: 414\n",
      "Predicción total: 55.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  26%|██▌       | 200/780 [40:21<1:41:09, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20244:\n",
      "Clientes procesados: 437\n",
      "Predicción total: 37.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  32%|███▏      | 250/780 [49:37<1:37:44, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20305:\n",
      "Clientes procesados: 392\n",
      "Predicción total: 23.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  38%|███▊      | 300/780 [59:27<1:38:48, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20365:\n",
      "Clientes procesados: 470\n",
      "Predicción total: 15.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  45%|████▍     | 350/780 [1:09:02<1:19:13, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20440:\n",
      "Clientes procesados: 262\n",
      "Predicción total: 48.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  51%|█████▏    | 400/780 [1:17:49<59:04,  9.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20526:\n",
      "Clientes procesados: 269\n",
      "Predicción total: 41.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  58%|█████▊    | 450/780 [1:26:37<1:05:20, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20601:\n",
      "Clientes procesados: 381\n",
      "Predicción total: 7.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  64%|██████▍   | 500/780 [1:34:57<43:05,  9.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20673:\n",
      "Clientes procesados: 230\n",
      "Predicción total: 12.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  71%|███████   | 550/780 [1:43:20<43:09, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20751:\n",
      "Clientes procesados: 342\n",
      "Predicción total: 12.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  77%|███████▋  | 600/780 [1:51:28<24:09,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20845:\n",
      "Clientes procesados: 51\n",
      "Predicción total: 26.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  83%|████████▎ | 650/780 [1:58:20<18:08,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20947:\n",
      "Clientes procesados: 255\n",
      "Predicción total: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  90%|████████▉ | 700/780 [2:05:19<10:22,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 21056:\n",
      "Clientes procesados: 209\n",
      "Predicción total: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  96%|█████████▌| 750/780 [2:10:55<04:01,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 21190:\n",
      "Clientes procesados: 277\n",
      "Predicción total: 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones: 100%|██████████| 780/780 [2:12:59<00:00, 10.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Predicciones completadas:\n",
      "   Total: 780\n",
      "   Con modelo: 780\n",
      "   Errores: 0\n",
      "\n",
      "1️⃣2️⃣ GUARDANDO RESULTADOS\n",
      "\n",
      "📈 ESTADÍSTICAS DE PREDICCIONES:\n",
      "   Media: 62.99\n",
      "   Mediana: 14.95\n",
      "   Std: 154.07\n",
      "   Min: 0.01\n",
      "   Max: 1825.30\n",
      "   Predicciones = 0: 0\n",
      "\n",
      "📊 GENERANDO SUBMISSIONS MÚLTIPLES\n",
      "💾 RandomForest guardado: submission_randomforest_20250610_0859.csv\n",
      "💾 XGBoost guardado: submission_xgboost_20250610_0900.csv\n",
      "💾 LightGBM guardado: submission_lightgbm_20250610_0900.csv\n",
      "💾 CatBoost guardado: submission_catboost_20250610_0900.csv\n",
      "💾 GradientBoosting guardado: submission_gradientboosting_20250610_0900.csv\n",
      "💾 ExtraTrees guardado: submission_extratrees_20250610_0901.csv\n",
      "💾 Lasso guardado: submission_lasso_20250610_0901.csv\n",
      "💾 ElasticNet guardado: submission_elasticnet_20250610_0901.csv\n",
      "💾 AdvancedEnsemble guardado: submission_advancedensemble_20250610_0903.csv\n",
      "💾 Submission guardada: submission_nocturnal_ridge_20250610_0903.csv\n",
      "📊 Resultados detallados guardados: results_nocturnal_20250610_0903.csv\n",
      "🤖 Modelo guardado: best_model_nocturnal_ridge_20250610_0903.pkl\n",
      "🔧 Preprocessor guardado: preprocessor_nocturnal_20250610_0903.pkl\n",
      "\n",
      "1️⃣4️⃣ MÉTRICAS DE CALIDAD\n",
      "📊 Distribución de predicciones:\n",
      "   zero_predictions: 0 (0.0%)\n",
      "   low_predictions: 95 (12.2%)\n",
      "   medium_predictions: 227 (29.1%)\n",
      "   high_predictions: 458 (58.7%)\n",
      "   extreme_predictions: 126 (16.2%)\n",
      "\n",
      "📈 Comparación con históricos:\n",
      "   Media histórica: 0.45\n",
      "   Media predicha: 62.99\n",
      "   Ratio: 140.05\n",
      "   Mediana histórica: 0.04\n",
      "   Mediana predicha: 14.95\n",
      "\n",
      "1️⃣5️⃣ VALIDACIÓN CRUZADA TEMPORAL\n",
      "   Error en fold 1: name 'clone' is not defined\n",
      "   Error en fold 2: name 'clone' is not defined\n",
      "   Error en fold 3: name 'clone' is not defined\n",
      "\n",
      "================================================================================\n",
      "🎯 RESUMEN FINAL DEL PIPELINE NOCTURNO\n",
      "================================================================================\n",
      "⏰ Tiempo total de ejecución: 56468.2 segundos\n",
      "🏆 Mejor modelo: Ridge\n",
      "📊 RMSE de validación: 0.281\n",
      "📊 MAE de validación: 0.031\n",
      "📊 R² de validación: 0.993\n",
      "🎯 Predicciones generadas: 780\n",
      "💾 Archivo de submission: submission_nocturnal_ridge_20250610_0903.csv\n",
      "✅ Pipeline completado exitosamente!\n",
      "\n",
      "🎉 PIPELINE COMPLETADO EXITOSAMENTE!\n",
      "🏆 Mejor modelo: Ridge\n",
      "📊 Submission generada con 780 predicciones\n",
      "\n",
      "⏰ Tiempo total: 56469.7 segundos (941.2 minutos)\n",
      "🌙 Fin del pipeline nocturno\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREDICCIÓN AVANZADA NOCTURNA - MÚLTIPLES MODELOS Y SERIES DE TIEMPO\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Proyecto: Predicción de Ventas - Competencia Kaggle (Versión Nocturna Avanzada)\n",
    "Objetivo: Predecir ventas para febrero 2020 (mes +2)\n",
    "Modelos: RF, XGB, LGB, CatBoost, GradientBoosting, ExtraTrees, SVR,\n",
    "         ARIMA, SARIMA, Prophet, AutoArima, Ensemble Avanzado\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORTS Y CONFIGURACIÓN EXTENDIDA\n",
    "# ============================================================================\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning Básico\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                             ExtraTreesRegressor, VotingRegressor)\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import (train_test_split, TimeSeriesSplit, \n",
    "                                   cross_val_score, GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Modelos avanzados\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Series de tiempo\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Prophet para series de tiempo\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Prophet no disponible, se omitirá\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# AutoML para series de tiempo\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    AUTO_ARIMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Auto-ARIMA no disponible, se omitirá\")\n",
    "    AUTO_ARIMA_AVAILABLE = False\n",
    "\n",
    "# Utilidades\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "print(\"📦 Librerías cargadas exitosamente!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CONFIGURACIÓN GLOBAL EXTENDIDA\n",
    "# ============================================================================\n",
    "\n",
    "# Configuración\n",
    "RANDOM_STATE = 42\n",
    "TARGET_DATE = '2020-02-01'\n",
    "VALIDATION_MONTHS = 2\n",
    "OUTPUT_DIR = 'kaggle_predictions_advanced'\n",
    "N_JOBS = -1  # Usar todos los cores disponibles\n",
    "\n",
    "# Crear directorio de salida\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configuración de modelos\n",
    "MODEL_CONFIG = {\n",
    "    'generate_multiple_submissions': True,\n",
    "    'save_model': True,\n",
    "    'use_hyperparameter_tuning': True,\n",
    "    'use_ensemble_stacking': True,\n",
    "    'use_time_series_models': True,\n",
    "    'use_prophet': PROPHET_AVAILABLE,\n",
    "    'use_auto_arima': AUTO_ARIMA_AVAILABLE,\n",
    "    'cross_validation_folds': 3,\n",
    "    'optuna_trials': 100,\n",
    "    'use_cross_validation': True\n",
    "}\n",
    "\n",
    "# Configuración visual\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"🖥️ Configuración del sistema:\")\n",
    "print(f\"   CPUs disponibles: {cpu_count()}\")\n",
    "print(f\"   Directorio de salida: {OUTPUT_DIR}\")\n",
    "print(f\"   Optimización de hiperparámetros: {MODEL_CONFIG['use_hyperparameter_tuning']}\")\n",
    "print(f\"   Ensemble stacking: {MODEL_CONFIG['use_ensemble_stacking']}\")\n",
    "print(f\"   Modelos de series de tiempo: {MODEL_CONFIG['use_time_series_models']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FUNCIONES DE CARGA Y PREPARACIÓN (MEJORADAS)\n",
    "# ============================================================================\n",
    "def validate_predictions(predictions_df, historical_data):\n",
    "    \"\"\"Valida que las predicciones estén en rangos razonables\"\"\"\n",
    "    \n",
    "    # Calcular estadísticas históricas por producto\n",
    "    historical_stats = historical_data.groupby('product_id')['tn'].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    \n",
    "    # Merge con predicciones\n",
    "    validation_df = predictions_df.merge(historical_stats, on='product_id', how='left')\n",
    "    \n",
    "    # Ajustar predicciones fuera de rango\n",
    "    for idx, row in validation_df.iterrows():\n",
    "        if row['prediction'] < row['mean'] * 0.01:  # Muy por debajo del promedio\n",
    "            validation_df.loc[idx, 'prediction'] = row['mean'] * 0.1\n",
    "        elif row['prediction'] > row['max'] * 2:  # Muy por encima del máximo histórico\n",
    "            validation_df.loc[idx, 'prediction'] = row['max'] * 1.2\n",
    "    \n",
    "    return validation_df[['product_id', 'prediction']]\n",
    "\n",
    "def inverse_transform_predictions(predictions, scaler):\n",
    "    \"\"\"Des-escala las predicciones\"\"\"\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    return scaler.inverse_transform(predictions).ravel()\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Carga y prepara todos los datasets con validaciones mejoradas\"\"\"\n",
    "    print(\"🔄 Cargando datasets...\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar datasets con validaciones\n",
    "        sales = pd.read_csv(\"../datasets/sell-in.txt\", sep=\"\\t\", dtype={\"periodo\": str})\n",
    "        stocks = pd.read_csv(\"../datasets/tb_stocks.txt\", sep=\"\\t\", dtype={\"periodo\": str}) \n",
    "        product_info = pd.read_csv(\"../datasets/tb_productos.txt\", sep=\"\\t\")\n",
    "        products_to_predict = pd.read_csv('../datasets/product_id_apredecir201912.txt')\n",
    "        \n",
    "        # Validaciones básicas\n",
    "        assert not sales.empty, \"Sales dataset está vacío\"\n",
    "        assert not products_to_predict.empty, \"Products to predict está vacío\"\n",
    "        \n",
    "        # Convertir periodos con validación\n",
    "        sales['periodo'] = pd.to_datetime(sales['periodo'], format='%Y%m', errors='coerce')\n",
    "        stocks['periodo'] = pd.to_datetime(stocks['periodo'], format='%Y%m', errors='coerce')\n",
    "        \n",
    "        # Eliminar fechas inválidas\n",
    "        sales = sales.dropna(subset=['periodo'])\n",
    "        stocks = stocks.dropna(subset=['periodo'])\n",
    "        \n",
    "        print(f\"✅ Sales: {sales.shape[0]:,} filas, {sales.shape[1]} columnas\")\n",
    "        print(f\"✅ Stocks: {stocks.shape[0]:,} filas, {stocks.shape[1]} columnas\") \n",
    "        print(f\"✅ Products: {product_info.shape[0]:,} productos\")\n",
    "        print(f\"✅ Products to predict: {len(products_to_predict):,} productos\")\n",
    "        print(f\"📅 Rango de fechas: {sales['periodo'].min()} a {sales['periodo'].max()}\")\n",
    "        \n",
    "        return sales, stocks, product_info, products_to_predict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error cargando datos: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_indec_data():\n",
    "    \"\"\"Carga y procesa datos del INDEC con limpieza mejorada\"\"\"\n",
    "    print(\"🇦🇷 Cargando datos del IPC INDEC...\")\n",
    "    \n",
    "    try:\n",
    "        INDEC = pd.read_csv('../datasets/serie_ipc_aperturas.csv', sep=';', encoding='latin-1')\n",
    "        INDEC['periodo'] = INDEC['periodo'].astype(str)\n",
    "        \n",
    "        INDEC_filtered = INDEC[\n",
    "            (INDEC['periodo'] >= '201701') & \n",
    "            (INDEC['periodo'] <= '202012') & \n",
    "            (INDEC['Descripcion_aperturas'] == 'Nivel general')\n",
    "        ].copy()\n",
    "        \n",
    "        def clean_and_convert(value):\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Limpiar múltiples formatos posibles\n",
    "                    cleaned = value.replace(',', '.').replace(' ', '')\n",
    "                    # Extraer números\n",
    "                    import re\n",
    "                    numbers = re.findall(r'-?\\d+\\.?\\d*', cleaned)\n",
    "                    if numbers:\n",
    "                        return float(numbers[0])\n",
    "                    return np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "            return float(value) if pd.notna(value) else np.nan\n",
    "        \n",
    "        INDEC_filtered['v_m_IPC'] = INDEC_filtered['v_m_IPC'].apply(clean_and_convert)\n",
    "        INDEC_filtered = INDEC_filtered.dropna(subset=['v_m_IPC'])\n",
    "        INDEC_processed = INDEC_filtered.groupby('periodo')['v_m_IPC'].mean().reset_index()\n",
    "        INDEC_processed['periodo'] = pd.to_datetime(INDEC_processed['periodo'], format='%Y%m')\n",
    "        \n",
    "        # Suavizado de valores extremos\n",
    "        q1, q3 = INDEC_processed['v_m_IPC'].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Cap outliers\n",
    "        INDEC_processed['v_m_IPC'] = np.clip(INDEC_processed['v_m_IPC'], lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"✅ IPC INDEC procesado: {len(INDEC_processed)} períodos\")\n",
    "        print(f\"📊 Rango IPC: {INDEC_processed['v_m_IPC'].min():.2f} a {INDEC_processed['v_m_IPC'].max():.2f}\")\n",
    "        \n",
    "        return INDEC_processed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error procesando INDEC: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_advanced_features_v2(sales, stocks, product_info, indec_data):\n",
    "    \"\"\"Versión mejorada de feature engineering con más features\"\"\"\n",
    "    print(\"🔧 Creando features avanzadas v2...\")\n",
    "    \n",
    "    # Merge inicial con validaciones\n",
    "    data = sales.copy()\n",
    "    initial_shape = data.shape[0]\n",
    "    \n",
    "    # Agregar información de productos\n",
    "    if product_info is not None:\n",
    "        data = data.merge(product_info, on='product_id', how='left')\n",
    "        print(f\"   → Después de merge productos: {data.shape[0]} filas\")\n",
    "    \n",
    "    # Agregar stocks\n",
    "    if stocks is not None:\n",
    "        data = data.merge(stocks, on=['periodo', 'product_id'], how='left')\n",
    "        print(f\"   → Después de merge stocks: {data.shape[0]} filas\")\n",
    "    \n",
    "    # Agregar IPC\n",
    "    if indec_data is not None:\n",
    "        data = data.merge(indec_data, on='periodo', how='left')\n",
    "        print(f\"   → Después de merge IPC: {data.shape[0]} filas\")\n",
    "    \n",
    "    # FEATURE ENGINEERING COMPLETO Y MEJORADO\n",
    "    print(\"📊 Aplicando feature engineering avanzado...\")\n",
    "    \n",
    "    # 1. Features temporales extendidas\n",
    "    data['year'] = data['periodo'].dt.year\n",
    "    data['month'] = data['periodo'].dt.month\n",
    "    data['quarter'] = data['periodo'].dt.quarter\n",
    "    data['day_of_year'] = data['periodo'].dt.dayofyear\n",
    "    data['week_of_year'] = data['periodo'].dt.isocalendar().week\n",
    "    data['is_weekend'] = data['periodo'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    data['is_month_start'] = data['periodo'].dt.is_month_start.astype(int)\n",
    "    data['is_month_end'] = data['periodo'].dt.is_month_end.astype(int)\n",
    "    data['is_quarter_start'] = data['periodo'].dt.is_quarter_start.astype(int)\n",
    "    data['is_quarter_end'] = data['periodo'].dt.is_quarter_end.astype(int)\n",
    "    data['days_in_month'] = data['periodo'].dt.days_in_month\n",
    "    \n",
    "    # 2. Features de estacionalidad múltiples\n",
    "    for period in [3, 4, 6, 12]:\n",
    "        data[f'sin_month_{period}'] = np.sin(2 * np.pi * data['month'] / period)\n",
    "        data[f'cos_month_{period}'] = np.cos(2 * np.pi * data['month'] / period)\n",
    "    \n",
    "    # Features cíclicas para quarter\n",
    "    data['sin_quarter'] = np.sin(2 * np.pi * data['quarter'] / 4)\n",
    "    data['cos_quarter'] = np.cos(2 * np.pi * data['quarter'] / 4)\n",
    "    \n",
    "    # 3. Lags extendidos con múltiples targets\n",
    "    print(\"🔄 Creando lags extendidos...\")\n",
    "    lag_periods = [1, 2, 3, 4, 5, 6, 9, 12, 15, 18, 24, 36]\n",
    "    \n",
    "    for lag in tqdm(lag_periods, desc=\"Creando lags\"):\n",
    "        data[f'sales_lag_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].shift(lag)\n",
    "        \n",
    "    # Lags por producto solamente (agregados)\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'product_sales_lag_{lag}'] = data.groupby('product_id')['tn'].shift(lag)\n",
    "\n",
    "    # 4. Rolling windows extendidos\n",
    "    print(\"🔄 Creando rolling features extendidos...\")\n",
    "    windows = [2, 3, 4, 6, 9, 12, 18, 24]\n",
    "    operations = ['mean', 'std', 'min', 'max', 'median', 'skew']\n",
    "    \n",
    "    for window in tqdm(windows, desc=\"Rolling windows\"):\n",
    "        # Por producto-cliente\n",
    "        rolling_group = data.groupby(['product_id', 'customer_id'])['tn']\n",
    "        data[f'sales_rolling_mean_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        data[f'sales_rolling_std_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "        data[f'sales_rolling_min_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "        data[f'sales_rolling_max_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        data[f'sales_rolling_median_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "        \n",
    "        # EWMA con diferentes alphas\n",
    "        for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "            data[f'sales_ewma_{window}_alpha_{str(alpha).replace(\".\", \"\")}'] = rolling_group.transform(\n",
    "                lambda x: x.ewm(alpha=alpha, min_periods=1).mean()\n",
    "            )\n",
    "\n",
    "    # 5. Features de tendencia y momentum\n",
    "    print(\"📈 Creando features de tendencia...\")\n",
    "    \n",
    "    # Diferencias y cambios porcentuales\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'sales_diff_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].diff(periods=lag)\n",
    "        data[f'sales_pct_change_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].pct_change(periods=lag)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    for short, long in [(3, 6), (6, 12), (12, 24)]:\n",
    "        data[f'momentum_{short}_{long}'] = (\n",
    "            data[f'sales_rolling_mean_{short}'] - data[f'sales_rolling_mean_{long}']\n",
    "        )\n",
    "        \n",
    "    # Acceleration (second derivative)\n",
    "    data['sales_acceleration'] = data.groupby(['product_id', 'customer_id'])['tn'].diff().diff()\n",
    "    \n",
    "    # 6. Features estadísticas avanzadas\n",
    "    print(\"📊 Creando features estadísticas...\")\n",
    "    \n",
    "    # Ratios y volatilidad\n",
    "    for window in [3, 6, 12]:\n",
    "        data[f'cv_{window}'] = data[f'sales_rolling_std_{window}'] / (data[f'sales_rolling_mean_{window}'] + 1e-8)\n",
    "        data[f'zscore_{window}'] = (data['tn'] - data[f'sales_rolling_mean_{window}']) / (data[f'sales_rolling_std_{window}'] + 1e-8)\n",
    "        data[f'range_ratio_{window}'] = (data[f'sales_rolling_max_{window}'] - data[f'sales_rolling_min_{window}']) / (data[f'sales_rolling_mean_{window}'] + 1e-8)\n",
    "    \n",
    "    # 7. Agregaciones categóricas extendidas\n",
    "    print(\"🏷️ Creando agregaciones categóricas...\")\n",
    "    \n",
    "    categorical_cols = ['product_id', 'customer_id']\n",
    "    if 'brand' in data.columns:\n",
    "        categorical_cols.extend(['brand', 'cat1', 'cat2', 'cat3'])\n",
    "    \n",
    "    aggregations = ['mean', 'std', 'median', 'min', 'max', 'count', 'sum']\n",
    "    \n",
    "    for cat in tqdm(categorical_cols, desc=\"Agregaciones categóricas\"):\n",
    "        if cat in data.columns:\n",
    "            grouped = data.groupby(cat)['tn']\n",
    "            for agg in aggregations:\n",
    "                try:\n",
    "                    data[f'{cat}_{agg}'] = grouped.transform(agg)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # 8. Features de interacción avanzadas\n",
    "    print(\"🔗 Creando features de interacción...\")\n",
    "    \n",
    "    # Interacciones temporales\n",
    "    data['sales_month_interaction'] = data['tn'] * data['month']\n",
    "    data['sales_quarter_interaction'] = data['tn'] * data['quarter']\n",
    "    data['sales_year_interaction'] = data['tn'] * (data['year'] - data['year'].min())\n",
    "    \n",
    "    # Interacciones con lags\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'sales_lag_ratio_{lag}'] = data['tn'] / (data[f'sales_lag_{lag}'] + 1e-8)\n",
    "        data[f'sales_lag_diff_{lag}'] = data['tn'] - data[f'sales_lag_{lag}']\n",
    "    \n",
    "    # 9. Features de stock mejoradas\n",
    "    if 'stock_final' in data.columns:\n",
    "        print(\"📦 Creando features de stock avanzadas...\")\n",
    "        \n",
    "        data['stock_turnover'] = data['tn'] / (data['stock_final'] + 1e-8)\n",
    "        data['days_of_stock'] = data['stock_final'] / (data['tn'] + 1e-8) * 30\n",
    "        data['stock_ratio'] = data['stock_final'] / (data['stock_final'].mean() + 1e-8)\n",
    "        \n",
    "        # Lags de stock\n",
    "        for lag in [1, 3, 6]:\n",
    "            data[f'stock_lag_{lag}'] = data.groupby(['product_id', 'customer_id'])['stock_final'].shift(lag)\n",
    "            data[f'stock_change_{lag}'] = data['stock_final'] - data[f'stock_lag_{lag}']\n",
    "    \n",
    "    # 10. Features de IPC avanzadas\n",
    "    if 'v_m_IPC' in data.columns:\n",
    "        print(\"💰 Creando features de IPC avanzadas...\")\n",
    "        \n",
    "        # Lags de IPC\n",
    "        for lag in [1, 2, 3, 6, 12]:\n",
    "            data[f'ipc_lag_{lag}'] = data['v_m_IPC'].shift(lag)\n",
    "        \n",
    "        # Rolling IPC\n",
    "        for window in [3, 6, 12]:\n",
    "            data[f'ipc_rolling_{window}'] = data['v_m_IPC'].rolling(window, min_periods=1).mean()\n",
    "            data[f'ipc_std_{window}'] = data['v_m_IPC'].rolling(window, min_periods=1).std()\n",
    "        \n",
    "        # Interacciones IPC-ventas\n",
    "        data['sales_ipc_interaction'] = data['tn'] * data['v_m_IPC']\n",
    "        data['sales_ipc_ratio'] = data['tn'] / (data['v_m_IPC'] + 1e-8)\n",
    "        \n",
    "        # Cambios en IPC\n",
    "        data['ipc_change'] = data['v_m_IPC'].diff()\n",
    "        data['ipc_acceleration'] = data['ipc_change'].diff()\n",
    "    \n",
    "    # 11. Features de ranking y percentiles\n",
    "    print(\"🏆 Creando features de ranking...\")\n",
    "    \n",
    "    # Rankings por período\n",
    "    data['product_rank_period'] = data.groupby('periodo')['tn'].rank(pct=True)\n",
    "    data['customer_rank_period'] = data.groupby(['periodo', 'customer_id'])['tn'].rank(pct=True)\n",
    "    \n",
    "    # Rankings históricos\n",
    "    data['product_rank_historical'] = data.groupby('product_id')['tn'].rank(pct=True)\n",
    "    data['customer_rank_historical'] = data.groupby('customer_id')['tn'].rank(pct=True)\n",
    "    \n",
    "    # 12. Features de frecuencia y consistencia\n",
    "    print(\"🔄 Creando features de frecuencia...\")\n",
    "    \n",
    "    # Conteo de períodos activos\n",
    "    data['periods_active'] = data.groupby(['product_id', 'customer_id']).cumcount() + 1\n",
    "    data['total_periods'] = data.groupby(['product_id', 'customer_id'])['periodo'].transform('count')\n",
    "    data['activity_ratio'] = data['periods_active'] / data['total_periods']\n",
    "    \n",
    "    # Consistencia de ventas\n",
    "    for window in [6, 12]:\n",
    "        rolling_group = data.groupby(['product_id', 'customer_id'])['tn']\n",
    "        data[f'consistency_{window}'] = rolling_group.transform(\n",
    "            lambda x: (x.rolling(window, min_periods=1).apply(lambda y: (y > 0).mean()))\n",
    "        )\n",
    "    \n",
    "    # 13. Features avanzadas de detección de patrones\n",
    "    print(\"🔍 Creando features de patrones...\")\n",
    "    \n",
    "    # Detectar picos y valles\n",
    "    for window in [3, 6]:\n",
    "        rolling_mean = data[f'sales_rolling_mean_{window}']\n",
    "        rolling_std = data[f'sales_rolling_std_{window}']\n",
    "        data[f'is_peak_{window}'] = (data['tn'] > rolling_mean + 2 * rolling_std).astype(int)\n",
    "        data[f'is_valley_{window}'] = (data['tn'] < rolling_mean - rolling_std).astype(int)\n",
    "    \n",
    "    # Detectar estacionalidad\n",
    "    data['seasonal_strength'] = np.abs(data['sin_month_12'])\n",
    "    \n",
    "    # 14. Features de target encoding\n",
    "    print(\"🎯 Creando target encoding...\")\n",
    "\n",
    "    # Target encoding con validación cruzada para evitar overfitting\n",
    "    for cat_col in ['cat1', 'cat2', 'cat3', 'brand']:\n",
    "        if cat_col in data.columns:\n",
    "            print(f\"   Procesando {cat_col}...\")\n",
    "            try:\n",
    "                # Media global como fallback\n",
    "                global_mean = data['tn'].mean()\n",
    "                \n",
    "                # Target encoding suavizado\n",
    "                cat_stats = data.groupby(cat_col)['tn'].agg(['mean', 'count']).reset_index()\n",
    "                cat_stats['smoothed_mean'] = (cat_stats['mean'] * cat_stats['count'] + global_mean * 10) / (cat_stats['count'] + 10)\n",
    "                \n",
    "                # Crear mapeo\n",
    "                mapping = dict(zip(cat_stats[cat_col], cat_stats['smoothed_mean']))\n",
    "                \n",
    "                # Aplicar mapeo directamente sin merge\n",
    "                data[f'{cat_col}_target_encoded'] = data[cat_col].map(mapping).fillna(global_mean)\n",
    "                \n",
    "                print(f\"   ✅ {cat_col}_target_encoded creada\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error en {cat_col}: {e}\")\n",
    "    \n",
    "    # 15. Limpieza final y validaciones\n",
    "    print(\"🧹 Limpieza final...\")\n",
    "    \n",
    "    # Reemplazar infinitos y valores extremos\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Imputación inteligente\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'tn':  # No imputar el target\n",
    "            if data[col].isna().sum() > 0:\n",
    "                # Usar mediana para imputación robusta\n",
    "                data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    # Llenar categóricas\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].fillna('unknown')\n",
    "    \n",
    "    print(f\"✅ Features v2 creadas. Shape final: {data.shape}\")\n",
    "    print(f\"📊 Features numéricas: {len(data.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"📊 Features categóricas: {len(data.select_dtypes(include=['object']).columns)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MODELOS DE SERIES DE TIEMPO\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesModels:\n",
    "    \"\"\"Clase para manejar modelos específicos de series de tiempo\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.fitted_models = {}\n",
    "    \n",
    "    def prepare_time_series_data(self, data, product_id):\n",
    "        \"\"\"Prepara datos para series de tiempo de un producto específico\"\"\"\n",
    "        product_data = data[data['product_id'] == product_id].copy()\n",
    "        product_data = product_data.sort_values('periodo')\n",
    "        \n",
    "        # Crear series temporal completa (rellenar períodos faltantes)\n",
    "        date_range = pd.date_range(\n",
    "            start=product_data['periodo'].min(),\n",
    "            end=product_data['periodo'].max(),\n",
    "            freq='M'\n",
    "        )\n",
    "        \n",
    "        # Reindexar y rellenar\n",
    "        product_data = product_data.set_index('periodo')\n",
    "        product_data = product_data.reindex(date_range, fill_value=0)\n",
    "        product_data.index.name = 'periodo'\n",
    "        product_data = product_data.reset_index()\n",
    "        \n",
    "        return product_data\n",
    "    \n",
    "    def fit_arima(self, ts_data, order=(1,1,1)):\n",
    "        \"\"\"Ajusta modelo ARIMA\"\"\"\n",
    "        try:\n",
    "            if len(ts_data) < 10:\n",
    "                return None, None\n",
    "                \n",
    "            # Eliminar ceros iniciales si existen\n",
    "            first_nonzero = np.argmax(ts_data > 0)\n",
    "            if first_nonzero > 0:\n",
    "                ts_data = ts_data[first_nonzero:]\n",
    "            \n",
    "            if len(ts_data) < 5:\n",
    "                return None, None\n",
    "            \n",
    "            model = ARIMA(ts_data, order=order)\n",
    "            fitted_model = model.fit()\n",
    "            return fitted_model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_sarima(self, ts_data, order=(1,1,1), seasonal_order=(0,1,1,12)):\n",
    "        \"\"\"Ajusta modelo SARIMA\"\"\"\n",
    "        try:\n",
    "            if len(ts_data) < 24:  # Necesita al menos 2 años para estacionalidad\n",
    "                return None, \"Insufficient data for SARIMA\"\n",
    "                \n",
    "            model = SARIMAX(ts_data, order=order, seasonal_order=seasonal_order)\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            return fitted_model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_prophet(self, data, product_id):\n",
    "        \"\"\"Ajusta modelo Prophet\"\"\"\n",
    "        if not PROPHET_AVAILABLE:\n",
    "            return None, \"Prophet not available\"\n",
    "            \n",
    "        try:\n",
    "            product_data = self.prepare_time_series_data(data, product_id)\n",
    "            \n",
    "            if len(product_data) < 10:\n",
    "                return None, \"Insufficient data\"\n",
    "            \n",
    "            # Preparar datos para Prophet\n",
    "            prophet_data = pd.DataFrame({\n",
    "                'ds': product_data['periodo'],\n",
    "                'y': product_data['tn']\n",
    "            })\n",
    "            \n",
    "            # Configurar Prophet\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            \n",
    "            # Agregar regresores externos si están disponibles\n",
    "            if 'v_m_IPC' in product_data.columns:\n",
    "                model.add_regressor('ipc')\n",
    "                prophet_data['ipc'] = product_data['v_m_IPC'].fillna(method='ffill')\n",
    "            \n",
    "            model.fit(prophet_data)\n",
    "            return model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_auto_arima(self, ts_data):\n",
    "        \"\"\"Ajusta Auto-ARIMA\"\"\"\n",
    "        if not AUTO_ARIMA_AVAILABLE:\n",
    "            return None, \"Auto-ARIMA not available\"\n",
    "            \n",
    "        try:\n",
    "            if len(ts_data) < 10:\n",
    "                return None, \"Insufficient data\"\n",
    "            \n",
    "            model = auto_arima(\n",
    "                ts_data,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=3, max_q=3,\n",
    "                seasonal=True,\n",
    "                start_P=0, start_Q=0,\n",
    "                max_P=2, max_Q=2,\n",
    "                m=12,\n",
    "                stepwise=True,\n",
    "                suppress_warnings=True,\n",
    "                error_action='ignore'\n",
    "            )\n",
    "            \n",
    "            return model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def predict_time_series(self, data, products_list, target_date='2020-02-01'):\n",
    "        \"\"\"Genera predicciones usando modelos de series de tiempo\"\"\"\n",
    "        print(\"⏰ Generando predicciones con modelos de series de tiempo...\")\n",
    "        \n",
    "        predictions = {}\n",
    "        errors = {}\n",
    "        \n",
    "        for product_id in tqdm(products_list, desc=\"Time Series Predictions\"):\n",
    "            try:\n",
    "                product_data = self.prepare_time_series_data(data, product_id)\n",
    "                product_sales = data[data['product_id'] == product_id]['tn'].values\n",
    "                \n",
    "                if len(product_sales) == 0:\n",
    "                    predictions[product_id] = {'arima': 0, 'sarima': 0, 'prophet': 0, 'auto_arima': 0}\n",
    "                    continue\n",
    "                \n",
    "                pred_dict = {}\n",
    "                \n",
    "                # ARIMA\n",
    "                arima_model, arima_error = self.fit_arima(product_sales)\n",
    "                if arima_model is not None:\n",
    "                    try:\n",
    "                        forecast = arima_model.forecast(steps=1)\n",
    "                        pred_dict['arima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # SARIMA\n",
    "                sarima_model, sarima_error = self.fit_sarima(product_sales)\n",
    "                if sarima_model is not None:\n",
    "                    try:\n",
    "                        forecast = sarima_model.forecast(steps=1)\n",
    "                        pred_dict['sarima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['sarima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['sarima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # Prophet\n",
    "                prophet_model, prophet_error = self.fit_prophet(data, product_id)\n",
    "                if prophet_model is not None:\n",
    "                    try:\n",
    "                        future = prophet_model.make_future_dataframe(periods=1, freq='M')\n",
    "                        if 'ipc' in future.columns:\n",
    "                            future['ipc'] = future['ipc'].fillna(method='ffill')\n",
    "                        forecast = prophet_model.predict(future)\n",
    "                        pred_dict['prophet'] = max(0, forecast['yhat'].iloc[-1])\n",
    "                    except:\n",
    "                        pred_dict['prophet'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['prophet'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # Auto-ARIMA\n",
    "                auto_arima_model, auto_arima_error = self.fit_auto_arima(product_sales)\n",
    "                if auto_arima_model is not None:\n",
    "                    try:\n",
    "                        forecast = auto_arima_model.predict(n_periods=1)\n",
    "                        pred_dict['auto_arima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['auto_arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['auto_arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                predictions[product_id] = pred_dict\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors[product_id] = str(e)\n",
    "                predictions[product_id] = {'arima': 0, 'sarima': 0, 'prophet': 0, 'auto_arima': 0}\n",
    "        \n",
    "        return predictions, errors\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENSEMBLE AVANZADO Y STACKING\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedEnsemble:\n",
    "    \"\"\"Ensemble avanzado con stacking multinivel\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.level1_models = {}\n",
    "        self.level2_models = {}\n",
    "        self.stacking_model = None\n",
    "        self.model_weights = {}\n",
    "        \n",
    "    def create_level1_models(self):\n",
    "        \"\"\"Crea modelos de primer nivel\"\"\"\n",
    "        models = {\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'catboost': CatBoostRegressor(\n",
    "                iterations=300,\n",
    "                depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                random_state=self.random_state,\n",
    "                verbose=False\n",
    "            ),\n",
    "            'gb': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'extra_trees': ExtraTreesRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'ridge': Ridge(alpha=1.0),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        self.level1_models = models\n",
    "        return models\n",
    "    \n",
    "    def create_stacking_model(self):\n",
    "        \"\"\"Crea modelo de stacking de segundo nivel\"\"\"\n",
    "        stacking_models = [\n",
    "            ('ridge', Ridge(alpha=0.1)),\n",
    "            ('lasso', Lasso(alpha=0.01, random_state=self.random_state)),\n",
    "            ('xgb_stack', xgb.XGBRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        self.stacking_model = VotingRegressor(\n",
    "            estimators=stacking_models,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return self.stacking_model\n",
    "    \n",
    "    def fit_ensemble(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrena ensemble con validación cruzada\"\"\"\n",
    "        print(\"🎯 Entrenando ensemble avanzado...\")\n",
    "        \n",
    "        # Crear modelos\n",
    "        self.create_level1_models()\n",
    "        \n",
    "        # Entrenar modelos de nivel 1\n",
    "        level1_predictions_train = np.zeros((len(X_train), len(self.level1_models)))\n",
    "        level1_predictions_val = np.zeros((len(X_val), len(self.level1_models)))\n",
    "        \n",
    "        model_scores = {}\n",
    "        \n",
    "        for i, (name, model) in enumerate(tqdm(self.level1_models.items(), desc=\"Training Level 1\")):\n",
    "            try:\n",
    "                # Entrenar modelo\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predicciones\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_val = model.predict(X_val)\n",
    "                \n",
    "                level1_predictions_train[:, i] = pred_train\n",
    "                level1_predictions_val[:, i] = pred_val\n",
    "                \n",
    "                # Evaluar\n",
    "                val_mae = mean_absolute_error(y_val, pred_val)\n",
    "                val_rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "                val_r2 = r2_score(y_val, pred_val)\n",
    "                \n",
    "                model_scores[name] = {\n",
    "                    'mae': val_mae,\n",
    "                    'rmse': val_rmse,\n",
    "                    'r2': val_r2\n",
    "                }\n",
    "                \n",
    "                print(f\"   ✅ {name}: MAE={val_mae:.3f}, RMSE={val_rmse:.3f}, R2={val_r2:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Error en {name}: {e}\")\n",
    "                level1_predictions_train[:, i] = y_train.mean()\n",
    "                level1_predictions_val[:, i] = y_train.mean()\n",
    "        \n",
    "        # Entrenar modelo de stacking\n",
    "        self.create_stacking_model()\n",
    "        self.stacking_model.fit(level1_predictions_train, y_train)\n",
    "        \n",
    "        # Predicción final del ensemble\n",
    "        final_pred = self.stacking_model.predict(level1_predictions_val)\n",
    "        \n",
    "        # Evaluar ensemble\n",
    "        ensemble_mae = mean_absolute_error(y_val, final_pred)\n",
    "        ensemble_rmse = np.sqrt(mean_squared_error(y_val, final_pred))\n",
    "        ensemble_r2 = r2_score(y_val, final_pred)\n",
    "        \n",
    "        print(f\"🏆 ENSEMBLE FINAL: MAE={ensemble_mae:.3f}, RMSE={ensemble_rmse:.3f}, R2={ensemble_r2:.3f}\")\n",
    "        \n",
    "        return model_scores, {\n",
    "            'mae': ensemble_mae,\n",
    "            'rmse': ensemble_rmse,\n",
    "            'r2': ensemble_r2\n",
    "        }\n",
    "    \n",
    "    def predict_ensemble(self, X_test):\n",
    "        \"\"\"Genera predicciones del ensemble\"\"\"\n",
    "        level1_predictions = np.zeros((len(X_test), len(self.level1_models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(self.level1_models.items()):\n",
    "            try:\n",
    "                level1_predictions[:, i] = model.predict(X_test)\n",
    "            except:\n",
    "                level1_predictions[:, i] = 0\n",
    "        \n",
    "        return self.stacking_model.predict(level1_predictions)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OPTIMIZACIÓN DE HIPERPARÁMETROS CON OPTUNA\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train, X_val, y_val, model_type='xgb', n_trials=100):\n",
    "    \"\"\"Optimiza hiperparámetros usando Optuna\"\"\"\n",
    "    print(f\"🔧 Optimizando {model_type} con Optuna...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_type == 'xgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            \n",
    "        elif model_type == 'lgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "                'depth': trial.suggest_int('depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'verbose': False\n",
    "            }\n",
    "            model = CatBoostRegressor(**params)\n",
    "        \n",
    "        elif model_type == 'rf':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Modelo {model_type} no soportado\")\n",
    "        \n",
    "        # Entrenar y evaluar\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        \n",
    "        return rmse\n",
    "    \n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   🏆 Mejor RMSE: {study.best_value:.4f}\")\n",
    "    print(f\"   📊 Mejores parámetros: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# ============================================================================\n",
    "# 7. PIPELINE PRINCIPAL EXTENDIDO\n",
    "# ============================================================================\n",
    "\n",
    "def run_advanced_nocturnal_pipeline():\n",
    "    \"\"\"Pipeline principal con todos los modelos y optimizaciones\"\"\"\n",
    "    print(\"🚀 INICIANDO PIPELINE AVANZADO NOCTURNO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Carga de datos\n",
    "    print(\"\\n1️⃣ CARGA DE DATOS\")\n",
    "    sales, stocks, product_info, products_to_predict = load_and_prepare_data()\n",
    "    if sales is None:\n",
    "        print(\"❌ Error cargando datos principales\")\n",
    "        return\n",
    "    \n",
    "    indec_data = load_indec_data()\n",
    "    \n",
    "    # 2. Feature Engineering Avanzado\n",
    "    print(\"\\n2️⃣ FEATURE ENGINEERING AVANZADO\")\n",
    "    data = create_advanced_features_v2(sales, stocks, product_info, indec_data)\n",
    "    \n",
    "    # 3. Agregación y preparación (CORREGIDA)\n",
    "    print(\"\\n3️⃣ AGREGACIÓN DE DATOS\")\n",
    "    print(\"📊 Agregando datos por producto-cliente...\")\n",
    "\n",
    "    # Filtrar datos hasta diciembre 2019\n",
    "    data_filtered = data[data['periodo'] <= '2019-12-01'].copy()\n",
    "\n",
    "    # REDUCIR FEATURES ANTES DE AGREGAR para ahorrar memoria\n",
    "    print(\"🔧 Reduciendo features para optimizar memoria...\")\n",
    "\n",
    "    # Verificar y limpiar columnas de agrupación\n",
    "    print(\"🔍 Verificando columnas de agrupación...\")\n",
    "    grouping_cols = ['product_id', 'customer_id', 'periodo']\n",
    "\n",
    "    for col in grouping_cols:\n",
    "        if col in data_filtered.columns:\n",
    "            print(f\"   {col}: {data_filtered[col].dtype}, shape: {data_filtered[col].shape}\")\n",
    "            # Asegurar que sea unidimensional\n",
    "            if len(data_filtered[col].shape) > 1:\n",
    "                data_filtered[col] = data_filtered[col].iloc[:, 0]  # Tomar primera columna si es multidimensional\n",
    "        else:\n",
    "            print(f\"   ⚠️ {col} no encontrada en el dataset\")\n",
    "\n",
    "    # Eliminar posibles columnas duplicadas\n",
    "    print(\"🧹 Eliminando columnas duplicadas...\")\n",
    "    data_filtered = data_filtered.loc[:, ~data_filtered.columns.duplicated()]\n",
    "\n",
    "    # Mantener solo las features más importantes\n",
    "    numeric_cols = data_filtered.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in grouping_cols + ['tn']]\n",
    "\n",
    "    # Seleccionar top 50 features numéricas (ajusta según necesidad)\n",
    "    important_features = grouping_cols + ['tn'] + numeric_cols[:50]\n",
    "    \n",
    "    # Filtrar solo features importantes y eliminar duplicados\n",
    "    important_features = list(dict.fromkeys(important_features))  # Eliminar duplicados manteniendo orden\n",
    "    data_filtered = data_filtered[important_features]\n",
    "\n",
    "    print(f\"📊 Features reducidas a {len(data_filtered.columns)}\")\n",
    "    print(f\"📊 Columnas finales: {list(data_filtered.columns)}\")\n",
    "\n",
    "    # Verificar tipos de datos\n",
    "    print(\"🔍 Verificando tipos de datos finales...\")\n",
    "    for col in grouping_cols:\n",
    "        if col in data_filtered.columns:\n",
    "            print(f\"   {col}: {data_filtered[col].dtype}\")\n",
    "\n",
    "    # Agregación por chunks para manejar memoria\n",
    "    print(\"🔄 Agregando por chunks...\")\n",
    "    chunk_size = 100000\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(data_filtered), chunk_size):\n",
    "        try:\n",
    "            chunk = data_filtered.iloc[i:i+chunk_size].copy()\n",
    "            \n",
    "            # Verificar que las columnas de agrupación existan y sean válidas\n",
    "            missing_cols = [col for col in grouping_cols if col not in chunk.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"   ⚠️ Columnas faltantes en chunk {i//chunk_size + 1}: {missing_cols}\")\n",
    "                continue\n",
    "                \n",
    "            # Crear diccionario de agregación dinámicamente\n",
    "            agg_dict = {'tn': 'sum'}\n",
    "            other_cols = [col for col in chunk.columns if col not in grouping_cols + ['tn']]\n",
    "            \n",
    "            for col in other_cols:\n",
    "                if chunk[col].dtype in ['object', 'category']:\n",
    "                    agg_dict[col] = 'first'\n",
    "                else:\n",
    "                    agg_dict[col] = 'mean'  # Usar mean para numéricas\n",
    "            \n",
    "            chunk_agg = chunk.groupby(grouping_cols).agg(agg_dict).reset_index()\n",
    "            chunks.append(chunk_agg)\n",
    "            print(f\"   Chunk {i//chunk_size + 1}: {len(chunk_agg)} registros\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error en chunk {i//chunk_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"❌ No se pudieron procesar chunks. Intentando agregación simple...\")\n",
    "        \n",
    "        # Fallback: agregación simple con menos features\n",
    "        essential_cols = ['product_id', 'customer_id', 'periodo', 'tn']\n",
    "        data_simple = data_filtered[essential_cols].copy()\n",
    "        \n",
    "        agg_data = data_simple.groupby(['product_id', 'customer_id', 'periodo']).agg({\n",
    "            'tn': 'sum'\n",
    "        }).reset_index()\n",
    "    else:\n",
    "        # Combinar chunks\n",
    "        print(\"🔗 Combinando chunks...\")\n",
    "        agg_data = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        # Reagrupar para consolidar posibles duplicados\n",
    "        if len(chunks) > 1:\n",
    "            print(\"🔄 Consolidando datos finales...\")\n",
    "            final_agg_dict = {'tn': 'sum'}\n",
    "            other_cols = [col for col in agg_data.columns if col not in grouping_cols + ['tn']]\n",
    "            \n",
    "            for col in other_cols:\n",
    "                if agg_data[col].dtype in ['object', 'category']:\n",
    "                    final_agg_dict[col] = 'first'\n",
    "                else:\n",
    "                    final_agg_dict[col] = 'mean'\n",
    "            \n",
    "            agg_data = agg_data.groupby(grouping_cols).agg(final_agg_dict).reset_index()\n",
    "\n",
    "    print(f\"✅ Datos agregados: {agg_data.shape}\")\n",
    "    \n",
    "    # 4. Splits temporales\n",
    "    print(\"\\n4️⃣ CREACIÓN DE SPLITS TEMPORALES\")\n",
    "    cutoff_date = pd.to_datetime('2019-10-01')\n",
    "    \n",
    "    train_data = agg_data[agg_data['periodo'] <= cutoff_date]\n",
    "    val_data = agg_data[agg_data['periodo'] > cutoff_date]\n",
    "    \n",
    "    print(f\"📊 Training: {len(train_data):,} registros (hasta {cutoff_date.strftime('%Y-%m')})\")\n",
    "    print(f\"📊 Validation: {len(val_data):,} registros (desde {cutoff_date.strftime('%Y-%m')})\")\n",
    "    \n",
    "    # Preparar features\n",
    "    feature_cols = [col for col in agg_data.columns if col not in ['tn', 'periodo', 'product_id', 'customer_id']]\n",
    "    \n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['tn']\n",
    "    X_val = val_data[feature_cols]\n",
    "    y_val = val_data['tn']\n",
    "    \n",
    "    # Crear un diccionario para almacenar los escaladores\n",
    "    # Remove target scaling - train on original scale\n",
    "    scalers = {}\n",
    "    # Don't scale the target variable\n",
    "    y_train_for_training = y_train  # Use original scale\n",
    "\n",
    "    # Then in your model training loop, use:\n",
    "    #model.fit(X_train_processed, y_train_for_training)  # Instead of y_train_scaled\n",
    "\n",
    "    # Handling missing values y encoding\n",
    "    print(\"🔧 Procesando features...\")\n",
    "    \n",
    "    # Separar numéricas y categóricas\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', RobustScaler(with_centering=True, with_scaling=True))\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Ajustar preprocessor\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    print(f\"✅ Features procesadas: {X_train_processed.shape[1]} dimensiones\")\n",
    "    \n",
    "    # 5. Modelos de Machine Learning\n",
    "    print(\"\\n5️⃣ EVALUACIÓN DE MODELOS DE ML\")\n",
    "    \n",
    "    models_to_test = {\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, \n",
    "            n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostRegressor(\n",
    "            iterations=300, depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=RANDOM_STATE, verbose=False\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingRegressor(\n",
    "            n_estimators=200, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        'ExtraTrees': ExtraTreesRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=0.1, random_state=RANDOM_STATE),\n",
    "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    ml_results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    for name, model in tqdm(models_to_test.items(), desc=\"Evaluando modelos ML\"):\n",
    "        try:\n",
    "            # Entrenar\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Predecir\n",
    "            y_pred = model.predict(X_val_processed)\n",
    "            \n",
    "            # Métricas\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred) * 100\n",
    "            \n",
    "            ml_results[name] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2,\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            \n",
    "            best_models[name] = model\n",
    "            \n",
    "            print(f\"✅ {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}, MAPE: {mape:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en {name}: {e}\")\n",
    "    \n",
    "    # 6. Optimización de hiperparámetros (para los mejores modelos)\n",
    "    if MODEL_CONFIG['use_hyperparameter_tuning']:\n",
    "        print(\"\\n6️⃣ OPTIMIZACIÓN DE HIPERPARÁMETROS\")\n",
    "        \n",
    "        # Seleccionar top 3 modelos\n",
    "        top_models = sorted(ml_results.items(), key=lambda x: x[1]['RMSE'])[:3]\n",
    "        optimized_models = {}\n",
    "        \n",
    "        for model_name, _ in top_models:\n",
    "            if model_name.lower() in ['xgboost', 'lightgbm', 'catboost']:\n",
    "                print(f\"🔧 Optimizando {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    best_params = optimize_hyperparameters(\n",
    "                        X_train_processed, y_train, X_val_processed, y_val,\n",
    "                        model_type=model_name.lower().replace('boost', 'b').replace('gradient', 'gb'),\n",
    "                        n_trials=MODEL_CONFIG['optuna_trials']\n",
    "                    )\n",
    "                    \n",
    "                    # Crear modelo optimizado\n",
    "                    if 'xg' in model_name.lower():\n",
    "                        optimized_model = xgb.XGBRegressor(**best_params)\n",
    "                    elif 'light' in model_name.lower():\n",
    "                        optimized_model = lgb.LGBMRegressor(**best_params)\n",
    "                    elif 'cat' in model_name.lower():\n",
    "                        optimized_model = CatBoostRegressor(**best_params)\n",
    "                    \n",
    "                    # Evaluar modelo optimizado\n",
    "                    optimized_model.fit(X_train_processed, y_train)\n",
    "                    y_pred_opt = optimized_model.predict(X_val_processed)\n",
    "                    \n",
    "                    mae_opt = mean_absolute_error(y_val, y_pred_opt)\n",
    "                    rmse_opt = np.sqrt(mean_squared_error(y_val, y_pred_opt))\n",
    "                    r2_opt = r2_score(y_val, y_pred_opt)\n",
    "                    mape_opt = mean_absolute_percentage_error(y_val, y_pred_opt) * 100\n",
    "                    \n",
    "                    optimized_models[f\"{model_name}_Optimized\"] = {\n",
    "                        'model': optimized_model,\n",
    "                        'MAE': mae_opt,\n",
    "                        'RMSE': rmse_opt,\n",
    "                        'R2': r2_opt,\n",
    "                        'MAPE': mape_opt\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"✅ {model_name} Optimizado - MAE: {mae_opt:.2f}, RMSE: {rmse_opt:.2f}, R2: {r2_opt:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error optimizando {model_name}: {e}\")\n",
    "    \n",
    "    # 7. Modelos de Series de Tiempo\n",
    "    if MODEL_CONFIG['use_time_series_models']:\n",
    "        print(\"\\n7️⃣ MODELOS DE SERIES DE TIEMPO\")\n",
    "        \n",
    "        ts_models = TimeSeriesModels(random_state=RANDOM_STATE)\n",
    "        products_list = products_to_predict.iloc[:, 0].tolist()[:50]  # Limitamos a 50 para prueba nocturna\n",
    "        \n",
    "        ts_predictions, ts_errors = ts_models.predict_time_series(\n",
    "            data_filtered, products_list, TARGET_DATE\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Predicciones de series de tiempo completadas para {len(ts_predictions)} productos\")\n",
    "        if ts_errors:\n",
    "            print(f\"⚠️ Errores en {len(ts_errors)} productos\")\n",
    "    \n",
    "    # 8. Ensemble Avanzado\n",
    "    if MODEL_CONFIG['use_ensemble_stacking']:\n",
    "        print(\"\\n8️⃣ ENSEMBLE AVANZADO\")\n",
    "        \n",
    "        ensemble = AdvancedEnsemble(random_state=RANDOM_STATE)\n",
    "        model_scores, ensemble_score = ensemble.fit_ensemble(\n",
    "            X_train_processed, y_train, X_val_processed, y_val\n",
    "        )\n",
    "        \n",
    "        # Agregar ensemble a resultados\n",
    "        ml_results['AdvancedEnsemble'] = ensemble_score\n",
    "        best_models['AdvancedEnsemble'] = ensemble\n",
    "    \n",
    "    # 9. Selección del mejor modelo\n",
    "    print(\"\\n9️⃣ SELECCIÓN DEL MEJOR MODELO\")\n",
    "    \n",
    "    # Combinar resultados normales y optimizados\n",
    "    all_results = ml_results.copy()\n",
    "    for name, result in optimized_models.items():\n",
    "        all_results[name] = {\n",
    "            'MAE': result['MAE'],\n",
    "            'RMSE': result['RMSE'],\n",
    "            'R2': result['R2'],\n",
    "            'MAPE': result['MAPE']\n",
    "        }\n",
    "        best_models[name] = result['model']\n",
    "    \n",
    "    # Crear DataFrame de resultados\n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    results_df = results_df.sort_values('RMSE')\n",
    "    \n",
    "    print(\"\\n📊 RESUMEN DE RESULTADOS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results_df.round(3))\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_model_name = results_df.index[0]\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\n🏆 MEJOR MODELO: {best_model_name}\")\n",
    "    print(f\"   RMSE: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "    print(f\"   MAE: {results_df.loc[best_model_name, 'MAE']:.3f}\")\n",
    "    print(f\"   R2: {results_df.loc[best_model_name, 'R2']:.3f}\")\n",
    "    \n",
    "    # 10. Reentrenamiento con datos completos\n",
    "    print(\"\\n🔟 REENTRENAMIENTO CON DATOS COMPLETOS\")\n",
    "    print(\"🔄 Reentrenando el mejor modelo con todos los datos disponibles...\")\n",
    "    \n",
    "    # Combinar train y validation\n",
    "    X_full = np.vstack([X_train_processed, X_val_processed])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Reentrenar\n",
    "    best_model.fit(X_full, y_full)\n",
    "    \n",
    "    # 11. PREDICCIONES FINALES\n",
    "    print(\"\\n1️⃣1️⃣ PREDICCIONES FINALES\")\n",
    "    print(f\"🎯 Generando predicciones para {TARGET_DATE}...\")\n",
    "\n",
    "    # Preparar datos para predicción\n",
    "    products_list = products_to_predict.iloc[:, 0].unique()\n",
    "    all_predictions = []\n",
    "\n",
    "    # Obtener lista única de clientes por producto\n",
    "    product_customer_pairs = agg_data.groupby('product_id')['customer_id'].unique()\n",
    "\n",
    "    for product_id in tqdm(products_list, desc=\"Generando predicciones\"):\n",
    "        try:\n",
    "            # Obtener todos los clientes históricos para este producto\n",
    "            customers = product_customer_pairs.get(product_id, [])\n",
    "            if len(customers) == 0:\n",
    "                # Si no hay clientes históricos, usar predicción por defecto\n",
    "                all_predictions.append({\n",
    "                    'product_id': product_id,\n",
    "                    'prediction': 0,\n",
    "                    'method': 'default'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            product_predictions = []\n",
    "            \n",
    "            # Generar predicción para cada cliente del producto\n",
    "            for customer_id in customers:\n",
    "                try:\n",
    "                    # Obtener último registro del par producto-cliente\n",
    "                    customer_data = agg_data[\n",
    "                        (agg_data['product_id'] == product_id) & \n",
    "                        (agg_data['customer_id'] == customer_id)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(customer_data) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Crear registro para período futuro\n",
    "                    last_record = customer_data.sort_values('periodo').iloc[-1:].copy()\n",
    "                    future_record = last_record.copy()\n",
    "                    target_period = pd.to_datetime(TARGET_DATE)\n",
    "                    \n",
    "                    # Actualizar features temporales\n",
    "                    if 'year' in feature_cols:\n",
    "                        future_record['year'] = target_period.year\n",
    "                    if 'month' in feature_cols:\n",
    "                        future_record['month'] = target_period.month\n",
    "                    if 'quarter' in feature_cols:\n",
    "                        future_record['quarter'] = (target_period.month - 1) // 3 + 1\n",
    "\n",
    "                    # Preparar features\n",
    "                    prediction_features = future_record[feature_cols]\n",
    "                    prediction_features = prediction_features.fillna(\n",
    "                        customer_data[feature_cols].median().fillna(0)\n",
    "                    )\n",
    "\n",
    "                    # Procesar features\n",
    "                    last_record_processed = preprocessor.transform(prediction_features)\n",
    "                    \n",
    "                    # Predecir\n",
    "                    if best_model_name == 'AdvancedEnsemble':\n",
    "                        pred = best_model.predict_ensemble(last_record_processed)[0]\n",
    "                    else:\n",
    "                        pred = best_model.predict(last_record_processed)[0]\n",
    "\n",
    "                    # Validar predicción\n",
    "                    pred = max(0, pred)  # No predicciones negativas\n",
    "                    historical_mean = customer_data['tn'].mean()\n",
    "                    \n",
    "                    if pred < historical_mean * 0.01:  # Si es menor al 1% del histórico\n",
    "                        pred = historical_mean * 0.1  # Usar 10% del histórico como mínimo\n",
    "                    \n",
    "                    product_predictions.append(pred)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en cliente {customer_id} para producto {product_id}: {e}\")\n",
    "                    # Usar media histórica del cliente como fallback\n",
    "                    historical_pred = customer_data['tn'].mean()\n",
    "                    if not np.isnan(historical_pred):\n",
    "                        product_predictions.append(historical_pred)\n",
    "\n",
    "            # Sumar todas las predicciones de clientes para obtener la predicción del producto\n",
    "            final_prediction = sum(product_predictions) if product_predictions else 0\n",
    "            \n",
    "            # Guardar predicción agregada del producto\n",
    "            all_predictions.append({\n",
    "                'product_id': product_id,\n",
    "                'prediction': final_prediction,\n",
    "                'method': best_model_name,\n",
    "                'n_customers': len(product_predictions)\n",
    "            })\n",
    "            \n",
    "            if len(all_predictions) % 50 == 0:\n",
    "                print(f\"\\nProducto {product_id}:\")\n",
    "                print(f\"Clientes procesados: {len(product_predictions)}\")\n",
    "                print(f\"Predicción total: {final_prediction:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en producto {product_id}: {e}\")\n",
    "            # Usar el promedio histórico como fallback\n",
    "            historical_pred = agg_data[agg_data['product_id'] == product_id]['tn'].sum()\n",
    "            all_predictions.append({\n",
    "                'product_id': product_id,\n",
    "                'prediction': max(0, historical_pred),\n",
    "                'method': 'historical_fallback',\n",
    "                'n_customers': 0\n",
    "            })\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    predictions_df = pd.DataFrame(all_predictions)\n",
    "    \n",
    "    print(f\"✅ Predicciones completadas:\")\n",
    "    print(f\"   Total: {len(predictions_df)}\")\n",
    "    print(f\"   Con modelo: {len(predictions_df[predictions_df['method'] != 'error'])}\")\n",
    "    print(f\"   Errores: {len(predictions_df[predictions_df['method'] == 'error'])}\")\n",
    "    \n",
    "    # 12. Guardar resultados\n",
    "    print(\"\\n1️⃣2️⃣ GUARDANDO RESULTADOS\")\n",
    "    \n",
    "\n",
    "    # Agrupar por producto y sumar predicciones\n",
    "    submission = predictions_df.groupby('product_id', as_index=False)['prediction'].sum()\n",
    "    submission = submission.rename(columns={'prediction': 'target_202002'})\n",
    "    # Crear submission\n",
    "    # Crear submission\n",
    "    #submission = pd.DataFrame({\n",
    "    #    'product_id': validated_predictions['product_id'],\n",
    "    #    'target_202002': validated_predictions['prediction']\n",
    "    #})\n",
    "    \n",
    "    # Estadísticas de predicciones\n",
    "    print(f\"\\n📈 ESTADÍSTICAS DE PREDICCIONES:\")\n",
    "    print(f\"   Media: {submission['target_202002'].mean():.2f}\")\n",
    "    print(f\"   Mediana: {submission['target_202002'].median():.2f}\")\n",
    "    print(f\"   Std: {submission['target_202002'].std():.2f}\")\n",
    "    print(f\"   Min: {submission['target_202002'].min():.2f}\")\n",
    "    print(f\"   Max: {submission['target_202002'].max():.2f}\")\n",
    "    print(f\"   Predicciones = 0: {(submission['target_202002'] == 0).sum()}\")\n",
    "    # Generar submissions para todos los modelos\n",
    "    \n",
    "    if MODEL_CONFIG.get('generate_multiple_submissions', True):\n",
    "        print(\"\\n📊 GENERANDO SUBMISSIONS MÚLTIPLES\")\n",
    "        \n",
    "        # Generar predicciones para cada modelo\n",
    "        for model_name, model in best_models.items():\n",
    "            if model_name == best_model_name:\n",
    "                continue  # Ya lo hicimos antes\n",
    "            \n",
    "            print(f\"\\nGenerando predicciones para {model_name}...\")\n",
    "            model_predictions = []\n",
    "            \n",
    "            # Iterar sobre cada producto\n",
    "            for product_id in tqdm(products_list, desc=f\"Prediciendo con {model_name}\"):\n",
    "                try:\n",
    "                    # Obtener todos los clientes históricos para este producto\n",
    "                    customers = product_customer_pairs.get(product_id, [])\n",
    "                    if len(customers) == 0:\n",
    "                        model_predictions.append({\n",
    "                            'product_id': product_id,\n",
    "                            'target_202002': 0\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    product_predictions = []\n",
    "                    \n",
    "                    # Predecir para cada cliente del producto\n",
    "                    for customer_id in customers:\n",
    "                        try:\n",
    "                            # Obtener datos del cliente\n",
    "                            customer_data = agg_data[\n",
    "                                (agg_data['product_id'] == product_id) & \n",
    "                                (agg_data['customer_id'] == customer_id)\n",
    "                            ]\n",
    "                            \n",
    "                            if len(customer_data) == 0:\n",
    "                                continue\n",
    "\n",
    "                            # Preparar features\n",
    "                            last_record = customer_data.sort_values('periodo').iloc[-1:].copy()\n",
    "                            future_record = last_record.copy()\n",
    "                            target_period = pd.to_datetime(TARGET_DATE)\n",
    "                            \n",
    "                            # Actualizar features temporales\n",
    "                            if 'year' in feature_cols:\n",
    "                                future_record['year'] = target_period.year\n",
    "                            if 'month' in feature_cols:\n",
    "                                future_record['month'] = target_period.month\n",
    "                            if 'quarter' in feature_cols:\n",
    "                                future_record['quarter'] = (target_period.month - 1) // 3 + 1\n",
    "\n",
    "                            prediction_features = future_record[feature_cols]\n",
    "                            prediction_features = prediction_features.fillna(\n",
    "                                customer_data[feature_cols].median().fillna(0)\n",
    "                            )\n",
    "\n",
    "                            # Procesar features\n",
    "                            last_record_processed = preprocessor.transform(prediction_features)\n",
    "                            \n",
    "                            # Predecir\n",
    "                            if model_name == 'AdvancedEnsemble':\n",
    "                                pred = model.predict_ensemble(last_record_processed)[0]\n",
    "                            else:\n",
    "                                pred = model.predict(last_record_processed)[0]\n",
    "\n",
    "                            # Validar predicción\n",
    "                            pred = max(0, pred)\n",
    "                            historical_mean = customer_data['tn'].mean()\n",
    "                            \n",
    "                            if pred < historical_mean * 0.01:\n",
    "                                pred = historical_mean * 0.1\n",
    "                            \n",
    "                            product_predictions.append(pred)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error en cliente {customer_id} para producto {product_id}: {e}\")\n",
    "                            historical_pred = customer_data['tn'].mean()\n",
    "                            if not np.isnan(historical_pred):\n",
    "                                product_predictions.append(historical_pred)\n",
    "\n",
    "                    # Sumar predicciones de todos los clientes\n",
    "                    final_prediction = sum(product_predictions) if product_predictions else 0\n",
    "                    \n",
    "                    model_predictions.append({\n",
    "                        'product_id': product_id,\n",
    "                        'target_202002': final_prediction\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en producto {product_id}: {e}\")\n",
    "                    historical_pred = agg_data[agg_data['product_id'] == product_id]['tn'].sum()\n",
    "                    model_predictions.append({\n",
    "                        'product_id': product_id,\n",
    "                        'target_202002': max(0, historical_pred)\n",
    "                    })\n",
    "            \n",
    "            # Guardar submission del modelo\n",
    "            model_submission = pd.DataFrame(model_predictions)\n",
    "            model_filename = f'submission_{model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "            model_submission.to_csv(model_filename, index=False)\n",
    "            print(f\"💾 {model_name} guardado: {model_filename}\")\n",
    "            \n",
    "            # Mostrar estadísticas\n",
    "            print(f\"   Media: {model_submission['target_202002'].mean():.2f}\")\n",
    "            print(f\"   Mediana: {model_submission['target_202002'].median():.2f}\")\n",
    "            print(f\"   Max: {model_submission['target_202002'].max():.2f}\")\n",
    "    # Guardar submission\n",
    "    submission_filename = f'submission_nocturnal_{best_model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"💾 Submission guardada: {submission_filename}\")\n",
    "    \n",
    "    # Guardar resultados detallados\n",
    "    results_filename = f'results_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "    results_df.to_csv(results_filename)\n",
    "    print(f\"📊 Resultados detallados guardados: {results_filename}\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    if MODEL_CONFIG['save_model']:\n",
    "        model_filename = f'best_model_nocturnal_{best_model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.pkl'\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        \n",
    "        # Guardar también el preprocessor\n",
    "        preprocessor_filename = f'preprocessor_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.pkl'\n",
    "        joblib.dump(preprocessor, preprocessor_filename)\n",
    "        \n",
    "        print(f\"🤖 Modelo guardado: {model_filename}\")\n",
    "        print(f\"🔧 Preprocessor guardado: {preprocessor_filename}\")\n",
    "    \n",
    "    # 13. Análisis de feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(\"\\n1️⃣3️⃣ ANÁLISIS DE FEATURE IMPORTANCE\")\n",
    "        \n",
    "        # Obtener nombres de features después del preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Features numéricas\n",
    "        feature_names.extend(numeric_features)\n",
    "        \n",
    "        # Features categóricas (después de one-hot encoding)\n",
    "        if categorical_features:\n",
    "            cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "            cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "            feature_names.extend(cat_feature_names)\n",
    "        \n",
    "        # Crear DataFrame de importancias\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\n🔍 TOP 20 FEATURES MÁS IMPORTANTES:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f}\")\n",
    "        \n",
    "        # Guardar importancias\n",
    "        importance_filename = f'feature_importance_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "        importance_df.to_csv(importance_filename, index=False)\n",
    "        print(f\"\\n💾 Feature importance guardada: {importance_filename}\")\n",
    "    \n",
    "    # 14. Métricas de calidad de predicción\n",
    "    print(\"\\n1️⃣4️⃣ MÉTRICAS DE CALIDAD\")\n",
    "    \n",
    "    # Distribución de predicciones\n",
    "    pred_stats = {\n",
    "        'zero_predictions': (submission['target_202002'] == 0).sum(),\n",
    "        'low_predictions': (submission['target_202002'] < 1).sum(),\n",
    "        'medium_predictions': ((submission['target_202002'] >= 1) & (submission['target_202002'] < 10)).sum(),\n",
    "        'high_predictions': (submission['target_202002'] >= 10).sum(),\n",
    "        'extreme_predictions': (submission['target_202002'] >= 100).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"📊 Distribución de predicciones:\")\n",
    "    for key, value in pred_stats.items():\n",
    "        percentage = (value / len(submission)) * 100\n",
    "        print(f\"   {key}: {value} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Comparación con datos históricos\n",
    "    historical_mean = data_filtered['tn'].mean()\n",
    "    historical_median = data_filtered['tn'].median()\n",
    "    \n",
    "    print(f\"\\n📈 Comparación con históricos:\")\n",
    "    print(f\"   Media histórica: {historical_mean:.2f}\")\n",
    "    print(f\"   Media predicha: {submission['target_202002'].mean():.2f}\")\n",
    "    print(f\"   Ratio: {submission['target_202002'].mean() / historical_mean:.2f}\")\n",
    "    print(f\"   Mediana histórica: {historical_median:.2f}\")\n",
    "    print(f\"   Mediana predicha: {submission['target_202002'].median():.2f}\")\n",
    "    \n",
    "    # 15. Validación cruzada temporal (si hay tiempo)\n",
    "    if MODEL_CONFIG['use_cross_validation']:\n",
    "        print(\"\\n1️⃣5️⃣ VALIDACIÓN CRUZADA TEMPORAL\")\n",
    "        \n",
    "        cv_scores = []\n",
    "        n_splits = 3\n",
    "        \n",
    "        # Crear splits temporales para CV\n",
    "        periods = sorted(agg_data['periodo'].unique())\n",
    "        split_size = len(periods) // (n_splits + 1)\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            # Definir fechas de corte\n",
    "            train_end_idx = (i + 1) * split_size\n",
    "            val_start_idx = train_end_idx\n",
    "            val_end_idx = train_end_idx + split_size\n",
    "            \n",
    "            if val_end_idx > len(periods):\n",
    "                break\n",
    "            \n",
    "            train_end_date = periods[train_end_idx - 1]\n",
    "            val_start_date = periods[val_start_idx]\n",
    "            val_end_date = periods[val_end_idx - 1]\n",
    "            \n",
    "            # Crear splits\n",
    "            cv_train = agg_data[agg_data['periodo'] <= train_end_date]\n",
    "            cv_val = agg_data[(agg_data['periodo'] >= val_start_date) & \n",
    "                             (agg_data['periodo'] <= val_end_date)]\n",
    "            \n",
    "            if len(cv_train) == 0 or len(cv_val) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Preparar datos\n",
    "                X_cv_train = cv_train[feature_cols]\n",
    "                y_cv_train = cv_train['tn']\n",
    "                X_cv_val = cv_val[feature_cols]\n",
    "                y_cv_val = cv_val['tn']\n",
    "                \n",
    "                # Procesar\n",
    "                X_cv_train_processed = preprocessor.fit_transform(X_cv_train)\n",
    "                X_cv_val_processed = preprocessor.transform(X_cv_val)\n",
    "                \n",
    "                # Entrenar y evaluar\n",
    "                cv_model = clone(best_model)\n",
    "                cv_model.fit(X_cv_train_processed, y_cv_train)\n",
    "                cv_pred = cv_model.predict(X_cv_val_processed)\n",
    "                \n",
    "                cv_rmse = np.sqrt(mean_squared_error(y_cv_val, cv_pred))\n",
    "                cv_scores.append(cv_rmse)\n",
    "                \n",
    "                print(f\"   Fold {i+1}: RMSE = {cv_rmse:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error en fold {i+1}: {e}\")\n",
    "        \n",
    "        if cv_scores:\n",
    "            print(f\"\\n📊 CV Results:\")\n",
    "            print(f\"   Mean RMSE: {np.mean(cv_scores):.3f} ± {np.std(cv_scores):.3f}\")\n",
    "            print(f\"   Scores: {[f'{score:.3f}' for score in cv_scores]}\")\n",
    "    \n",
    "    # 16. Resumen final\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎯 RESUMEN FINAL DEL PIPELINE NOCTURNO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"⏰ Tiempo total de ejecución: {time.time() - start_time:.1f} segundos\")\n",
    "    print(f\"🏆 Mejor modelo: {best_model_name}\")\n",
    "    print(f\"📊 RMSE de validación: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "    print(f\"📊 MAE de validación: {results_df.loc[best_model_name, 'MAE']:.3f}\")\n",
    "    print(f\"📊 R² de validación: {results_df.loc[best_model_name, 'R2']:.3f}\")\n",
    "    print(f\"🎯 Predicciones generadas: {len(submission)}\")\n",
    "    print(f\"💾 Archivo de submission: {submission_filename}\")\n",
    "    print(f\"✅ Pipeline completado exitosamente!\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'preprocessor': preprocessor,\n",
    "        'results': results_df,\n",
    "        'submission': submission,\n",
    "        'predictions': predictions_df,\n",
    "        'model_name': best_model_name\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. EJECUCIÓN PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🌙 INICIANDO PIPELINE NOCTURNO AVANZADO\")\n",
    "    print(\"⏰ Hora de inicio:\", pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Ejecutar pipeline principal\n",
    "        results = run_advanced_nocturnal_pipeline()\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\n🎉 PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
    "            print(f\"🏆 Mejor modelo: {results['model_name']}\")\n",
    "            print(f\"📊 Submission generada con {len(results['submission'])} predicciones\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n❌ ERROR EN EL PIPELINE\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏹️ Pipeline interrumpido por el usuario\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n💥 ERROR CRÍTICO: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n⏰ Tiempo total: {total_time:.1f} segundos ({total_time/60:.1f} minutos)\")\n",
    "        print(\"🌙 Fin del pipeline nocturno\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
