{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Auto-ARIMA no disponible, se omitirÃ¡\n",
      "ğŸ“¦ LibrerÃ­as cargadas exitosamente!\n",
      "ğŸ–¥ï¸ ConfiguraciÃ³n del sistema:\n",
      "   CPUs disponibles: 8\n",
      "   Directorio de salida: kaggle_predictions_advanced\n",
      "   OptimizaciÃ³n de hiperparÃ¡metros: True\n",
      "   Ensemble stacking: True\n",
      "   Modelos de series de tiempo: True\n",
      "ğŸŒ™ INICIANDO PIPELINE NOCTURNO AVANZADO\n",
      "â° Hora de inicio: 2025-06-09 17:24:03\n",
      "ğŸš€ INICIANDO PIPELINE AVANZADO NOCTURNO\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ CARGA DE DATOS\n",
      "ğŸ”„ Cargando datasets...\n",
      "âœ… Sales: 2,945,818 filas, 7 columnas\n",
      "âœ… Stocks: 13,691 filas, 3 columnas\n",
      "âœ… Products: 1,251 productos\n",
      "âœ… Products to predict: 780 productos\n",
      "ğŸ“… Rango de fechas: 2017-01-01 00:00:00 a 2019-12-01 00:00:00\n",
      "ğŸ‡¦ğŸ‡· Cargando datos del IPC INDEC...\n",
      "âœ… IPC INDEC procesado: 48 perÃ­odos\n",
      "ğŸ“Š Rango IPC: 1.13 a 5.68\n",
      "\n",
      "2ï¸âƒ£ FEATURE ENGINEERING AVANZADO\n",
      "ğŸ”§ Creando features avanzadas v2...\n",
      "   â†’ DespuÃ©s de merge productos: 2945818 filas\n",
      "   â†’ DespuÃ©s de merge stocks: 2945818 filas\n",
      "   â†’ DespuÃ©s de merge IPC: 2945818 filas\n",
      "ğŸ“Š Aplicando feature engineering avanzado...\n",
      "ğŸ”„ Creando lags extendidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando lags: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:06<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Creando rolling features extendidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling windows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [1:46:51<00:00, 801.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Creando features de tendencia...\n",
      "ğŸ“Š Creando features estadÃ­sticas...\n",
      "ğŸ·ï¸ Creando agregaciones categÃ³ricas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agregaciones categÃ³ricas: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Creando features de interacciÃ³n...\n",
      "ğŸ“¦ Creando features de stock avanzadas...\n",
      "ğŸ’° Creando features de IPC avanzadas...\n",
      "ğŸ† Creando features de ranking...\n",
      "ğŸ”„ Creando features de frecuencia...\n",
      "ğŸ” Creando features de patrones...\n",
      "ğŸ¯ Creando target encoding...\n",
      "   Procesando cat1...\n",
      "   âœ… cat1_target_encoded creada\n",
      "   Procesando cat2...\n",
      "   âœ… cat2_target_encoded creada\n",
      "   Procesando cat3...\n",
      "   âœ… cat3_target_encoded creada\n",
      "   Procesando brand...\n",
      "   âœ… brand_target_encoded creada\n",
      "ğŸ§¹ Limpieza final...\n",
      "âœ… Features v2 creadas. Shape final: (2945818, 248)\n",
      "ğŸ“Š Features numÃ©ricas: 242\n",
      "ğŸ“Š Features categÃ³ricas: 5\n",
      "\n",
      "3ï¸âƒ£ AGREGACIÃ“N DE DATOS\n",
      "ğŸ“Š Agregando datos por producto-cliente...\n",
      "ğŸ”§ Reduciendo features para optimizar memoria...\n",
      "ğŸ” Verificando columnas de agrupaciÃ³n...\n",
      "   product_id: int64, shape: (2945818,)\n",
      "   customer_id: int64, shape: (2945818,)\n",
      "   periodo: datetime64[ns], shape: (2945818,)\n",
      "ğŸ§¹ Eliminando columnas duplicadas...\n",
      "ğŸ“Š Features reducidas a 54\n",
      "ğŸ“Š Columnas finales: ['product_id', 'customer_id', 'periodo', 'tn', 'plan_precios_cuidados', 'cust_request_qty', 'cust_request_tn', 'sku_size', 'stock_final', 'v_m_IPC', 'year', 'month', 'quarter', 'day_of_year', 'week_of_year', 'is_weekend', 'is_month_start', 'is_month_end', 'is_quarter_start', 'is_quarter_end', 'days_in_month', 'sin_month_3', 'cos_month_3', 'sin_month_4', 'cos_month_4', 'sin_month_6', 'cos_month_6', 'sin_month_12', 'cos_month_12', 'sin_quarter', 'cos_quarter', 'sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_4', 'sales_lag_5', 'sales_lag_6', 'sales_lag_9', 'sales_lag_12', 'sales_lag_15', 'sales_lag_18', 'sales_lag_24', 'sales_lag_36', 'product_sales_lag_1', 'product_sales_lag_3', 'product_sales_lag_6', 'product_sales_lag_12', 'sales_rolling_mean_2', 'sales_rolling_std_2', 'sales_rolling_min_2', 'sales_rolling_max_2', 'sales_rolling_median_2', 'sales_ewma_2_alpha_01', 'sales_ewma_2_alpha_03']\n",
      "ğŸ” Verificando tipos de datos finales...\n",
      "   product_id: int64\n",
      "   customer_id: int64\n",
      "   periodo: datetime64[ns]\n",
      "ğŸ”„ Agregando por chunks...\n",
      "   Chunk 1: 100000 registros\n",
      "   Chunk 2: 100000 registros\n",
      "   Chunk 3: 100000 registros\n",
      "   Chunk 4: 100000 registros\n",
      "   Chunk 5: 100000 registros\n",
      "   Chunk 6: 100000 registros\n",
      "   Chunk 7: 100000 registros\n",
      "   Chunk 8: 100000 registros\n",
      "   Chunk 9: 100000 registros\n",
      "   Chunk 10: 100000 registros\n",
      "   Chunk 11: 100000 registros\n",
      "   Chunk 12: 100000 registros\n",
      "   Chunk 13: 100000 registros\n",
      "   Chunk 14: 100000 registros\n",
      "   Chunk 15: 100000 registros\n",
      "   Chunk 16: 100000 registros\n",
      "   Chunk 17: 100000 registros\n",
      "   Chunk 18: 100000 registros\n",
      "   Chunk 19: 100000 registros\n",
      "   Chunk 20: 100000 registros\n",
      "   Chunk 21: 100000 registros\n",
      "   Chunk 22: 100000 registros\n",
      "   Chunk 23: 100000 registros\n",
      "   Chunk 24: 100000 registros\n",
      "   Chunk 25: 100000 registros\n",
      "   Chunk 26: 100000 registros\n",
      "   Chunk 27: 100000 registros\n",
      "   Chunk 28: 100000 registros\n",
      "   Chunk 29: 100000 registros\n",
      "   Chunk 30: 45818 registros\n",
      "ğŸ”— Combinando chunks...\n",
      "ğŸ”„ Consolidando datos finales...\n",
      "âœ… Datos agregados: (2945818, 54)\n",
      "\n",
      "4ï¸âƒ£ CREACIÃ“N DE SPLITS TEMPORALES\n",
      "ğŸ“Š Training: 2,814,453 registros (hasta 2019-10)\n",
      "ğŸ“Š Validation: 131,365 registros (desde 2019-10)\n",
      "ğŸ”§ Procesando features...\n",
      "âœ… Features procesadas: 49 dimensiones\n",
      "\n",
      "5ï¸âƒ£ EVALUACIÃ“N DE MODELOS DE ML\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  11%|â–ˆ         | 1/9 [1:18:08<10:25:11, 4688.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RandomForest - MAE: 0.01, RMSE: 0.48, R2: 0.980, MAPE: 0.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  22%|â–ˆâ–ˆâ–       | 2/9 [1:19:17<3:49:57, 1971.03s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… XGBoost - MAE: 0.06, RMSE: 1.44, R2: 0.818, MAPE: 13.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  33%|â–ˆâ–ˆâ–ˆâ–      | 3/9 [1:19:46<1:48:24, 1084.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LightGBM - MAE: 0.07, RMSE: 1.51, R2: 0.800, MAPE: 64.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [1:22:09<59:22, 712.55s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CatBoost - MAE: 0.08, RMSE: 1.58, R2: 0.781, MAPE: 74.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [4:35:13<5:08:52, 4633.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GradientBoosting - MAE: 0.01, RMSE: 0.36, R2: 0.988, MAPE: 2.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [5:26:43<3:25:25, 4108.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ExtraTrees - MAE: 0.01, RMSE: 0.48, R2: 0.980, MAPE: 6.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [5:26:46<1:32:12, 2766.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ridge - MAE: 0.03, RMSE: 0.28, R2: 0.993, MAPE: 160.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [5:30:32<32:37, 1957.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Lasso - MAE: 0.02, RMSE: 0.34, R2: 0.990, MAPE: 28.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando modelos ML: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [5:34:31<00:00, 2230.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ElasticNet - MAE: 0.03, RMSE: 0.31, R2: 0.991, MAPE: 43.0%\n",
      "\n",
      "6ï¸âƒ£ OPTIMIZACIÃ“N DE HIPERPARÃMETROS\n",
      "\n",
      "7ï¸âƒ£ MODELOS DE SERIES DE TIEMPO\n",
      "â° Generando predicciones con modelos de series de tiempo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time Series Predictions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 36.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predicciones de series de tiempo completadas para 50 productos\n",
      "âš ï¸ Errores en 50 productos\n",
      "\n",
      "8ï¸âƒ£ ENSEMBLE AVANZADO\n",
      "ğŸ¯ Entrenando ensemble avanzado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  11%|â–ˆ         | 1/9 [1:17:59<10:23:59, 4679.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… rf: MAE=0.008, RMSE=0.478, R2=0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  22%|â–ˆâ–ˆâ–       | 2/9 [1:19:18<3:50:14, 1973.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… xgb: MAE=0.061, RMSE=1.439, R2=0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  33%|â–ˆâ–ˆâ–ˆâ–      | 3/9 [1:19:57<1:48:59, 1089.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… lgb: MAE=0.067, RMSE=1.507, R2=0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [1:22:20<59:40, 716.15s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… catboost: MAE=0.082, RMSE=1.579, R2=0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [4:35:07<5:08:36, 4629.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… gb: MAE=0.006, RMSE=0.365, R2=0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [5:26:40<3:25:20, 4106.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… extra_trees: MAE=0.009, RMSE=0.480, R2=0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [5:26:43<1:32:10, 2765.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… ridge: MAE=0.031, RMSE=0.281, R2=0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [5:30:30<32:37, 1957.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… lasso: MAE=0.025, RMSE=0.336, R2=0.990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Level 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [5:34:29<00:00, 2229.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… elastic: MAE=0.025, RMSE=0.312, R2=0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ† ENSEMBLE FINAL: MAE=0.029, RMSE=0.695, R2=0.958\n",
      "\n",
      "9ï¸âƒ£ SELECCIÃ“N DEL MEJOR MODELO\n",
      "\n",
      "ğŸ“Š RESUMEN DE RESULTADOS\n",
      "============================================================\n",
      "                    MAE   RMSE     R2     MAPE    mae   rmse     r2\n",
      "Ridge             0.031  0.281  0.993  160.513    NaN    NaN    NaN\n",
      "ElasticNet        0.025  0.312  0.991   43.004    NaN    NaN    NaN\n",
      "Lasso             0.025  0.336  0.990   28.868    NaN    NaN    NaN\n",
      "GradientBoosting  0.006  0.365  0.988    2.861    NaN    NaN    NaN\n",
      "RandomForest      0.008  0.478  0.980    0.726    NaN    NaN    NaN\n",
      "ExtraTrees        0.009  0.480  0.980    6.852    NaN    NaN    NaN\n",
      "XGBoost           0.061  1.439  0.818   13.188    NaN    NaN    NaN\n",
      "LightGBM          0.067  1.507  0.800   64.729    NaN    NaN    NaN\n",
      "CatBoost          0.082  1.579  0.781   74.818    NaN    NaN    NaN\n",
      "AdvancedEnsemble    NaN    NaN    NaN      NaN  0.029  0.695  0.958\n",
      "\n",
      "ğŸ† MEJOR MODELO: Ridge\n",
      "   RMSE: 0.281\n",
      "   MAE: 0.031\n",
      "   R2: 0.993\n",
      "\n",
      "ğŸ”Ÿ REENTRENAMIENTO CON DATOS COMPLETOS\n",
      "ğŸ”„ Reentrenando el mejor modelo con todos los datos disponibles...\n",
      "\n",
      "1ï¸âƒ£1ï¸âƒ£ PREDICCIONES FINALES\n",
      "ğŸ¯ Generando predicciones para 2020-02-01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:   6%|â–‹         | 50/780 [11:05<2:41:03, 13.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20054:\n",
      "Clientes procesados: 489\n",
      "PredicciÃ³n total: 251.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  13%|â–ˆâ–        | 100/780 [21:20<2:18:17, 12.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20114:\n",
      "Clientes procesados: 413\n",
      "PredicciÃ³n total: 93.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  19%|â–ˆâ–‰        | 150/780 [31:05<2:06:06, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20180:\n",
      "Clientes procesados: 414\n",
      "PredicciÃ³n total: 55.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  26%|â–ˆâ–ˆâ–Œ       | 200/780 [40:21<1:41:09, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20244:\n",
      "Clientes procesados: 437\n",
      "PredicciÃ³n total: 37.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  32%|â–ˆâ–ˆâ–ˆâ–      | 250/780 [49:37<1:37:44, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20305:\n",
      "Clientes procesados: 392\n",
      "PredicciÃ³n total: 23.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 300/780 [59:27<1:38:48, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20365:\n",
      "Clientes procesados: 470\n",
      "PredicciÃ³n total: 15.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 350/780 [1:09:02<1:19:13, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20440:\n",
      "Clientes procesados: 262\n",
      "PredicciÃ³n total: 48.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 400/780 [1:17:49<59:04,  9.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20526:\n",
      "Clientes procesados: 269\n",
      "PredicciÃ³n total: 41.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 450/780 [1:26:37<1:05:20, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20601:\n",
      "Clientes procesados: 381\n",
      "PredicciÃ³n total: 7.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 500/780 [1:34:57<43:05,  9.23s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20673:\n",
      "Clientes procesados: 230\n",
      "PredicciÃ³n total: 12.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 550/780 [1:43:20<43:09, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20751:\n",
      "Clientes procesados: 342\n",
      "PredicciÃ³n total: 12.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 600/780 [1:51:28<24:09,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20845:\n",
      "Clientes procesados: 51\n",
      "PredicciÃ³n total: 26.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 650/780 [1:58:20<18:08,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 20947:\n",
      "Clientes procesados: 255\n",
      "PredicciÃ³n total: 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 700/780 [2:05:19<10:22,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 21056:\n",
      "Clientes procesados: 209\n",
      "PredicciÃ³n total: 0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 750/780 [2:10:55<04:01,  8.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Producto 21190:\n",
      "Clientes procesados: 277\n",
      "PredicciÃ³n total: 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generando predicciones: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780/780 [2:12:59<00:00, 10.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predicciones completadas:\n",
      "   Total: 780\n",
      "   Con modelo: 780\n",
      "   Errores: 0\n",
      "\n",
      "1ï¸âƒ£2ï¸âƒ£ GUARDANDO RESULTADOS\n",
      "\n",
      "ğŸ“ˆ ESTADÃSTICAS DE PREDICCIONES:\n",
      "   Media: 62.99\n",
      "   Mediana: 14.95\n",
      "   Std: 154.07\n",
      "   Min: 0.01\n",
      "   Max: 1825.30\n",
      "   Predicciones = 0: 0\n",
      "\n",
      "ğŸ“Š GENERANDO SUBMISSIONS MÃšLTIPLES\n",
      "ğŸ’¾ RandomForest guardado: submission_randomforest_20250610_0859.csv\n",
      "ğŸ’¾ XGBoost guardado: submission_xgboost_20250610_0900.csv\n",
      "ğŸ’¾ LightGBM guardado: submission_lightgbm_20250610_0900.csv\n",
      "ğŸ’¾ CatBoost guardado: submission_catboost_20250610_0900.csv\n",
      "ğŸ’¾ GradientBoosting guardado: submission_gradientboosting_20250610_0900.csv\n",
      "ğŸ’¾ ExtraTrees guardado: submission_extratrees_20250610_0901.csv\n",
      "ğŸ’¾ Lasso guardado: submission_lasso_20250610_0901.csv\n",
      "ğŸ’¾ ElasticNet guardado: submission_elasticnet_20250610_0901.csv\n",
      "ğŸ’¾ AdvancedEnsemble guardado: submission_advancedensemble_20250610_0903.csv\n",
      "ğŸ’¾ Submission guardada: submission_nocturnal_ridge_20250610_0903.csv\n",
      "ğŸ“Š Resultados detallados guardados: results_nocturnal_20250610_0903.csv\n",
      "ğŸ¤– Modelo guardado: best_model_nocturnal_ridge_20250610_0903.pkl\n",
      "ğŸ”§ Preprocessor guardado: preprocessor_nocturnal_20250610_0903.pkl\n",
      "\n",
      "1ï¸âƒ£4ï¸âƒ£ MÃ‰TRICAS DE CALIDAD\n",
      "ğŸ“Š DistribuciÃ³n de predicciones:\n",
      "   zero_predictions: 0 (0.0%)\n",
      "   low_predictions: 95 (12.2%)\n",
      "   medium_predictions: 227 (29.1%)\n",
      "   high_predictions: 458 (58.7%)\n",
      "   extreme_predictions: 126 (16.2%)\n",
      "\n",
      "ğŸ“ˆ ComparaciÃ³n con histÃ³ricos:\n",
      "   Media histÃ³rica: 0.45\n",
      "   Media predicha: 62.99\n",
      "   Ratio: 140.05\n",
      "   Mediana histÃ³rica: 0.04\n",
      "   Mediana predicha: 14.95\n",
      "\n",
      "1ï¸âƒ£5ï¸âƒ£ VALIDACIÃ“N CRUZADA TEMPORAL\n",
      "   Error en fold 1: name 'clone' is not defined\n",
      "   Error en fold 2: name 'clone' is not defined\n",
      "   Error en fold 3: name 'clone' is not defined\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ RESUMEN FINAL DEL PIPELINE NOCTURNO\n",
      "================================================================================\n",
      "â° Tiempo total de ejecuciÃ³n: 56468.2 segundos\n",
      "ğŸ† Mejor modelo: Ridge\n",
      "ğŸ“Š RMSE de validaciÃ³n: 0.281\n",
      "ğŸ“Š MAE de validaciÃ³n: 0.031\n",
      "ğŸ“Š RÂ² de validaciÃ³n: 0.993\n",
      "ğŸ¯ Predicciones generadas: 780\n",
      "ğŸ’¾ Archivo de submission: submission_nocturnal_ridge_20250610_0903.csv\n",
      "âœ… Pipeline completado exitosamente!\n",
      "\n",
      "ğŸ‰ PIPELINE COMPLETADO EXITOSAMENTE!\n",
      "ğŸ† Mejor modelo: Ridge\n",
      "ğŸ“Š Submission generada con 780 predicciones\n",
      "\n",
      "â° Tiempo total: 56469.7 segundos (941.2 minutos)\n",
      "ğŸŒ™ Fin del pipeline nocturno\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PREDICCIÃ“N AVANZADA NOCTURNA - MÃšLTIPLES MODELOS Y SERIES DE TIEMPO\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Proyecto: PredicciÃ³n de Ventas - Competencia Kaggle (VersiÃ³n Nocturna Avanzada)\n",
    "Objetivo: Predecir ventas para febrero 2020 (mes +2)\n",
    "Modelos: RF, XGB, LGB, CatBoost, GradientBoosting, ExtraTrees, SVR,\n",
    "         ARIMA, SARIMA, Prophet, AutoArima, Ensemble Avanzado\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORTS Y CONFIGURACIÃ“N EXTENDIDA\n",
    "# ============================================================================\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning BÃ¡sico\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                             ExtraTreesRegressor, VotingRegressor)\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import (train_test_split, TimeSeriesSplit, \n",
    "                                   cross_val_score, GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Modelos avanzados\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Series de tiempo\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Prophet para series de tiempo\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Prophet no disponible, se omitirÃ¡\")\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# AutoML para series de tiempo\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    AUTO_ARIMA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Auto-ARIMA no disponible, se omitirÃ¡\")\n",
    "    AUTO_ARIMA_AVAILABLE = False\n",
    "\n",
    "# Utilidades\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import itertools\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "print(\"ğŸ“¦ LibrerÃ­as cargadas exitosamente!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. CONFIGURACIÃ“N GLOBAL EXTENDIDA\n",
    "# ============================================================================\n",
    "\n",
    "# ConfiguraciÃ³n\n",
    "RANDOM_STATE = 42\n",
    "TARGET_DATE = '2020-02-01'\n",
    "VALIDATION_MONTHS = 2\n",
    "OUTPUT_DIR = 'kaggle_predictions_advanced'\n",
    "N_JOBS = -1  # Usar todos los cores disponibles\n",
    "\n",
    "# Crear directorio de salida\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ConfiguraciÃ³n de modelos\n",
    "MODEL_CONFIG = {\n",
    "    'generate_multiple_submissions': True,\n",
    "    'save_model': True,\n",
    "    'use_hyperparameter_tuning': True,\n",
    "    'use_ensemble_stacking': True,\n",
    "    'use_time_series_models': True,\n",
    "    'use_prophet': PROPHET_AVAILABLE,\n",
    "    'use_auto_arima': AUTO_ARIMA_AVAILABLE,\n",
    "    'cross_validation_folds': 3,\n",
    "    'optuna_trials': 100,\n",
    "    'use_cross_validation': True\n",
    "}\n",
    "\n",
    "# ConfiguraciÃ³n visual\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"ğŸ–¥ï¸ ConfiguraciÃ³n del sistema:\")\n",
    "print(f\"   CPUs disponibles: {cpu_count()}\")\n",
    "print(f\"   Directorio de salida: {OUTPUT_DIR}\")\n",
    "print(f\"   OptimizaciÃ³n de hiperparÃ¡metros: {MODEL_CONFIG['use_hyperparameter_tuning']}\")\n",
    "print(f\"   Ensemble stacking: {MODEL_CONFIG['use_ensemble_stacking']}\")\n",
    "print(f\"   Modelos de series de tiempo: {MODEL_CONFIG['use_time_series_models']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. FUNCIONES DE CARGA Y PREPARACIÃ“N (MEJORADAS)\n",
    "# ============================================================================\n",
    "def validate_predictions(predictions_df, historical_data):\n",
    "    \"\"\"Valida que las predicciones estÃ©n en rangos razonables\"\"\"\n",
    "    \n",
    "    # Calcular estadÃ­sticas histÃ³ricas por producto\n",
    "    historical_stats = historical_data.groupby('product_id')['tn'].agg(['mean', 'std', 'max', 'min']).reset_index()\n",
    "    \n",
    "    # Merge con predicciones\n",
    "    validation_df = predictions_df.merge(historical_stats, on='product_id', how='left')\n",
    "    \n",
    "    # Ajustar predicciones fuera de rango\n",
    "    for idx, row in validation_df.iterrows():\n",
    "        if row['prediction'] < row['mean'] * 0.01:  # Muy por debajo del promedio\n",
    "            validation_df.loc[idx, 'prediction'] = row['mean'] * 0.1\n",
    "        elif row['prediction'] > row['max'] * 2:  # Muy por encima del mÃ¡ximo histÃ³rico\n",
    "            validation_df.loc[idx, 'prediction'] = row['max'] * 1.2\n",
    "    \n",
    "    return validation_df[['product_id', 'prediction']]\n",
    "\n",
    "def inverse_transform_predictions(predictions, scaler):\n",
    "    \"\"\"Des-escala las predicciones\"\"\"\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    return scaler.inverse_transform(predictions).ravel()\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Carga y prepara todos los datasets con validaciones mejoradas\"\"\"\n",
    "    print(\"ğŸ”„ Cargando datasets...\")\n",
    "    \n",
    "    try:\n",
    "        # Cargar datasets con validaciones\n",
    "        sales = pd.read_csv(\"../datasets/sell-in.txt\", sep=\"\\t\", dtype={\"periodo\": str})\n",
    "        stocks = pd.read_csv(\"../datasets/tb_stocks.txt\", sep=\"\\t\", dtype={\"periodo\": str}) \n",
    "        product_info = pd.read_csv(\"../datasets/tb_productos.txt\", sep=\"\\t\")\n",
    "        products_to_predict = pd.read_csv('../datasets/product_id_apredecir201912.txt')\n",
    "        \n",
    "        # Validaciones bÃ¡sicas\n",
    "        assert not sales.empty, \"Sales dataset estÃ¡ vacÃ­o\"\n",
    "        assert not products_to_predict.empty, \"Products to predict estÃ¡ vacÃ­o\"\n",
    "        \n",
    "        # Convertir periodos con validaciÃ³n\n",
    "        sales['periodo'] = pd.to_datetime(sales['periodo'], format='%Y%m', errors='coerce')\n",
    "        stocks['periodo'] = pd.to_datetime(stocks['periodo'], format='%Y%m', errors='coerce')\n",
    "        \n",
    "        # Eliminar fechas invÃ¡lidas\n",
    "        sales = sales.dropna(subset=['periodo'])\n",
    "        stocks = stocks.dropna(subset=['periodo'])\n",
    "        \n",
    "        print(f\"âœ… Sales: {sales.shape[0]:,} filas, {sales.shape[1]} columnas\")\n",
    "        print(f\"âœ… Stocks: {stocks.shape[0]:,} filas, {stocks.shape[1]} columnas\") \n",
    "        print(f\"âœ… Products: {product_info.shape[0]:,} productos\")\n",
    "        print(f\"âœ… Products to predict: {len(products_to_predict):,} productos\")\n",
    "        print(f\"ğŸ“… Rango de fechas: {sales['periodo'].min()} a {sales['periodo'].max()}\")\n",
    "        \n",
    "        return sales, stocks, product_info, products_to_predict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cargando datos: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def load_indec_data():\n",
    "    \"\"\"Carga y procesa datos del INDEC con limpieza mejorada\"\"\"\n",
    "    print(\"ğŸ‡¦ğŸ‡· Cargando datos del IPC INDEC...\")\n",
    "    \n",
    "    try:\n",
    "        INDEC = pd.read_csv('../datasets/serie_ipc_aperturas.csv', sep=';', encoding='latin-1')\n",
    "        INDEC['periodo'] = INDEC['periodo'].astype(str)\n",
    "        \n",
    "        INDEC_filtered = INDEC[\n",
    "            (INDEC['periodo'] >= '201701') & \n",
    "            (INDEC['periodo'] <= '202012') & \n",
    "            (INDEC['Descripcion_aperturas'] == 'Nivel general')\n",
    "        ].copy()\n",
    "        \n",
    "        def clean_and_convert(value):\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    # Limpiar mÃºltiples formatos posibles\n",
    "                    cleaned = value.replace(',', '.').replace(' ', '')\n",
    "                    # Extraer nÃºmeros\n",
    "                    import re\n",
    "                    numbers = re.findall(r'-?\\d+\\.?\\d*', cleaned)\n",
    "                    if numbers:\n",
    "                        return float(numbers[0])\n",
    "                    return np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "            return float(value) if pd.notna(value) else np.nan\n",
    "        \n",
    "        INDEC_filtered['v_m_IPC'] = INDEC_filtered['v_m_IPC'].apply(clean_and_convert)\n",
    "        INDEC_filtered = INDEC_filtered.dropna(subset=['v_m_IPC'])\n",
    "        INDEC_processed = INDEC_filtered.groupby('periodo')['v_m_IPC'].mean().reset_index()\n",
    "        INDEC_processed['periodo'] = pd.to_datetime(INDEC_processed['periodo'], format='%Y%m')\n",
    "        \n",
    "        # Suavizado de valores extremos\n",
    "        q1, q3 = INDEC_processed['v_m_IPC'].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        # Cap outliers\n",
    "        INDEC_processed['v_m_IPC'] = np.clip(INDEC_processed['v_m_IPC'], lower_bound, upper_bound)\n",
    "        \n",
    "        print(f\"âœ… IPC INDEC procesado: {len(INDEC_processed)} perÃ­odos\")\n",
    "        print(f\"ğŸ“Š Rango IPC: {INDEC_processed['v_m_IPC'].min():.2f} a {INDEC_processed['v_m_IPC'].max():.2f}\")\n",
    "        \n",
    "        return INDEC_processed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error procesando INDEC: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_advanced_features_v2(sales, stocks, product_info, indec_data):\n",
    "    \"\"\"VersiÃ³n mejorada de feature engineering con mÃ¡s features\"\"\"\n",
    "    print(\"ğŸ”§ Creando features avanzadas v2...\")\n",
    "    \n",
    "    # Merge inicial con validaciones\n",
    "    data = sales.copy()\n",
    "    initial_shape = data.shape[0]\n",
    "    \n",
    "    # Agregar informaciÃ³n de productos\n",
    "    if product_info is not None:\n",
    "        data = data.merge(product_info, on='product_id', how='left')\n",
    "        print(f\"   â†’ DespuÃ©s de merge productos: {data.shape[0]} filas\")\n",
    "    \n",
    "    # Agregar stocks\n",
    "    if stocks is not None:\n",
    "        data = data.merge(stocks, on=['periodo', 'product_id'], how='left')\n",
    "        print(f\"   â†’ DespuÃ©s de merge stocks: {data.shape[0]} filas\")\n",
    "    \n",
    "    # Agregar IPC\n",
    "    if indec_data is not None:\n",
    "        data = data.merge(indec_data, on='periodo', how='left')\n",
    "        print(f\"   â†’ DespuÃ©s de merge IPC: {data.shape[0]} filas\")\n",
    "    \n",
    "    # FEATURE ENGINEERING COMPLETO Y MEJORADO\n",
    "    print(\"ğŸ“Š Aplicando feature engineering avanzado...\")\n",
    "    \n",
    "    # 1. Features temporales extendidas\n",
    "    data['year'] = data['periodo'].dt.year\n",
    "    data['month'] = data['periodo'].dt.month\n",
    "    data['quarter'] = data['periodo'].dt.quarter\n",
    "    data['day_of_year'] = data['periodo'].dt.dayofyear\n",
    "    data['week_of_year'] = data['periodo'].dt.isocalendar().week\n",
    "    data['is_weekend'] = data['periodo'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    data['is_month_start'] = data['periodo'].dt.is_month_start.astype(int)\n",
    "    data['is_month_end'] = data['periodo'].dt.is_month_end.astype(int)\n",
    "    data['is_quarter_start'] = data['periodo'].dt.is_quarter_start.astype(int)\n",
    "    data['is_quarter_end'] = data['periodo'].dt.is_quarter_end.astype(int)\n",
    "    data['days_in_month'] = data['periodo'].dt.days_in_month\n",
    "    \n",
    "    # 2. Features de estacionalidad mÃºltiples\n",
    "    for period in [3, 4, 6, 12]:\n",
    "        data[f'sin_month_{period}'] = np.sin(2 * np.pi * data['month'] / period)\n",
    "        data[f'cos_month_{period}'] = np.cos(2 * np.pi * data['month'] / period)\n",
    "    \n",
    "    # Features cÃ­clicas para quarter\n",
    "    data['sin_quarter'] = np.sin(2 * np.pi * data['quarter'] / 4)\n",
    "    data['cos_quarter'] = np.cos(2 * np.pi * data['quarter'] / 4)\n",
    "    \n",
    "    # 3. Lags extendidos con mÃºltiples targets\n",
    "    print(\"ğŸ”„ Creando lags extendidos...\")\n",
    "    lag_periods = [1, 2, 3, 4, 5, 6, 9, 12, 15, 18, 24, 36]\n",
    "    \n",
    "    for lag in tqdm(lag_periods, desc=\"Creando lags\"):\n",
    "        data[f'sales_lag_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].shift(lag)\n",
    "        \n",
    "    # Lags por producto solamente (agregados)\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'product_sales_lag_{lag}'] = data.groupby('product_id')['tn'].shift(lag)\n",
    "\n",
    "    # 4. Rolling windows extendidos\n",
    "    print(\"ğŸ”„ Creando rolling features extendidos...\")\n",
    "    windows = [2, 3, 4, 6, 9, 12, 18, 24]\n",
    "    operations = ['mean', 'std', 'min', 'max', 'median', 'skew']\n",
    "    \n",
    "    for window in tqdm(windows, desc=\"Rolling windows\"):\n",
    "        # Por producto-cliente\n",
    "        rolling_group = data.groupby(['product_id', 'customer_id'])['tn']\n",
    "        data[f'sales_rolling_mean_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "        data[f'sales_rolling_std_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "        data[f'sales_rolling_min_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "        data[f'sales_rolling_max_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "        data[f'sales_rolling_median_{window}'] = rolling_group.transform(lambda x: x.rolling(window, min_periods=1).median())\n",
    "        \n",
    "        # EWMA con diferentes alphas\n",
    "        for alpha in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "            data[f'sales_ewma_{window}_alpha_{str(alpha).replace(\".\", \"\")}'] = rolling_group.transform(\n",
    "                lambda x: x.ewm(alpha=alpha, min_periods=1).mean()\n",
    "            )\n",
    "\n",
    "    # 5. Features de tendencia y momentum\n",
    "    print(\"ğŸ“ˆ Creando features de tendencia...\")\n",
    "    \n",
    "    # Diferencias y cambios porcentuales\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'sales_diff_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].diff(periods=lag)\n",
    "        data[f'sales_pct_change_{lag}'] = data.groupby(['product_id', 'customer_id'])['tn'].pct_change(periods=lag)\n",
    "    \n",
    "    # Momentum indicators\n",
    "    for short, long in [(3, 6), (6, 12), (12, 24)]:\n",
    "        data[f'momentum_{short}_{long}'] = (\n",
    "            data[f'sales_rolling_mean_{short}'] - data[f'sales_rolling_mean_{long}']\n",
    "        )\n",
    "        \n",
    "    # Acceleration (second derivative)\n",
    "    data['sales_acceleration'] = data.groupby(['product_id', 'customer_id'])['tn'].diff().diff()\n",
    "    \n",
    "    # 6. Features estadÃ­sticas avanzadas\n",
    "    print(\"ğŸ“Š Creando features estadÃ­sticas...\")\n",
    "    \n",
    "    # Ratios y volatilidad\n",
    "    for window in [3, 6, 12]:\n",
    "        data[f'cv_{window}'] = data[f'sales_rolling_std_{window}'] / (data[f'sales_rolling_mean_{window}'] + 1e-8)\n",
    "        data[f'zscore_{window}'] = (data['tn'] - data[f'sales_rolling_mean_{window}']) / (data[f'sales_rolling_std_{window}'] + 1e-8)\n",
    "        data[f'range_ratio_{window}'] = (data[f'sales_rolling_max_{window}'] - data[f'sales_rolling_min_{window}']) / (data[f'sales_rolling_mean_{window}'] + 1e-8)\n",
    "    \n",
    "    # 7. Agregaciones categÃ³ricas extendidas\n",
    "    print(\"ğŸ·ï¸ Creando agregaciones categÃ³ricas...\")\n",
    "    \n",
    "    categorical_cols = ['product_id', 'customer_id']\n",
    "    if 'brand' in data.columns:\n",
    "        categorical_cols.extend(['brand', 'cat1', 'cat2', 'cat3'])\n",
    "    \n",
    "    aggregations = ['mean', 'std', 'median', 'min', 'max', 'count', 'sum']\n",
    "    \n",
    "    for cat in tqdm(categorical_cols, desc=\"Agregaciones categÃ³ricas\"):\n",
    "        if cat in data.columns:\n",
    "            grouped = data.groupby(cat)['tn']\n",
    "            for agg in aggregations:\n",
    "                try:\n",
    "                    data[f'{cat}_{agg}'] = grouped.transform(agg)\n",
    "                except:\n",
    "                    continue\n",
    "    \n",
    "    # 8. Features de interacciÃ³n avanzadas\n",
    "    print(\"ğŸ”— Creando features de interacciÃ³n...\")\n",
    "    \n",
    "    # Interacciones temporales\n",
    "    data['sales_month_interaction'] = data['tn'] * data['month']\n",
    "    data['sales_quarter_interaction'] = data['tn'] * data['quarter']\n",
    "    data['sales_year_interaction'] = data['tn'] * (data['year'] - data['year'].min())\n",
    "    \n",
    "    # Interacciones con lags\n",
    "    for lag in [1, 3, 6, 12]:\n",
    "        data[f'sales_lag_ratio_{lag}'] = data['tn'] / (data[f'sales_lag_{lag}'] + 1e-8)\n",
    "        data[f'sales_lag_diff_{lag}'] = data['tn'] - data[f'sales_lag_{lag}']\n",
    "    \n",
    "    # 9. Features de stock mejoradas\n",
    "    if 'stock_final' in data.columns:\n",
    "        print(\"ğŸ“¦ Creando features de stock avanzadas...\")\n",
    "        \n",
    "        data['stock_turnover'] = data['tn'] / (data['stock_final'] + 1e-8)\n",
    "        data['days_of_stock'] = data['stock_final'] / (data['tn'] + 1e-8) * 30\n",
    "        data['stock_ratio'] = data['stock_final'] / (data['stock_final'].mean() + 1e-8)\n",
    "        \n",
    "        # Lags de stock\n",
    "        for lag in [1, 3, 6]:\n",
    "            data[f'stock_lag_{lag}'] = data.groupby(['product_id', 'customer_id'])['stock_final'].shift(lag)\n",
    "            data[f'stock_change_{lag}'] = data['stock_final'] - data[f'stock_lag_{lag}']\n",
    "    \n",
    "    # 10. Features de IPC avanzadas\n",
    "    if 'v_m_IPC' in data.columns:\n",
    "        print(\"ğŸ’° Creando features de IPC avanzadas...\")\n",
    "        \n",
    "        # Lags de IPC\n",
    "        for lag in [1, 2, 3, 6, 12]:\n",
    "            data[f'ipc_lag_{lag}'] = data['v_m_IPC'].shift(lag)\n",
    "        \n",
    "        # Rolling IPC\n",
    "        for window in [3, 6, 12]:\n",
    "            data[f'ipc_rolling_{window}'] = data['v_m_IPC'].rolling(window, min_periods=1).mean()\n",
    "            data[f'ipc_std_{window}'] = data['v_m_IPC'].rolling(window, min_periods=1).std()\n",
    "        \n",
    "        # Interacciones IPC-ventas\n",
    "        data['sales_ipc_interaction'] = data['tn'] * data['v_m_IPC']\n",
    "        data['sales_ipc_ratio'] = data['tn'] / (data['v_m_IPC'] + 1e-8)\n",
    "        \n",
    "        # Cambios en IPC\n",
    "        data['ipc_change'] = data['v_m_IPC'].diff()\n",
    "        data['ipc_acceleration'] = data['ipc_change'].diff()\n",
    "    \n",
    "    # 11. Features de ranking y percentiles\n",
    "    print(\"ğŸ† Creando features de ranking...\")\n",
    "    \n",
    "    # Rankings por perÃ­odo\n",
    "    data['product_rank_period'] = data.groupby('periodo')['tn'].rank(pct=True)\n",
    "    data['customer_rank_period'] = data.groupby(['periodo', 'customer_id'])['tn'].rank(pct=True)\n",
    "    \n",
    "    # Rankings histÃ³ricos\n",
    "    data['product_rank_historical'] = data.groupby('product_id')['tn'].rank(pct=True)\n",
    "    data['customer_rank_historical'] = data.groupby('customer_id')['tn'].rank(pct=True)\n",
    "    \n",
    "    # 12. Features de frecuencia y consistencia\n",
    "    print(\"ğŸ”„ Creando features de frecuencia...\")\n",
    "    \n",
    "    # Conteo de perÃ­odos activos\n",
    "    data['periods_active'] = data.groupby(['product_id', 'customer_id']).cumcount() + 1\n",
    "    data['total_periods'] = data.groupby(['product_id', 'customer_id'])['periodo'].transform('count')\n",
    "    data['activity_ratio'] = data['periods_active'] / data['total_periods']\n",
    "    \n",
    "    # Consistencia de ventas\n",
    "    for window in [6, 12]:\n",
    "        rolling_group = data.groupby(['product_id', 'customer_id'])['tn']\n",
    "        data[f'consistency_{window}'] = rolling_group.transform(\n",
    "            lambda x: (x.rolling(window, min_periods=1).apply(lambda y: (y > 0).mean()))\n",
    "        )\n",
    "    \n",
    "    # 13. Features avanzadas de detecciÃ³n de patrones\n",
    "    print(\"ğŸ” Creando features de patrones...\")\n",
    "    \n",
    "    # Detectar picos y valles\n",
    "    for window in [3, 6]:\n",
    "        rolling_mean = data[f'sales_rolling_mean_{window}']\n",
    "        rolling_std = data[f'sales_rolling_std_{window}']\n",
    "        data[f'is_peak_{window}'] = (data['tn'] > rolling_mean + 2 * rolling_std).astype(int)\n",
    "        data[f'is_valley_{window}'] = (data['tn'] < rolling_mean - rolling_std).astype(int)\n",
    "    \n",
    "    # Detectar estacionalidad\n",
    "    data['seasonal_strength'] = np.abs(data['sin_month_12'])\n",
    "    \n",
    "    # 14. Features de target encoding\n",
    "    print(\"ğŸ¯ Creando target encoding...\")\n",
    "\n",
    "    # Target encoding con validaciÃ³n cruzada para evitar overfitting\n",
    "    for cat_col in ['cat1', 'cat2', 'cat3', 'brand']:\n",
    "        if cat_col in data.columns:\n",
    "            print(f\"   Procesando {cat_col}...\")\n",
    "            try:\n",
    "                # Media global como fallback\n",
    "                global_mean = data['tn'].mean()\n",
    "                \n",
    "                # Target encoding suavizado\n",
    "                cat_stats = data.groupby(cat_col)['tn'].agg(['mean', 'count']).reset_index()\n",
    "                cat_stats['smoothed_mean'] = (cat_stats['mean'] * cat_stats['count'] + global_mean * 10) / (cat_stats['count'] + 10)\n",
    "                \n",
    "                # Crear mapeo\n",
    "                mapping = dict(zip(cat_stats[cat_col], cat_stats['smoothed_mean']))\n",
    "                \n",
    "                # Aplicar mapeo directamente sin merge\n",
    "                data[f'{cat_col}_target_encoded'] = data[cat_col].map(mapping).fillna(global_mean)\n",
    "                \n",
    "                print(f\"   âœ… {cat_col}_target_encoded creada\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error en {cat_col}: {e}\")\n",
    "    \n",
    "    # 15. Limpieza final y validaciones\n",
    "    print(\"ğŸ§¹ Limpieza final...\")\n",
    "    \n",
    "    # Reemplazar infinitos y valores extremos\n",
    "    data = data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # ImputaciÃ³n inteligente\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if col != 'tn':  # No imputar el target\n",
    "            if data[col].isna().sum() > 0:\n",
    "                # Usar mediana para imputaciÃ³n robusta\n",
    "                data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "    # Llenar categÃ³ricas\n",
    "    categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        data[col] = data[col].fillna('unknown')\n",
    "    \n",
    "    print(f\"âœ… Features v2 creadas. Shape final: {data.shape}\")\n",
    "    print(f\"ğŸ“Š Features numÃ©ricas: {len(data.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"ğŸ“Š Features categÃ³ricas: {len(data.select_dtypes(include=['object']).columns)}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ============================================================================\n",
    "# 4. MODELOS DE SERIES DE TIEMPO\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesModels:\n",
    "    \"\"\"Clase para manejar modelos especÃ­ficos de series de tiempo\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.fitted_models = {}\n",
    "    \n",
    "    def prepare_time_series_data(self, data, product_id):\n",
    "        \"\"\"Prepara datos para series de tiempo de un producto especÃ­fico\"\"\"\n",
    "        product_data = data[data['product_id'] == product_id].copy()\n",
    "        product_data = product_data.sort_values('periodo')\n",
    "        \n",
    "        # Crear series temporal completa (rellenar perÃ­odos faltantes)\n",
    "        date_range = pd.date_range(\n",
    "            start=product_data['periodo'].min(),\n",
    "            end=product_data['periodo'].max(),\n",
    "            freq='M'\n",
    "        )\n",
    "        \n",
    "        # Reindexar y rellenar\n",
    "        product_data = product_data.set_index('periodo')\n",
    "        product_data = product_data.reindex(date_range, fill_value=0)\n",
    "        product_data.index.name = 'periodo'\n",
    "        product_data = product_data.reset_index()\n",
    "        \n",
    "        return product_data\n",
    "    \n",
    "    def fit_arima(self, ts_data, order=(1,1,1)):\n",
    "        \"\"\"Ajusta modelo ARIMA\"\"\"\n",
    "        try:\n",
    "            if len(ts_data) < 10:\n",
    "                return None, None\n",
    "                \n",
    "            # Eliminar ceros iniciales si existen\n",
    "            first_nonzero = np.argmax(ts_data > 0)\n",
    "            if first_nonzero > 0:\n",
    "                ts_data = ts_data[first_nonzero:]\n",
    "            \n",
    "            if len(ts_data) < 5:\n",
    "                return None, None\n",
    "            \n",
    "            model = ARIMA(ts_data, order=order)\n",
    "            fitted_model = model.fit()\n",
    "            return fitted_model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_sarima(self, ts_data, order=(1,1,1), seasonal_order=(0,1,1,12)):\n",
    "        \"\"\"Ajusta modelo SARIMA\"\"\"\n",
    "        try:\n",
    "            if len(ts_data) < 24:  # Necesita al menos 2 aÃ±os para estacionalidad\n",
    "                return None, \"Insufficient data for SARIMA\"\n",
    "                \n",
    "            model = SARIMAX(ts_data, order=order, seasonal_order=seasonal_order)\n",
    "            fitted_model = model.fit(disp=False)\n",
    "            return fitted_model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_prophet(self, data, product_id):\n",
    "        \"\"\"Ajusta modelo Prophet\"\"\"\n",
    "        if not PROPHET_AVAILABLE:\n",
    "            return None, \"Prophet not available\"\n",
    "            \n",
    "        try:\n",
    "            product_data = self.prepare_time_series_data(data, product_id)\n",
    "            \n",
    "            if len(product_data) < 10:\n",
    "                return None, \"Insufficient data\"\n",
    "            \n",
    "            # Preparar datos para Prophet\n",
    "            prophet_data = pd.DataFrame({\n",
    "                'ds': product_data['periodo'],\n",
    "                'y': product_data['tn']\n",
    "            })\n",
    "            \n",
    "            # Configurar Prophet\n",
    "            model = Prophet(\n",
    "                yearly_seasonality=True,\n",
    "                weekly_seasonality=False,\n",
    "                daily_seasonality=False,\n",
    "                seasonality_mode='multiplicative',\n",
    "                changepoint_prior_scale=0.05\n",
    "            )\n",
    "            \n",
    "            # Agregar regresores externos si estÃ¡n disponibles\n",
    "            if 'v_m_IPC' in product_data.columns:\n",
    "                model.add_regressor('ipc')\n",
    "                prophet_data['ipc'] = product_data['v_m_IPC'].fillna(method='ffill')\n",
    "            \n",
    "            model.fit(prophet_data)\n",
    "            return model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def fit_auto_arima(self, ts_data):\n",
    "        \"\"\"Ajusta Auto-ARIMA\"\"\"\n",
    "        if not AUTO_ARIMA_AVAILABLE:\n",
    "            return None, \"Auto-ARIMA not available\"\n",
    "            \n",
    "        try:\n",
    "            if len(ts_data) < 10:\n",
    "                return None, \"Insufficient data\"\n",
    "            \n",
    "            model = auto_arima(\n",
    "                ts_data,\n",
    "                start_p=0, start_q=0,\n",
    "                max_p=3, max_q=3,\n",
    "                seasonal=True,\n",
    "                start_P=0, start_Q=0,\n",
    "                max_P=2, max_Q=2,\n",
    "                m=12,\n",
    "                stepwise=True,\n",
    "                suppress_warnings=True,\n",
    "                error_action='ignore'\n",
    "            )\n",
    "            \n",
    "            return model, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, str(e)\n",
    "    \n",
    "    def predict_time_series(self, data, products_list, target_date='2020-02-01'):\n",
    "        \"\"\"Genera predicciones usando modelos de series de tiempo\"\"\"\n",
    "        print(\"â° Generando predicciones con modelos de series de tiempo...\")\n",
    "        \n",
    "        predictions = {}\n",
    "        errors = {}\n",
    "        \n",
    "        for product_id in tqdm(products_list, desc=\"Time Series Predictions\"):\n",
    "            try:\n",
    "                product_data = self.prepare_time_series_data(data, product_id)\n",
    "                product_sales = data[data['product_id'] == product_id]['tn'].values\n",
    "                \n",
    "                if len(product_sales) == 0:\n",
    "                    predictions[product_id] = {'arima': 0, 'sarima': 0, 'prophet': 0, 'auto_arima': 0}\n",
    "                    continue\n",
    "                \n",
    "                pred_dict = {}\n",
    "                \n",
    "                # ARIMA\n",
    "                arima_model, arima_error = self.fit_arima(product_sales)\n",
    "                if arima_model is not None:\n",
    "                    try:\n",
    "                        forecast = arima_model.forecast(steps=1)\n",
    "                        pred_dict['arima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # SARIMA\n",
    "                sarima_model, sarima_error = self.fit_sarima(product_sales)\n",
    "                if sarima_model is not None:\n",
    "                    try:\n",
    "                        forecast = sarima_model.forecast(steps=1)\n",
    "                        pred_dict['sarima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['sarima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['sarima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # Prophet\n",
    "                prophet_model, prophet_error = self.fit_prophet(data, product_id)\n",
    "                if prophet_model is not None:\n",
    "                    try:\n",
    "                        future = prophet_model.make_future_dataframe(periods=1, freq='M')\n",
    "                        if 'ipc' in future.columns:\n",
    "                            future['ipc'] = future['ipc'].fillna(method='ffill')\n",
    "                        forecast = prophet_model.predict(future)\n",
    "                        pred_dict['prophet'] = max(0, forecast['yhat'].iloc[-1])\n",
    "                    except:\n",
    "                        pred_dict['prophet'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['prophet'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                # Auto-ARIMA\n",
    "                auto_arima_model, auto_arima_error = self.fit_auto_arima(product_sales)\n",
    "                if auto_arima_model is not None:\n",
    "                    try:\n",
    "                        forecast = auto_arima_model.predict(n_periods=1)\n",
    "                        pred_dict['auto_arima'] = max(0, forecast[0])\n",
    "                    except:\n",
    "                        pred_dict['auto_arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                else:\n",
    "                    pred_dict['auto_arima'] = product_sales[-1] if len(product_sales) > 0 else 0\n",
    "                \n",
    "                predictions[product_id] = pred_dict\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors[product_id] = str(e)\n",
    "                predictions[product_id] = {'arima': 0, 'sarima': 0, 'prophet': 0, 'auto_arima': 0}\n",
    "        \n",
    "        return predictions, errors\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENSEMBLE AVANZADO Y STACKING\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedEnsemble:\n",
    "    \"\"\"Ensemble avanzado con stacking multinivel\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.level1_models = {}\n",
    "        self.level2_models = {}\n",
    "        self.stacking_model = None\n",
    "        self.model_weights = {}\n",
    "        \n",
    "    def create_level1_models(self):\n",
    "        \"\"\"Crea modelos de primer nivel\"\"\"\n",
    "        models = {\n",
    "            'rf': RandomForestRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'xgb': xgb.XGBRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'lgb': lgb.LGBMRegressor(\n",
    "                n_estimators=300,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "                verbose=-1\n",
    "            ),\n",
    "            'catboost': CatBoostRegressor(\n",
    "                iterations=300,\n",
    "                depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                random_state=self.random_state,\n",
    "                verbose=False\n",
    "            ),\n",
    "            'gb': GradientBoostingRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=8,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                random_state=self.random_state\n",
    "            ),\n",
    "            'extra_trees': ExtraTreesRegressor(\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'ridge': Ridge(alpha=1.0),\n",
    "            'lasso': Lasso(alpha=0.1, random_state=self.random_state),\n",
    "            'elastic': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=self.random_state)\n",
    "        }\n",
    "        \n",
    "        self.level1_models = models\n",
    "        return models\n",
    "    \n",
    "    def create_stacking_model(self):\n",
    "        \"\"\"Crea modelo de stacking de segundo nivel\"\"\"\n",
    "        stacking_models = [\n",
    "            ('ridge', Ridge(alpha=0.1)),\n",
    "            ('lasso', Lasso(alpha=0.01, random_state=self.random_state)),\n",
    "            ('xgb_stack', xgb.XGBRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=4,\n",
    "                learning_rate=0.1,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        self.stacking_model = VotingRegressor(\n",
    "            estimators=stacking_models,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return self.stacking_model\n",
    "    \n",
    "    def fit_ensemble(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Entrena ensemble con validaciÃ³n cruzada\"\"\"\n",
    "        print(\"ğŸ¯ Entrenando ensemble avanzado...\")\n",
    "        \n",
    "        # Crear modelos\n",
    "        self.create_level1_models()\n",
    "        \n",
    "        # Entrenar modelos de nivel 1\n",
    "        level1_predictions_train = np.zeros((len(X_train), len(self.level1_models)))\n",
    "        level1_predictions_val = np.zeros((len(X_val), len(self.level1_models)))\n",
    "        \n",
    "        model_scores = {}\n",
    "        \n",
    "        for i, (name, model) in enumerate(tqdm(self.level1_models.items(), desc=\"Training Level 1\")):\n",
    "            try:\n",
    "                # Entrenar modelo\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # Predicciones\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_val = model.predict(X_val)\n",
    "                \n",
    "                level1_predictions_train[:, i] = pred_train\n",
    "                level1_predictions_val[:, i] = pred_val\n",
    "                \n",
    "                # Evaluar\n",
    "                val_mae = mean_absolute_error(y_val, pred_val)\n",
    "                val_rmse = np.sqrt(mean_squared_error(y_val, pred_val))\n",
    "                val_r2 = r2_score(y_val, pred_val)\n",
    "                \n",
    "                model_scores[name] = {\n",
    "                    'mae': val_mae,\n",
    "                    'rmse': val_rmse,\n",
    "                    'r2': val_r2\n",
    "                }\n",
    "                \n",
    "                print(f\"   âœ… {name}: MAE={val_mae:.3f}, RMSE={val_rmse:.3f}, R2={val_r2:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error en {name}: {e}\")\n",
    "                level1_predictions_train[:, i] = y_train.mean()\n",
    "                level1_predictions_val[:, i] = y_train.mean()\n",
    "        \n",
    "        # Entrenar modelo de stacking\n",
    "        self.create_stacking_model()\n",
    "        self.stacking_model.fit(level1_predictions_train, y_train)\n",
    "        \n",
    "        # PredicciÃ³n final del ensemble\n",
    "        final_pred = self.stacking_model.predict(level1_predictions_val)\n",
    "        \n",
    "        # Evaluar ensemble\n",
    "        ensemble_mae = mean_absolute_error(y_val, final_pred)\n",
    "        ensemble_rmse = np.sqrt(mean_squared_error(y_val, final_pred))\n",
    "        ensemble_r2 = r2_score(y_val, final_pred)\n",
    "        \n",
    "        print(f\"ğŸ† ENSEMBLE FINAL: MAE={ensemble_mae:.3f}, RMSE={ensemble_rmse:.3f}, R2={ensemble_r2:.3f}\")\n",
    "        \n",
    "        return model_scores, {\n",
    "            'mae': ensemble_mae,\n",
    "            'rmse': ensemble_rmse,\n",
    "            'r2': ensemble_r2\n",
    "        }\n",
    "    \n",
    "    def predict_ensemble(self, X_test):\n",
    "        \"\"\"Genera predicciones del ensemble\"\"\"\n",
    "        level1_predictions = np.zeros((len(X_test), len(self.level1_models)))\n",
    "        \n",
    "        for i, (name, model) in enumerate(self.level1_models.items()):\n",
    "            try:\n",
    "                level1_predictions[:, i] = model.predict(X_test)\n",
    "            except:\n",
    "                level1_predictions[:, i] = 0\n",
    "        \n",
    "        return self.stacking_model.predict(level1_predictions)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. OPTIMIZACIÃ“N DE HIPERPARÃMETROS CON OPTUNA\n",
    "# ============================================================================\n",
    "\n",
    "def optimize_hyperparameters(X_train, y_train, X_val, y_val, model_type='xgb', n_trials=100):\n",
    "    \"\"\"Optimiza hiperparÃ¡metros usando Optuna\"\"\"\n",
    "    print(f\"ğŸ”§ Optimizando {model_type} con Optuna...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        if model_type == 'xgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "            \n",
    "        elif model_type == 'lgb':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "            \n",
    "        elif model_type == 'catboost':\n",
    "            params = {\n",
    "                'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "                'depth': trial.suggest_int('depth', 3, 12),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'verbose': False\n",
    "            }\n",
    "            model = CatBoostRegressor(**params)\n",
    "        \n",
    "        elif model_type == 'rf':\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Modelo {model_type} no soportado\")\n",
    "        \n",
    "        # Entrenar y evaluar\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "        \n",
    "        return rmse\n",
    "    \n",
    "    # Crear estudio\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"   ğŸ† Mejor RMSE: {study.best_value:.4f}\")\n",
    "    print(f\"   ğŸ“Š Mejores parÃ¡metros: {study.best_params}\")\n",
    "    \n",
    "    return study.best_params\n",
    "\n",
    "# ============================================================================\n",
    "# 7. PIPELINE PRINCIPAL EXTENDIDO\n",
    "# ============================================================================\n",
    "\n",
    "def run_advanced_nocturnal_pipeline():\n",
    "    \"\"\"Pipeline principal con todos los modelos y optimizaciones\"\"\"\n",
    "    print(\"ğŸš€ INICIANDO PIPELINE AVANZADO NOCTURNO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Carga de datos\n",
    "    print(\"\\n1ï¸âƒ£ CARGA DE DATOS\")\n",
    "    sales, stocks, product_info, products_to_predict = load_and_prepare_data()\n",
    "    if sales is None:\n",
    "        print(\"âŒ Error cargando datos principales\")\n",
    "        return\n",
    "    \n",
    "    indec_data = load_indec_data()\n",
    "    \n",
    "    # 2. Feature Engineering Avanzado\n",
    "    print(\"\\n2ï¸âƒ£ FEATURE ENGINEERING AVANZADO\")\n",
    "    data = create_advanced_features_v2(sales, stocks, product_info, indec_data)\n",
    "    \n",
    "    # 3. AgregaciÃ³n y preparaciÃ³n (CORREGIDA)\n",
    "    print(\"\\n3ï¸âƒ£ AGREGACIÃ“N DE DATOS\")\n",
    "    print(\"ğŸ“Š Agregando datos por producto-cliente...\")\n",
    "\n",
    "    # Filtrar datos hasta diciembre 2019\n",
    "    data_filtered = data[data['periodo'] <= '2019-12-01'].copy()\n",
    "\n",
    "    # REDUCIR FEATURES ANTES DE AGREGAR para ahorrar memoria\n",
    "    print(\"ğŸ”§ Reduciendo features para optimizar memoria...\")\n",
    "\n",
    "    # Verificar y limpiar columnas de agrupaciÃ³n\n",
    "    print(\"ğŸ” Verificando columnas de agrupaciÃ³n...\")\n",
    "    grouping_cols = ['product_id', 'customer_id', 'periodo']\n",
    "\n",
    "    for col in grouping_cols:\n",
    "        if col in data_filtered.columns:\n",
    "            print(f\"   {col}: {data_filtered[col].dtype}, shape: {data_filtered[col].shape}\")\n",
    "            # Asegurar que sea unidimensional\n",
    "            if len(data_filtered[col].shape) > 1:\n",
    "                data_filtered[col] = data_filtered[col].iloc[:, 0]  # Tomar primera columna si es multidimensional\n",
    "        else:\n",
    "            print(f\"   âš ï¸ {col} no encontrada en el dataset\")\n",
    "\n",
    "    # Eliminar posibles columnas duplicadas\n",
    "    print(\"ğŸ§¹ Eliminando columnas duplicadas...\")\n",
    "    data_filtered = data_filtered.loc[:, ~data_filtered.columns.duplicated()]\n",
    "\n",
    "    # Mantener solo las features mÃ¡s importantes\n",
    "    numeric_cols = data_filtered.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in grouping_cols + ['tn']]\n",
    "\n",
    "    # Seleccionar top 50 features numÃ©ricas (ajusta segÃºn necesidad)\n",
    "    important_features = grouping_cols + ['tn'] + numeric_cols[:50]\n",
    "    \n",
    "    # Filtrar solo features importantes y eliminar duplicados\n",
    "    important_features = list(dict.fromkeys(important_features))  # Eliminar duplicados manteniendo orden\n",
    "    data_filtered = data_filtered[important_features]\n",
    "\n",
    "    print(f\"ğŸ“Š Features reducidas a {len(data_filtered.columns)}\")\n",
    "    print(f\"ğŸ“Š Columnas finales: {list(data_filtered.columns)}\")\n",
    "\n",
    "    # Verificar tipos de datos\n",
    "    print(\"ğŸ” Verificando tipos de datos finales...\")\n",
    "    for col in grouping_cols:\n",
    "        if col in data_filtered.columns:\n",
    "            print(f\"   {col}: {data_filtered[col].dtype}\")\n",
    "\n",
    "    # AgregaciÃ³n por chunks para manejar memoria\n",
    "    print(\"ğŸ”„ Agregando por chunks...\")\n",
    "    chunk_size = 100000\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(data_filtered), chunk_size):\n",
    "        try:\n",
    "            chunk = data_filtered.iloc[i:i+chunk_size].copy()\n",
    "            \n",
    "            # Verificar que las columnas de agrupaciÃ³n existan y sean vÃ¡lidas\n",
    "            missing_cols = [col for col in grouping_cols if col not in chunk.columns]\n",
    "            if missing_cols:\n",
    "                print(f\"   âš ï¸ Columnas faltantes en chunk {i//chunk_size + 1}: {missing_cols}\")\n",
    "                continue\n",
    "                \n",
    "            # Crear diccionario de agregaciÃ³n dinÃ¡micamente\n",
    "            agg_dict = {'tn': 'sum'}\n",
    "            other_cols = [col for col in chunk.columns if col not in grouping_cols + ['tn']]\n",
    "            \n",
    "            for col in other_cols:\n",
    "                if chunk[col].dtype in ['object', 'category']:\n",
    "                    agg_dict[col] = 'first'\n",
    "                else:\n",
    "                    agg_dict[col] = 'mean'  # Usar mean para numÃ©ricas\n",
    "            \n",
    "            chunk_agg = chunk.groupby(grouping_cols).agg(agg_dict).reset_index()\n",
    "            chunks.append(chunk_agg)\n",
    "            print(f\"   Chunk {i//chunk_size + 1}: {len(chunk_agg)} registros\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error en chunk {i//chunk_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not chunks:\n",
    "        print(\"âŒ No se pudieron procesar chunks. Intentando agregaciÃ³n simple...\")\n",
    "        \n",
    "        # Fallback: agregaciÃ³n simple con menos features\n",
    "        essential_cols = ['product_id', 'customer_id', 'periodo', 'tn']\n",
    "        data_simple = data_filtered[essential_cols].copy()\n",
    "        \n",
    "        agg_data = data_simple.groupby(['product_id', 'customer_id', 'periodo']).agg({\n",
    "            'tn': 'sum'\n",
    "        }).reset_index()\n",
    "    else:\n",
    "        # Combinar chunks\n",
    "        print(\"ğŸ”— Combinando chunks...\")\n",
    "        agg_data = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        # Reagrupar para consolidar posibles duplicados\n",
    "        if len(chunks) > 1:\n",
    "            print(\"ğŸ”„ Consolidando datos finales...\")\n",
    "            final_agg_dict = {'tn': 'sum'}\n",
    "            other_cols = [col for col in agg_data.columns if col not in grouping_cols + ['tn']]\n",
    "            \n",
    "            for col in other_cols:\n",
    "                if agg_data[col].dtype in ['object', 'category']:\n",
    "                    final_agg_dict[col] = 'first'\n",
    "                else:\n",
    "                    final_agg_dict[col] = 'mean'\n",
    "            \n",
    "            agg_data = agg_data.groupby(grouping_cols).agg(final_agg_dict).reset_index()\n",
    "\n",
    "    print(f\"âœ… Datos agregados: {agg_data.shape}\")\n",
    "    \n",
    "    # 4. Splits temporales\n",
    "    print(\"\\n4ï¸âƒ£ CREACIÃ“N DE SPLITS TEMPORALES\")\n",
    "    cutoff_date = pd.to_datetime('2019-10-01')\n",
    "    \n",
    "    train_data = agg_data[agg_data['periodo'] <= cutoff_date]\n",
    "    val_data = agg_data[agg_data['periodo'] > cutoff_date]\n",
    "    \n",
    "    print(f\"ğŸ“Š Training: {len(train_data):,} registros (hasta {cutoff_date.strftime('%Y-%m')})\")\n",
    "    print(f\"ğŸ“Š Validation: {len(val_data):,} registros (desde {cutoff_date.strftime('%Y-%m')})\")\n",
    "    \n",
    "    # Preparar features\n",
    "    feature_cols = [col for col in agg_data.columns if col not in ['tn', 'periodo', 'product_id', 'customer_id']]\n",
    "    \n",
    "    X_train = train_data[feature_cols]\n",
    "    y_train = train_data['tn']\n",
    "    X_val = val_data[feature_cols]\n",
    "    y_val = val_data['tn']\n",
    "    \n",
    "    # Crear un diccionario para almacenar los escaladores\n",
    "    # Remove target scaling - train on original scale\n",
    "    scalers = {}\n",
    "    # Don't scale the target variable\n",
    "    y_train_for_training = y_train  # Use original scale\n",
    "\n",
    "    # Then in your model training loop, use:\n",
    "    #model.fit(X_train_processed, y_train_for_training)  # Instead of y_train_scaled\n",
    "\n",
    "    # Handling missing values y encoding\n",
    "    print(\"ğŸ”§ Procesando features...\")\n",
    "    \n",
    "    # Separar numÃ©ricas y categÃ³ricas\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "\n",
    "    # Preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', RobustScaler(with_centering=True, with_scaling=True))\n",
    "            ]), numeric_features),\n",
    "            ('cat', Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ]), categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Ajustar preprocessor\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "    \n",
    "    print(f\"âœ… Features procesadas: {X_train_processed.shape[1]} dimensiones\")\n",
    "    \n",
    "    # 5. Modelos de Machine Learning\n",
    "    print(\"\\n5ï¸âƒ£ EVALUACIÃ“N DE MODELOS DE ML\")\n",
    "    \n",
    "    models_to_test = {\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBRegressor(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMRegressor(\n",
    "            n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE, \n",
    "            n_jobs=-1, verbose=-1\n",
    "        ),\n",
    "        'CatBoost': CatBoostRegressor(\n",
    "            iterations=300, depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=RANDOM_STATE, verbose=False\n",
    "        ),\n",
    "        'GradientBoosting': GradientBoostingRegressor(\n",
    "            n_estimators=200, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, random_state=RANDOM_STATE\n",
    "        ),\n",
    "        'ExtraTrees': ExtraTreesRegressor(\n",
    "            n_estimators=200, max_depth=15, min_samples_split=10,\n",
    "            min_samples_leaf=5, random_state=RANDOM_STATE, n_jobs=-1\n",
    "        ),\n",
    "        'Ridge': Ridge(alpha=1.0),\n",
    "        'Lasso': Lasso(alpha=0.1, random_state=RANDOM_STATE),\n",
    "        'ElasticNet': ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    \n",
    "    # Evaluar modelos\n",
    "    ml_results = {}\n",
    "    best_models = {}\n",
    "    \n",
    "    for name, model in tqdm(models_to_test.items(), desc=\"Evaluando modelos ML\"):\n",
    "        try:\n",
    "            # Entrenar\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Predecir\n",
    "            y_pred = model.predict(X_val_processed)\n",
    "            \n",
    "            # MÃ©tricas\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            r2 = r2_score(y_val, y_pred)\n",
    "            mape = mean_absolute_percentage_error(y_val, y_pred) * 100\n",
    "            \n",
    "            ml_results[name] = {\n",
    "                'MAE': mae,\n",
    "                'RMSE': rmse,\n",
    "                'R2': r2,\n",
    "                'MAPE': mape\n",
    "            }\n",
    "            \n",
    "            best_models[name] = model\n",
    "            \n",
    "            print(f\"âœ… {name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}, MAPE: {mape:.1f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error en {name}: {e}\")\n",
    "    \n",
    "    # 6. OptimizaciÃ³n de hiperparÃ¡metros (para los mejores modelos)\n",
    "    if MODEL_CONFIG['use_hyperparameter_tuning']:\n",
    "        print(\"\\n6ï¸âƒ£ OPTIMIZACIÃ“N DE HIPERPARÃMETROS\")\n",
    "        \n",
    "        # Seleccionar top 3 modelos\n",
    "        top_models = sorted(ml_results.items(), key=lambda x: x[1]['RMSE'])[:3]\n",
    "        optimized_models = {}\n",
    "        \n",
    "        for model_name, _ in top_models:\n",
    "            if model_name.lower() in ['xgboost', 'lightgbm', 'catboost']:\n",
    "                print(f\"ğŸ”§ Optimizando {model_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    best_params = optimize_hyperparameters(\n",
    "                        X_train_processed, y_train, X_val_processed, y_val,\n",
    "                        model_type=model_name.lower().replace('boost', 'b').replace('gradient', 'gb'),\n",
    "                        n_trials=MODEL_CONFIG['optuna_trials']\n",
    "                    )\n",
    "                    \n",
    "                    # Crear modelo optimizado\n",
    "                    if 'xg' in model_name.lower():\n",
    "                        optimized_model = xgb.XGBRegressor(**best_params)\n",
    "                    elif 'light' in model_name.lower():\n",
    "                        optimized_model = lgb.LGBMRegressor(**best_params)\n",
    "                    elif 'cat' in model_name.lower():\n",
    "                        optimized_model = CatBoostRegressor(**best_params)\n",
    "                    \n",
    "                    # Evaluar modelo optimizado\n",
    "                    optimized_model.fit(X_train_processed, y_train)\n",
    "                    y_pred_opt = optimized_model.predict(X_val_processed)\n",
    "                    \n",
    "                    mae_opt = mean_absolute_error(y_val, y_pred_opt)\n",
    "                    rmse_opt = np.sqrt(mean_squared_error(y_val, y_pred_opt))\n",
    "                    r2_opt = r2_score(y_val, y_pred_opt)\n",
    "                    mape_opt = mean_absolute_percentage_error(y_val, y_pred_opt) * 100\n",
    "                    \n",
    "                    optimized_models[f\"{model_name}_Optimized\"] = {\n",
    "                        'model': optimized_model,\n",
    "                        'MAE': mae_opt,\n",
    "                        'RMSE': rmse_opt,\n",
    "                        'R2': r2_opt,\n",
    "                        'MAPE': mape_opt\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"âœ… {model_name} Optimizado - MAE: {mae_opt:.2f}, RMSE: {rmse_opt:.2f}, R2: {r2_opt:.3f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error optimizando {model_name}: {e}\")\n",
    "    \n",
    "    # 7. Modelos de Series de Tiempo\n",
    "    if MODEL_CONFIG['use_time_series_models']:\n",
    "        print(\"\\n7ï¸âƒ£ MODELOS DE SERIES DE TIEMPO\")\n",
    "        \n",
    "        ts_models = TimeSeriesModels(random_state=RANDOM_STATE)\n",
    "        products_list = products_to_predict.iloc[:, 0].tolist()[:50]  # Limitamos a 50 para prueba nocturna\n",
    "        \n",
    "        ts_predictions, ts_errors = ts_models.predict_time_series(\n",
    "            data_filtered, products_list, TARGET_DATE\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Predicciones de series de tiempo completadas para {len(ts_predictions)} productos\")\n",
    "        if ts_errors:\n",
    "            print(f\"âš ï¸ Errores en {len(ts_errors)} productos\")\n",
    "    \n",
    "    # 8. Ensemble Avanzado\n",
    "    if MODEL_CONFIG['use_ensemble_stacking']:\n",
    "        print(\"\\n8ï¸âƒ£ ENSEMBLE AVANZADO\")\n",
    "        \n",
    "        ensemble = AdvancedEnsemble(random_state=RANDOM_STATE)\n",
    "        model_scores, ensemble_score = ensemble.fit_ensemble(\n",
    "            X_train_processed, y_train, X_val_processed, y_val\n",
    "        )\n",
    "        \n",
    "        # Agregar ensemble a resultados\n",
    "        ml_results['AdvancedEnsemble'] = ensemble_score\n",
    "        best_models['AdvancedEnsemble'] = ensemble\n",
    "    \n",
    "    # 9. SelecciÃ³n del mejor modelo\n",
    "    print(\"\\n9ï¸âƒ£ SELECCIÃ“N DEL MEJOR MODELO\")\n",
    "    \n",
    "    # Combinar resultados normales y optimizados\n",
    "    all_results = ml_results.copy()\n",
    "    for name, result in optimized_models.items():\n",
    "        all_results[name] = {\n",
    "            'MAE': result['MAE'],\n",
    "            'RMSE': result['RMSE'],\n",
    "            'R2': result['R2'],\n",
    "            'MAPE': result['MAPE']\n",
    "        }\n",
    "        best_models[name] = result['model']\n",
    "    \n",
    "    # Crear DataFrame de resultados\n",
    "    results_df = pd.DataFrame(all_results).T\n",
    "    results_df = results_df.sort_values('RMSE')\n",
    "    \n",
    "    print(\"\\nğŸ“Š RESUMEN DE RESULTADOS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(results_df.round(3))\n",
    "    \n",
    "    # Mejor modelo\n",
    "    best_model_name = results_df.index[0]\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    print(f\"\\nğŸ† MEJOR MODELO: {best_model_name}\")\n",
    "    print(f\"   RMSE: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "    print(f\"   MAE: {results_df.loc[best_model_name, 'MAE']:.3f}\")\n",
    "    print(f\"   R2: {results_df.loc[best_model_name, 'R2']:.3f}\")\n",
    "    \n",
    "    # 10. Reentrenamiento con datos completos\n",
    "    print(\"\\nğŸ”Ÿ REENTRENAMIENTO CON DATOS COMPLETOS\")\n",
    "    print(\"ğŸ”„ Reentrenando el mejor modelo con todos los datos disponibles...\")\n",
    "    \n",
    "    # Combinar train y validation\n",
    "    X_full = np.vstack([X_train_processed, X_val_processed])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Reentrenar\n",
    "    best_model.fit(X_full, y_full)\n",
    "    \n",
    "    # 11. PREDICCIONES FINALES\n",
    "    print(\"\\n1ï¸âƒ£1ï¸âƒ£ PREDICCIONES FINALES\")\n",
    "    print(f\"ğŸ¯ Generando predicciones para {TARGET_DATE}...\")\n",
    "\n",
    "    # Preparar datos para predicciÃ³n\n",
    "    products_list = products_to_predict.iloc[:, 0].unique()\n",
    "    all_predictions = []\n",
    "\n",
    "    # Obtener lista Ãºnica de clientes por producto\n",
    "    product_customer_pairs = agg_data.groupby('product_id')['customer_id'].unique()\n",
    "\n",
    "    for product_id in tqdm(products_list, desc=\"Generando predicciones\"):\n",
    "        try:\n",
    "            # Obtener todos los clientes histÃ³ricos para este producto\n",
    "            customers = product_customer_pairs.get(product_id, [])\n",
    "            if len(customers) == 0:\n",
    "                # Si no hay clientes histÃ³ricos, usar predicciÃ³n por defecto\n",
    "                all_predictions.append({\n",
    "                    'product_id': product_id,\n",
    "                    'prediction': 0,\n",
    "                    'method': 'default'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            product_predictions = []\n",
    "            \n",
    "            # Generar predicciÃ³n para cada cliente del producto\n",
    "            for customer_id in customers:\n",
    "                try:\n",
    "                    # Obtener Ãºltimo registro del par producto-cliente\n",
    "                    customer_data = agg_data[\n",
    "                        (agg_data['product_id'] == product_id) & \n",
    "                        (agg_data['customer_id'] == customer_id)\n",
    "                    ]\n",
    "                    \n",
    "                    if len(customer_data) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # Crear registro para perÃ­odo futuro\n",
    "                    last_record = customer_data.sort_values('periodo').iloc[-1:].copy()\n",
    "                    future_record = last_record.copy()\n",
    "                    target_period = pd.to_datetime(TARGET_DATE)\n",
    "                    \n",
    "                    # Actualizar features temporales\n",
    "                    if 'year' in feature_cols:\n",
    "                        future_record['year'] = target_period.year\n",
    "                    if 'month' in feature_cols:\n",
    "                        future_record['month'] = target_period.month\n",
    "                    if 'quarter' in feature_cols:\n",
    "                        future_record['quarter'] = (target_period.month - 1) // 3 + 1\n",
    "\n",
    "                    # Preparar features\n",
    "                    prediction_features = future_record[feature_cols]\n",
    "                    prediction_features = prediction_features.fillna(\n",
    "                        customer_data[feature_cols].median().fillna(0)\n",
    "                    )\n",
    "\n",
    "                    # Procesar features\n",
    "                    last_record_processed = preprocessor.transform(prediction_features)\n",
    "                    \n",
    "                    # Predecir\n",
    "                    if best_model_name == 'AdvancedEnsemble':\n",
    "                        pred = best_model.predict_ensemble(last_record_processed)[0]\n",
    "                    else:\n",
    "                        pred = best_model.predict(last_record_processed)[0]\n",
    "\n",
    "                    # Validar predicciÃ³n\n",
    "                    pred = max(0, pred)  # No predicciones negativas\n",
    "                    historical_mean = customer_data['tn'].mean()\n",
    "                    \n",
    "                    if pred < historical_mean * 0.01:  # Si es menor al 1% del histÃ³rico\n",
    "                        pred = historical_mean * 0.1  # Usar 10% del histÃ³rico como mÃ­nimo\n",
    "                    \n",
    "                    product_predictions.append(pred)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en cliente {customer_id} para producto {product_id}: {e}\")\n",
    "                    # Usar media histÃ³rica del cliente como fallback\n",
    "                    historical_pred = customer_data['tn'].mean()\n",
    "                    if not np.isnan(historical_pred):\n",
    "                        product_predictions.append(historical_pred)\n",
    "\n",
    "            # Sumar todas las predicciones de clientes para obtener la predicciÃ³n del producto\n",
    "            final_prediction = sum(product_predictions) if product_predictions else 0\n",
    "            \n",
    "            # Guardar predicciÃ³n agregada del producto\n",
    "            all_predictions.append({\n",
    "                'product_id': product_id,\n",
    "                'prediction': final_prediction,\n",
    "                'method': best_model_name,\n",
    "                'n_customers': len(product_predictions)\n",
    "            })\n",
    "            \n",
    "            if len(all_predictions) % 50 == 0:\n",
    "                print(f\"\\nProducto {product_id}:\")\n",
    "                print(f\"Clientes procesados: {len(product_predictions)}\")\n",
    "                print(f\"PredicciÃ³n total: {final_prediction:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error en producto {product_id}: {e}\")\n",
    "            # Usar el promedio histÃ³rico como fallback\n",
    "            historical_pred = agg_data[agg_data['product_id'] == product_id]['tn'].sum()\n",
    "            all_predictions.append({\n",
    "                'product_id': product_id,\n",
    "                'prediction': max(0, historical_pred),\n",
    "                'method': 'historical_fallback',\n",
    "                'n_customers': 0\n",
    "            })\n",
    "\n",
    "    # Convertir a DataFrame\n",
    "    predictions_df = pd.DataFrame(all_predictions)\n",
    "    \n",
    "    print(f\"âœ… Predicciones completadas:\")\n",
    "    print(f\"   Total: {len(predictions_df)}\")\n",
    "    print(f\"   Con modelo: {len(predictions_df[predictions_df['method'] != 'error'])}\")\n",
    "    print(f\"   Errores: {len(predictions_df[predictions_df['method'] == 'error'])}\")\n",
    "    \n",
    "    # 12. Guardar resultados\n",
    "    print(\"\\n1ï¸âƒ£2ï¸âƒ£ GUARDANDO RESULTADOS\")\n",
    "    \n",
    "\n",
    "    # Agrupar por producto y sumar predicciones\n",
    "    submission = predictions_df.groupby('product_id', as_index=False)['prediction'].sum()\n",
    "    submission = submission.rename(columns={'prediction': 'target_202002'})\n",
    "    # Crear submission\n",
    "    # Crear submission\n",
    "    #submission = pd.DataFrame({\n",
    "    #    'product_id': validated_predictions['product_id'],\n",
    "    #    'target_202002': validated_predictions['prediction']\n",
    "    #})\n",
    "    \n",
    "    # EstadÃ­sticas de predicciones\n",
    "    print(f\"\\nğŸ“ˆ ESTADÃSTICAS DE PREDICCIONES:\")\n",
    "    print(f\"   Media: {submission['target_202002'].mean():.2f}\")\n",
    "    print(f\"   Mediana: {submission['target_202002'].median():.2f}\")\n",
    "    print(f\"   Std: {submission['target_202002'].std():.2f}\")\n",
    "    print(f\"   Min: {submission['target_202002'].min():.2f}\")\n",
    "    print(f\"   Max: {submission['target_202002'].max():.2f}\")\n",
    "    print(f\"   Predicciones = 0: {(submission['target_202002'] == 0).sum()}\")\n",
    "    # Generar submissions para todos los modelos\n",
    "    \n",
    "    if MODEL_CONFIG.get('generate_multiple_submissions', True):\n",
    "        print(\"\\nğŸ“Š GENERANDO SUBMISSIONS MÃšLTIPLES\")\n",
    "        \n",
    "        # Generar predicciones para cada modelo\n",
    "        for model_name, model in best_models.items():\n",
    "            if model_name == best_model_name:\n",
    "                continue  # Ya lo hicimos antes\n",
    "            \n",
    "            print(f\"\\nGenerando predicciones para {model_name}...\")\n",
    "            model_predictions = []\n",
    "            \n",
    "            # Iterar sobre cada producto\n",
    "            for product_id in tqdm(products_list, desc=f\"Prediciendo con {model_name}\"):\n",
    "                try:\n",
    "                    # Obtener todos los clientes histÃ³ricos para este producto\n",
    "                    customers = product_customer_pairs.get(product_id, [])\n",
    "                    if len(customers) == 0:\n",
    "                        model_predictions.append({\n",
    "                            'product_id': product_id,\n",
    "                            'target_202002': 0\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    product_predictions = []\n",
    "                    \n",
    "                    # Predecir para cada cliente del producto\n",
    "                    for customer_id in customers:\n",
    "                        try:\n",
    "                            # Obtener datos del cliente\n",
    "                            customer_data = agg_data[\n",
    "                                (agg_data['product_id'] == product_id) & \n",
    "                                (agg_data['customer_id'] == customer_id)\n",
    "                            ]\n",
    "                            \n",
    "                            if len(customer_data) == 0:\n",
    "                                continue\n",
    "\n",
    "                            # Preparar features\n",
    "                            last_record = customer_data.sort_values('periodo').iloc[-1:].copy()\n",
    "                            future_record = last_record.copy()\n",
    "                            target_period = pd.to_datetime(TARGET_DATE)\n",
    "                            \n",
    "                            # Actualizar features temporales\n",
    "                            if 'year' in feature_cols:\n",
    "                                future_record['year'] = target_period.year\n",
    "                            if 'month' in feature_cols:\n",
    "                                future_record['month'] = target_period.month\n",
    "                            if 'quarter' in feature_cols:\n",
    "                                future_record['quarter'] = (target_period.month - 1) // 3 + 1\n",
    "\n",
    "                            prediction_features = future_record[feature_cols]\n",
    "                            prediction_features = prediction_features.fillna(\n",
    "                                customer_data[feature_cols].median().fillna(0)\n",
    "                            )\n",
    "\n",
    "                            # Procesar features\n",
    "                            last_record_processed = preprocessor.transform(prediction_features)\n",
    "                            \n",
    "                            # Predecir\n",
    "                            if model_name == 'AdvancedEnsemble':\n",
    "                                pred = model.predict_ensemble(last_record_processed)[0]\n",
    "                            else:\n",
    "                                pred = model.predict(last_record_processed)[0]\n",
    "\n",
    "                            # Validar predicciÃ³n\n",
    "                            pred = max(0, pred)\n",
    "                            historical_mean = customer_data['tn'].mean()\n",
    "                            \n",
    "                            if pred < historical_mean * 0.01:\n",
    "                                pred = historical_mean * 0.1\n",
    "                            \n",
    "                            product_predictions.append(pred)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error en cliente {customer_id} para producto {product_id}: {e}\")\n",
    "                            historical_pred = customer_data['tn'].mean()\n",
    "                            if not np.isnan(historical_pred):\n",
    "                                product_predictions.append(historical_pred)\n",
    "\n",
    "                    # Sumar predicciones de todos los clientes\n",
    "                    final_prediction = sum(product_predictions) if product_predictions else 0\n",
    "                    \n",
    "                    model_predictions.append({\n",
    "                        'product_id': product_id,\n",
    "                        'target_202002': final_prediction\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error en producto {product_id}: {e}\")\n",
    "                    historical_pred = agg_data[agg_data['product_id'] == product_id]['tn'].sum()\n",
    "                    model_predictions.append({\n",
    "                        'product_id': product_id,\n",
    "                        'target_202002': max(0, historical_pred)\n",
    "                    })\n",
    "            \n",
    "            # Guardar submission del modelo\n",
    "            model_submission = pd.DataFrame(model_predictions)\n",
    "            model_filename = f'submission_{model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "            model_submission.to_csv(model_filename, index=False)\n",
    "            print(f\"ğŸ’¾ {model_name} guardado: {model_filename}\")\n",
    "            \n",
    "            # Mostrar estadÃ­sticas\n",
    "            print(f\"   Media: {model_submission['target_202002'].mean():.2f}\")\n",
    "            print(f\"   Mediana: {model_submission['target_202002'].median():.2f}\")\n",
    "            print(f\"   Max: {model_submission['target_202002'].max():.2f}\")\n",
    "    # Guardar submission\n",
    "    submission_filename = f'submission_nocturnal_{best_model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"ğŸ’¾ Submission guardada: {submission_filename}\")\n",
    "    \n",
    "    # Guardar resultados detallados\n",
    "    results_filename = f'results_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "    results_df.to_csv(results_filename)\n",
    "    print(f\"ğŸ“Š Resultados detallados guardados: {results_filename}\")\n",
    "    \n",
    "    # Guardar modelo\n",
    "    if MODEL_CONFIG['save_model']:\n",
    "        model_filename = f'best_model_nocturnal_{best_model_name.lower()}_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.pkl'\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        \n",
    "        # Guardar tambiÃ©n el preprocessor\n",
    "        preprocessor_filename = f'preprocessor_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.pkl'\n",
    "        joblib.dump(preprocessor, preprocessor_filename)\n",
    "        \n",
    "        print(f\"ğŸ¤– Modelo guardado: {model_filename}\")\n",
    "        print(f\"ğŸ”§ Preprocessor guardado: {preprocessor_filename}\")\n",
    "    \n",
    "    # 13. AnÃ¡lisis de feature importance\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        print(\"\\n1ï¸âƒ£3ï¸âƒ£ ANÃLISIS DE FEATURE IMPORTANCE\")\n",
    "        \n",
    "        # Obtener nombres de features despuÃ©s del preprocessing\n",
    "        feature_names = []\n",
    "        \n",
    "        # Features numÃ©ricas\n",
    "        feature_names.extend(numeric_features)\n",
    "        \n",
    "        # Features categÃ³ricas (despuÃ©s de one-hot encoding)\n",
    "        if categorical_features:\n",
    "            cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "            cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "            feature_names.extend(cat_feature_names)\n",
    "        \n",
    "        # Crear DataFrame de importancias\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nğŸ” TOP 20 FEATURES MÃS IMPORTANTES:\")\n",
    "        print(\"=\" * 50)\n",
    "        for i, (_, row) in enumerate(importance_df.head(20).iterrows(), 1):\n",
    "            print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f}\")\n",
    "        \n",
    "        # Guardar importancias\n",
    "        importance_filename = f'feature_importance_nocturnal_{pd.Timestamp.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "        importance_df.to_csv(importance_filename, index=False)\n",
    "        print(f\"\\nğŸ’¾ Feature importance guardada: {importance_filename}\")\n",
    "    \n",
    "    # 14. MÃ©tricas de calidad de predicciÃ³n\n",
    "    print(\"\\n1ï¸âƒ£4ï¸âƒ£ MÃ‰TRICAS DE CALIDAD\")\n",
    "    \n",
    "    # DistribuciÃ³n de predicciones\n",
    "    pred_stats = {\n",
    "        'zero_predictions': (submission['target_202002'] == 0).sum(),\n",
    "        'low_predictions': (submission['target_202002'] < 1).sum(),\n",
    "        'medium_predictions': ((submission['target_202002'] >= 1) & (submission['target_202002'] < 10)).sum(),\n",
    "        'high_predictions': (submission['target_202002'] >= 10).sum(),\n",
    "        'extreme_predictions': (submission['target_202002'] >= 100).sum()\n",
    "    }\n",
    "    \n",
    "    print(f\"ğŸ“Š DistribuciÃ³n de predicciones:\")\n",
    "    for key, value in pred_stats.items():\n",
    "        percentage = (value / len(submission)) * 100\n",
    "        print(f\"   {key}: {value} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # ComparaciÃ³n con datos histÃ³ricos\n",
    "    historical_mean = data_filtered['tn'].mean()\n",
    "    historical_median = data_filtered['tn'].median()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ ComparaciÃ³n con histÃ³ricos:\")\n",
    "    print(f\"   Media histÃ³rica: {historical_mean:.2f}\")\n",
    "    print(f\"   Media predicha: {submission['target_202002'].mean():.2f}\")\n",
    "    print(f\"   Ratio: {submission['target_202002'].mean() / historical_mean:.2f}\")\n",
    "    print(f\"   Mediana histÃ³rica: {historical_median:.2f}\")\n",
    "    print(f\"   Mediana predicha: {submission['target_202002'].median():.2f}\")\n",
    "    \n",
    "    # 15. ValidaciÃ³n cruzada temporal (si hay tiempo)\n",
    "    if MODEL_CONFIG['use_cross_validation']:\n",
    "        print(\"\\n1ï¸âƒ£5ï¸âƒ£ VALIDACIÃ“N CRUZADA TEMPORAL\")\n",
    "        \n",
    "        cv_scores = []\n",
    "        n_splits = 3\n",
    "        \n",
    "        # Crear splits temporales para CV\n",
    "        periods = sorted(agg_data['periodo'].unique())\n",
    "        split_size = len(periods) // (n_splits + 1)\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            # Definir fechas de corte\n",
    "            train_end_idx = (i + 1) * split_size\n",
    "            val_start_idx = train_end_idx\n",
    "            val_end_idx = train_end_idx + split_size\n",
    "            \n",
    "            if val_end_idx > len(periods):\n",
    "                break\n",
    "            \n",
    "            train_end_date = periods[train_end_idx - 1]\n",
    "            val_start_date = periods[val_start_idx]\n",
    "            val_end_date = periods[val_end_idx - 1]\n",
    "            \n",
    "            # Crear splits\n",
    "            cv_train = agg_data[agg_data['periodo'] <= train_end_date]\n",
    "            cv_val = agg_data[(agg_data['periodo'] >= val_start_date) & \n",
    "                             (agg_data['periodo'] <= val_end_date)]\n",
    "            \n",
    "            if len(cv_train) == 0 or len(cv_val) == 0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Preparar datos\n",
    "                X_cv_train = cv_train[feature_cols]\n",
    "                y_cv_train = cv_train['tn']\n",
    "                X_cv_val = cv_val[feature_cols]\n",
    "                y_cv_val = cv_val['tn']\n",
    "                \n",
    "                # Procesar\n",
    "                X_cv_train_processed = preprocessor.fit_transform(X_cv_train)\n",
    "                X_cv_val_processed = preprocessor.transform(X_cv_val)\n",
    "                \n",
    "                # Entrenar y evaluar\n",
    "                cv_model = clone(best_model)\n",
    "                cv_model.fit(X_cv_train_processed, y_cv_train)\n",
    "                cv_pred = cv_model.predict(X_cv_val_processed)\n",
    "                \n",
    "                cv_rmse = np.sqrt(mean_squared_error(y_cv_val, cv_pred))\n",
    "                cv_scores.append(cv_rmse)\n",
    "                \n",
    "                print(f\"   Fold {i+1}: RMSE = {cv_rmse:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error en fold {i+1}: {e}\")\n",
    "        \n",
    "        if cv_scores:\n",
    "            print(f\"\\nğŸ“Š CV Results:\")\n",
    "            print(f\"   Mean RMSE: {np.mean(cv_scores):.3f} Â± {np.std(cv_scores):.3f}\")\n",
    "            print(f\"   Scores: {[f'{score:.3f}' for score in cv_scores]}\")\n",
    "    \n",
    "    # 16. Resumen final\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¯ RESUMEN FINAL DEL PIPELINE NOCTURNO\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"â° Tiempo total de ejecuciÃ³n: {time.time() - start_time:.1f} segundos\")\n",
    "    print(f\"ğŸ† Mejor modelo: {best_model_name}\")\n",
    "    print(f\"ğŸ“Š RMSE de validaciÃ³n: {results_df.loc[best_model_name, 'RMSE']:.3f}\")\n",
    "    print(f\"ğŸ“Š MAE de validaciÃ³n: {results_df.loc[best_model_name, 'MAE']:.3f}\")\n",
    "    print(f\"ğŸ“Š RÂ² de validaciÃ³n: {results_df.loc[best_model_name, 'R2']:.3f}\")\n",
    "    print(f\"ğŸ¯ Predicciones generadas: {len(submission)}\")\n",
    "    print(f\"ğŸ’¾ Archivo de submission: {submission_filename}\")\n",
    "    print(f\"âœ… Pipeline completado exitosamente!\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'preprocessor': preprocessor,\n",
    "        'results': results_df,\n",
    "        'submission': submission,\n",
    "        'predictions': predictions_df,\n",
    "        'model_name': best_model_name\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# 8. EJECUCIÃ“N PRINCIPAL\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸŒ™ INICIANDO PIPELINE NOCTURNO AVANZADO\")\n",
    "    print(\"â° Hora de inicio:\", pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Ejecutar pipeline principal\n",
    "        results = run_advanced_nocturnal_pipeline()\n",
    "        \n",
    "        if results:\n",
    "            print(\"\\nğŸ‰ PIPELINE COMPLETADO EXITOSAMENTE!\")\n",
    "            print(f\"ğŸ† Mejor modelo: {results['model_name']}\")\n",
    "            print(f\"ğŸ“Š Submission generada con {len(results['submission'])} predicciones\")\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nâŒ ERROR EN EL PIPELINE\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâ¹ï¸ Pipeline interrumpido por el usuario\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nğŸ’¥ ERROR CRÃTICO: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nâ° Tiempo total: {total_time:.1f} segundos ({total_time/60:.1f} minutos)\")\n",
    "        print(\"ğŸŒ™ Fin del pipeline nocturno\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
